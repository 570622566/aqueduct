{
    "docs": [
        {
            "location": "/", 
            "text": "Aqueduct is a server-side framework for building and deploying REST applications. It is written in Dart.\n\n\nImportant Links\n\n\nGetting Started and Installation\n\n\nTutorial\n.\n\n\nAPI Reference\n (or you can install it in \nDash\n).\n\n\nExample Repository\n\n\nCheck out \nSnippets\n for quick code snippets to get you up and running faster.\n\n\nImport \nthis file\n into IntelliJ IDEA for Aqueduct file and code templates.\n\n\nHow to Use this Documentation\n\n\nEach topic covers a major component of the Aqueduct framework and are displayed in the side menu. Within each topic, there is an overview page and a number of guides. Each guide contains example code, explanations and best practices for building Aqueduct applications.\n\n\nGuides create an initial understanding and give context to the Aqueduct framework. The API reference details each type, property and method in the Aqueduct framework. The tutorial is a guided exercise that teaches the very basics of Aqueduct while creating a server application.", 
            "title": "Home"
        }, 
        {
            "location": "/#important-links", 
            "text": "Getting Started and Installation  Tutorial .  API Reference  (or you can install it in  Dash ).  Example Repository  Check out  Snippets  for quick code snippets to get you up and running faster.  Import  this file  into IntelliJ IDEA for Aqueduct file and code templates.", 
            "title": "Important Links"
        }, 
        {
            "location": "/#how-to-use-this-documentation", 
            "text": "Each topic covers a major component of the Aqueduct framework and are displayed in the side menu. Within each topic, there is an overview page and a number of guides. Each guide contains example code, explanations and best practices for building Aqueduct applications.  Guides create an initial understanding and give context to the Aqueduct framework. The API reference details each type, property and method in the Aqueduct framework. The tutorial is a guided exercise that teaches the very basics of Aqueduct while creating a server application.", 
            "title": "How to Use this Documentation"
        }, 
        {
            "location": "/getting_started/", 
            "text": "Getting Started with Aqueduct\n\n\nInstallation\n\n\n\n\nInstall Dart\n.\n\n\n\n\nActivate the Aqueduct CLI\n\n\npub global activate aqueduct\n\n\n\n\n\n\n\n\n\nCreate a new project.\n\n\naqueduct create my_project\n\n\n\n\n\n\n\n\n\nOpen the project directory in an \nIntelliJ IDE\n, \nAtom\n or \nVisual Studio Code\n. All three IDEs have a Dart plugin.\n\n\nHow to Learn Aqueduct\n\n\nThere are different approaches depending on how you prefer to learn.\n\n\n\n\nThe \nguided tutorial\n is a hands-on walkthrough where you build an application while learning basic Aqueduct concepts.\n\n\nThe \nexample repository\n contains a few deployable applications that you may review or tinker with.\n\n\nThe guides (located in the menu on this website) dive deeply into the concepts of Aqueduct and show example code.\n\n\nCreating a new project\n and using the \nAPI reference\n to jump right in.\n\n\n\n\nIt is best to first understand how HTTP requests are responded to - the foundation of Aqueduct - before moving on to topics such as the ORM and OAuth 2.0. Both the tutorial and the \nHTTP guides\n are the primary source of this information. A project created by the \naqueduct\n tool has example routes connected for modification, too.\n\n\nCreating a Project\n\n\nThe \naqueduct create\n command-line tool creates new Aqueduct project directories. The default template contains the minimal project structure for running an Aqueduct application. A project name must be snake_case.\n\n\naqueduct create my_project_name\n\n\n\n\n\nOther templates exist that contain foundational code for using Aqueduct's ORM and OAuth 2.0 implementation. These templates can be listed:\n\n\naqueduct create list-templates\n\n\n\n\n\nYou may provide the name of a template when creating a project to use that template:\n\n\naqueduct create -t db my_project_name\n\n\n\n\n\nUsing the Aqueduct ORM\n\n\nAqueduct's ORM uses PostgreSQL. During development, you will need a locally running instance of PostgreSQL. On macOS, installing  \nPostgres.app\n is a very convenient way to run PostgreSQL locally. For other platforms, see \nPostgreSQL's downloads page\n.\n\n\nWhen creating a project, use the \ndb\n template. If adding to an existing project, see \nthis guide\n.\n\n\nTo create a database, make sure PostgreSQL is running and open the command-line utility to connect to it.\n\n\npsql\n\n\n\n\n\nThen, create a database that your application will connect to and a user that it will connect with:\n\n\nCREATE\n \nDATABASE\n \nmy_database_name\n;\n\n\nCREATE\n \nUSER\n \ndart_app\n \nWITH\n \nPASSWORD\n \ndart\n;\n\n\nGRANT\n \nALL\n \nON\n \nDATABASE\n \nmy_database_name\n \nTO\n \ndart_app\n;\n\n\n\n\n\n\nAn application must create a \nManagedContext\n that connects to this database:\n\n\nclass\n \nMyProjectSink\n \nextends\n \nRequestSink\n \n{\n\n  \nMyProjectSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n    \nvar\n \ndataModel\n \n=\n \nnew\n \nManagedDataModel\n.\nfromCurrentMirrorSystem\n();\n\n    \nvar\n \nstore\n \n=\n \nnew\n \nPostgreSQLPersistentStore\n.\nfromConnectionInfo\n(\n\n      \ndart_app\n,\n \ndart\n,\n \nlocalhost\n,\n \n5432\n,\n \nmy_database_name\n);\n\n    \nManagedContext\n.\ndefaultContext\n \n=\n \nnew\n \nManagedContext\n(\ndataModel\n,\n \nstore\n);\n\n  \n}\n\n\n  \n...\n\n\n}\n\n\n\n\n\n\nOnce you have declared \nManagedObject\ns in your application, generate the database schema by generating and executing migrations from your project's directory:\n\n\naqueduct db generate\naqueduct db upgrade --connect postgres://dart_app:dart@localhost:5432/my_database_name\n\n\n\n\n\nSee the guides on \nconnecting to a database\n and \ntesting with a database", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting_started/#getting-started-with-aqueduct", 
            "text": "", 
            "title": "Getting Started with Aqueduct"
        }, 
        {
            "location": "/getting_started/#installation", 
            "text": "Install Dart .   Activate the Aqueduct CLI  pub global activate aqueduct    Create a new project.  aqueduct create my_project    Open the project directory in an  IntelliJ IDE ,  Atom  or  Visual Studio Code . All three IDEs have a Dart plugin.", 
            "title": "Installation"
        }, 
        {
            "location": "/getting_started/#how-to-learn-aqueduct", 
            "text": "There are different approaches depending on how you prefer to learn.   The  guided tutorial  is a hands-on walkthrough where you build an application while learning basic Aqueduct concepts.  The  example repository  contains a few deployable applications that you may review or tinker with.  The guides (located in the menu on this website) dive deeply into the concepts of Aqueduct and show example code.  Creating a new project  and using the  API reference  to jump right in.   It is best to first understand how HTTP requests are responded to - the foundation of Aqueduct - before moving on to topics such as the ORM and OAuth 2.0. Both the tutorial and the  HTTP guides  are the primary source of this information. A project created by the  aqueduct  tool has example routes connected for modification, too.", 
            "title": "How to Learn Aqueduct"
        }, 
        {
            "location": "/getting_started/#creating-a-project", 
            "text": "The  aqueduct create  command-line tool creates new Aqueduct project directories. The default template contains the minimal project structure for running an Aqueduct application. A project name must be snake_case.  aqueduct create my_project_name  Other templates exist that contain foundational code for using Aqueduct's ORM and OAuth 2.0 implementation. These templates can be listed:  aqueduct create list-templates  You may provide the name of a template when creating a project to use that template:  aqueduct create -t db my_project_name", 
            "title": "Creating a Project"
        }, 
        {
            "location": "/getting_started/#using-the-aqueduct-orm", 
            "text": "Aqueduct's ORM uses PostgreSQL. During development, you will need a locally running instance of PostgreSQL. On macOS, installing   Postgres.app  is a very convenient way to run PostgreSQL locally. For other platforms, see  PostgreSQL's downloads page .  When creating a project, use the  db  template. If adding to an existing project, see  this guide .  To create a database, make sure PostgreSQL is running and open the command-line utility to connect to it.  psql  Then, create a database that your application will connect to and a user that it will connect with:  CREATE   DATABASE   my_database_name ;  CREATE   USER   dart_app   WITH   PASSWORD   dart ;  GRANT   ALL   ON   DATABASE   my_database_name   TO   dart_app ;   An application must create a  ManagedContext  that connects to this database:  class   MyProjectSink   extends   RequestSink   { \n   MyProjectSink ( ApplicationConfiguration   config )   :   super ( config )   { \n     var   dataModel   =   new   ManagedDataModel . fromCurrentMirrorSystem (); \n     var   store   =   new   PostgreSQLPersistentStore . fromConnectionInfo ( \n       dart_app ,   dart ,   localhost ,   5432 ,   my_database_name ); \n     ManagedContext . defaultContext   =   new   ManagedContext ( dataModel ,   store ); \n   } \n\n   ...  }   Once you have declared  ManagedObject s in your application, generate the database schema by generating and executing migrations from your project's directory:  aqueduct db generate\naqueduct db upgrade --connect postgres://dart_app:dart@localhost:5432/my_database_name  See the guides on  connecting to a database  and  testing with a database", 
            "title": "Using the Aqueduct ORM"
        }, 
        {
            "location": "/tour/", 
            "text": "Aqueduct: A Tour\n\n\nCreate applications with the \naqueduct\n tool:\n\n\naqueduct create my_app\n\n\n\n\n\nRun applications by using the \naqueduct\n tool in a project directory:\n\n\naqueduct serve\n\n\n\n\n\nStructure\n\n\nAn Aqueduct application is a series of controllers that form a \nchannel\n for a request to flow through. Any of those controllers may respond to a request and take it out of the channel. Controllers in the middle of the channel often verify something, while the controller at the end fulfills the request. Fulfillment might mean returning the contents of a file or storing data from the request body in a database.\n\n\nInitialization\n\n\nAn application's channel is created by subclassing \nRequestSink\n. This type also performs any other application initialization, like creating database connections and defining how authorization occurs.\n\n\nimport\n \npackage:aqueduct/aqueduct.dart\n;\n\n\n\nclass\n \nAppRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nAppRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n    \ndatabaseContext\n \n=\n \ncontextFrom\n(\nconfig\n);\n\n  \n}\n\n\n  \n@\noverride\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n    \nrouter\n\n      \n.\nroute\n(\n/resource/[:id]\n)\n\n      \n.\ngenerate\n(()\n \n=\n \nnew\n \nResourceController\n(\ndatabaseContext\n));\n\n  \n}\n\n\n}\n\n\n\n\n\n\nRouting\n\n\nA \nrouter\n splits a channel into sub-channels based on the path of a request. A request with the path \n/users\n will be handled by a different controller than a request with the path \n/posts\n, for example. Routes are defined by \nroute specification syntax\n. Routes can contain variables and optional segments, enabling routes to be grouped together.\n\n\n@\noverride\n\n\nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n    \n  \nrouter\n\n    \n.\nroute\n(\n/users/[:id]\n)\n\n    \n.\ngenerate\n(()\n \n=\n \nnew\n \nUserController\n());\n\n\n  \nrouter\n\n    \n.\nroute\n(\n/file/*\n)\n\n    \n.\ngenerate\n(()\n \n=\n \nnew\n \nHTTPFileController\n());\n\n\n  \nrouter\n\n    \n.\nroute\n(\n/health\n)\n\n    \n.\nlisten\n((\nreq\n)\n \nasync\n \n=\n \nnew\n \nResponse\n.\nok\n(\nnull\n));\n\n\n}\n    \n\n\n\n\n\nControllers\n\n\nHTTPController\n are the controller that most often fulfill a request. An \nHTTPController\n subclass handles all operations for resource, e.g. \nPOST /users\n, \nGET /users\n and \nGET /users/1\n.\n\n\nSubclasses implement a \nresponder method\n for each operation:\n\n\nimport\n \npackage:aqueduct/aqueduct.dart\n\n\n\nclass\n \nResourceController\n \nextends\n \nHTTPController\n \n{\n\n  \n@\nhttpGet\n\n  \nFuture\nResponse\n \ngetAllResources\n()\n \nasync\n \n{\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\nawait\n \nfetchResources\n())\n;\n\n  \n}\n\n\n  \n@\nhttpGet\n\n  \nFuture\nResponse\n \ngetResourceByID\n(\n@\nHTTPPath\n(\nid\n)\n \nint\n \nid\n)\n \nasync\n \n{\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\nawait\n \nfetchResource\n(\nid\n));\n\n  \n}\n\n\n  \n@\nhttpPost\n\n  \nFuture\nResponse\n \ncreateResource\n(\n@\nHTTPBody\n()\n \nResource\n \nresource\n)\n \nasync\n \n{\n\n    \nvar\n \ninserted\n \n=\n \nawait\n \ninsertResource\n(\nresource\n);\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\ninserted\n);\n\n  \n}\n\n\n}\n\n\n\n\n\n\nProperties of the request are bound to responder method arguments and controller properties:\n\n\nclass\n \nResourceController\n \nextends\n \nHTTPController\n \n{\n\n  \n@\nhttpGet\n\n  \nFuture\nResponse\n \ngetAllResources\n(\n\n      \n@\nHTTPHeader\n(\nx-request-id\n)\n \nString\n \nrequestID\n,\n\n      \n{\n@\nHTTPQuery\n(\nlimit\n)\n \nint\n \nlimit\n})\n \nasync\n \n{\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\nawait\n \nfetchResources\n(\nlimit\n \n??\n \n0\n));\n\n  \n}\n\n\n  \n@\nhttpPost\n\n  \nFuture\nResponse\n \ncreateResource\n(\n@\nHTTPBody\n()\n \nResource\n \nresource\n)\n \nasync\n \n{\n\n    \nvar\n \ninserted\n \n=\n \nawait\n \ninsertResourceIntoDatabase\n(\nresource\n);\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\ninserted\n);\n\n  \n}\n\n\n}\n\n\n\n\n\n\nManagedObjectController\nT\ns are \nHTTPController\ns that automatically map a REST interface to database queries:\n\n\nrouter\n\n  \n.\nroute\n(\n/users/[:id]\n)\n\n  \n.\ngenerate\n(()\n \n=\n \nnew\n \nManagedObjectController\nUser\n());\n\n\n\n\n\n\nRequestController\n is the base class for all controllers that form a channel. They only have a single method to handle the request, and must either return the request or a response. When a request controller returns a response, the request is taken out of the channel.\n\n\nclass\n \nVerifyingController\n \nextends\n \nRequestController\n \n{\n\n  \n@\noverride\n\n  \nFuture\nRequestOrResponse\n \nprocessRequest\n(\nRequest\n \nrequest\n)\n \nasync\n \n{\n\n    \nif\n \n(\nrequest\n.\ninnerRequest\n.\nheaders\n.\nvalue\n(\nx-secret-key\n)\n \n==\n \nsecret!\n)\n \n{\n\n      \nreturn\n \nrequest\n;\n\n    \n}\n\n\n    \nreturn\n \nnew\n \nResponse\n.\nbadRequest\n();\n\n  \n}\n\n\n}\n\n\n\n\n\n\nThis behavior lets a channel prevent invalid requests from being fulfilled, or let's a controller be reused in multiple places to provide some preprocessing step.\n\n\nUncaught exceptions are caught by the controller and translated into an appropriate response, removing the request from the channel. Exceptions should only be caught when another response is desired or when the request should continue to the next controller in the channel.\n\n\nConfiguration\n\n\nRead YAML configuration data into type-safe and name-safe structures at startup:\n\n\n// config.yaml\ndatabase:\n  host: ...\n  port: 5432\n  databaseName: foo\notherOption: hello\nnumberOfDoodads: 3  \n\n\n\n\n\nSubclass \nConfigurationItem\n and declare a property for each key in the configuration file:\n\n\nclass\n \nAppOptions\n \nextends\n \nConfigurationItem\n \n{\n\n  \nAppOptions\n(\nString\n \npath\n)\n \n:\n \nsuper\n.\nfromFile\n(\npath\n);\n\n\n  \nDatabaseConnectionInfo\n \ndatabase\n;\n\n  \nString\n \notherOption\n;\n\n  \nint\n \nnumberOfDoodads\n;\n\n\n}\n\n\n\n\n\n\nRead the configuration file identified by an \nApplicationConfiguration\n:\n\n\nimport\n \npackage:aqueduct/aqueduct.dart\n;\n\n\n\nclass\n \nAppRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nAppRequestSink\n(\nApplicationConfig\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n    \nvar\n \noptions\n \n=\n \nnew\n \nAppOptions\n(\nconfig\n.\nconfigurationFilePath\n);\n\n    \n...\n\n  \n}\n\n\n}\n\n\n\n\n\n\nRunning and Concurrency\n\n\nAqueduct applications are run with the \naqueduct serve\n command line tool, which can also open debugging and instrumentation tools and specify how many threads the application should run on:\n\n\naqueduct serve --observe --isolates 5\n\n\n\n\n\nRun applications detached or still connected to the shell:\n\n\naqueduct serve --detached --port $PORT\n\n\n\n\n\nAqueduct applications are multi-isolate (multi-threaded). Each isolate runs a replica of the same web server with its own set of services like database connections. This makes behavior like database connection pooling implicit.\n\n\nQuerying a Database\n\n\nDatabase operations are built and executed with instances of \nQuery\nT\n.\n\n\nimport\n \npackage:aqueduct/aqueduct.dart\n\n\n\nclass\n \nResourceController\n \nextends\n \nHTTPController\n \n{\n\n  \n@\nhttpGet\n\n  \nFuture\nResponse\n \ngetAllResources\n()\n \nasync\n \n{\n\n    \nvar\n \nquery\n \n=\n \nnew\n \nQuery\nResource\n()\n;\n\n\n    \nvar\n \nresults\n \n=\n \nawait\n \nquery\n.\nfetch\n();\n\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\nresults\n);\n\n  \n}\n\n\n}\n\n\n\n\n\n\nThe results can be filtered by the \nQuery.where\n property, which has the same properties as the object being queried.\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nEmployee\n()\n\n  \n..\nwhere\n.\nname\n \n=\n \nwhereStartsWith\n(\nSa\n)\n\n  \n..\nwhere\n.\nsalary\n \n=\n \nwhereGreaterThan\n(\n50000\n);\n\n\nvar\n \nresults\n \n=\n \nawait\n \nquery\n.\nfetch\n();\n\n\n\n\n\n\nValues set on the properties of \nQuery.values\n are sent to the database on insert and update operations. Like \nQuery.where\n, \nQuery.values\n has the same properties as the object being inserted or updated.\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nEmployee\n()\n\n  \n..\nvalues\n.\nname\n \n=\n \nBob\n\n  \n..\nvalues\n.\nsalary\n \n=\n \n50000\n;\n\n\n\nvar\n \nbob\n \n=\n \nawait\n \nquery\n.\ninsert\n();\n  \n\n\nvar\n \nupdateQuery\n \n=\n \nnew\n \nQuery\nEmployee\n()\n\n  \n..\nwhere\n.\nid\n \n=\n \nbob\n.\nid\n\n  \n..\nvalues\n.\nname\n \n=\n \nBobby\n;\n\n\nbob\n \n=\n \nawait\n \nupdateQuery\n.\nupdateOne\n();\n  \n\n\n\n\n\nQuery\nT\ns can sort and page on a result set. It can also join tables and return objects and their relationships:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nEmployee\n()\n\n  \n..\nwhere\n.\nname\n \n=\n \nSue Gallagher\n\n  \n..\njoin\n(\nobject:\n \n(\ne\n)\n \n=\n \ne\n.\nmanager\n)\n\n  \n..\njoin\n(\nset\n:\n \n(\ne\n)\n \n=\n \ne\n.\ndirectReports\n);\n\n\n\nvar\n \nherAndHerManagerAndHerDirectReports\n \n=\n \nawait\n \nquery\n.\nfetchOne\n();\n\n\n\n\n\n\nExceptions thrown for queries are caught by a controller and translated into the appropriate status code. Unique constraint conflicts return 409,\nmissing required properties return 400 and database connection failure returns 503.\n\n\nDefining a Data Model\n\n\nManagedObject\nT\n instances represent a row in a database; each property is a column in the corresponding table. This class is always subclassed and is in fact made up of two classes:\n\n\nclass\n \nEmployee\n \nextends\n \nManagedObject\n_Employee\n \nimplements\n \n_Employee\n \n{\n\n  \nbool\n \nget\n \nwasRecentlyHired\n \n=\n \nhireDate\n.\ndifference\n(\nnew\n \nDateTime\n.\nnow\n()).\ninDays\n \n \n30\n;\n\n\n}\n\n\nclass\n \n_Employee\n  \n{\n\n  \n@\nmanagedPrimaryKey\n\n  \nint\n \nindex\n;\n\n\n  \n@\nManagedColumnAttributes\n(\nindexed:\n \ntrue\n)\n\n  \nString\n \nname\n;\n\n\n  \nDateTime\n \nhireDate\n;\n\n  \nint\n \nsalary\n;\n\n\n}\n\n\n\n\n\n\nManagedObject\nT\ns have relationship properties for has-one, has-many and many-to-many references to other \nManagedObject\nT\ns. The property with \nManagedRelationship\n metadata is a foreign key column.\n\n\nclass\n \nEmployee\n \nextends\n \nManagedObject\n_Employee\n \nimplements\n \n_Employee\n \n{}\n\n\nclass\n \n_Employee\n \n{\n\n  \n...\n\n\n  \nManagedSet\nInitiative\n \ninitiatives\n;\n\n\n}\n\n\n\nclass\n \nInitiative\n \nextends\n \nManagedObject\n_Initiative\n \nimplements\n \n_Initiative\n \n{}\n\n\nclass\n \n_Initiative\n \n{\n\n  \n...\n\n\n  \n@\nManagedRelationship\n(\n#\ninitiatives\n)\n\n  \nEmployee\n \nleader\n;\n\n\n}\n\n\n\n\n\n\nManagedObject\nT\ns are easily read from and written to JSON (or any other format):\n\n\nclass\n \nUserController\n \nextends\n \nHTTPController\n \n{\n\n  \n@\nhttpPut\n\n  \nFuture\nResponse\n \nupdateUser\n(\n@\nHTTPPath\n(\nid\n)\n \nint\n \nid\n,\n \n@\nHTTPBody\n()\n \nUser\n \nuser\n)\n \nasync\n \n{\n\n    \nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n      \n..\nwhere\n.\nid\n \n=\n \nid\n\n      \n..\nvalues\n \n=\n \nuser\n;\n\n\n    \nvar\n \nupdatedUser\n \n=\n \nawait\n \nquery\n.\nupdateOne\n();\n\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\nupdatedUser\n);\n\n  \n}\n\n\n}\n\n\n\n\n\n\nAutomatic Database Migration\n\n\nGenerate and run database migrations with the \naqueduct db\n tool:\n\n\naqueduct db generate\naqueduct db validate\naqueduct db upgrade --connect postgres@://...\n\n\n\n\n\nOAuth 2.0\n\n\nAuthentication and authorization are enabled at application startup by creating an \nAuthServer\n with \nManagedAuthStorage\n:\n\n\nimport\n \npackage:aqueduct/aqueduct.dart\n;\n\n\nimport\n \npackage:aqueduct/managed_auth.dart\n;\n\n\n\nclass\n \nAppRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nAppRequestSink\n(\nApplicationConfig\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n    \nvar\n \nstorage\n \n=\n \nnew\n \nManagedAuthStorage\nUser\n(\nManagedContext\n.\ndefaultContext\n);\n\n    \nauthServer\n \n=\n \nnew\n \nAuthServer\n(\nstorage\n);\n\n  \n}\n\n\n  \nAuthServer\n \nauthServer\n;\n\n\n}\n\n\n\n\n\n\nSet up routes to exchange credentials for tokens using \nAuthController\n and \nAuthCodeController\n. Add \nAuthorizer\ns between routes and their controller to restrict access to authorized resource owners only:\n\n\nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n  \nrouter\n\n    \n.\nroute\n(\n/auth/token\n)\n\n    \n.\ngenerate\n(()\n \n=\n \nnew\n \nAuthController\n(\nauthServer\n));\n\n\n  \nrouter\n\n    \n.\nroute\n(\n/auth/code\n)\n\n    \n.\ngenerate\n(()\n \n=\n \nnew\n \nAuthCodeController\n(\nauthServer\n));\n\n\n  \nrouter\n\n    \n.\nroute\n(\n/protected\n)\n\n    \n.\npipe\n(\nnew\n \nAuthorizer\n.\nbearer\n(\nauthServer\n))\n\n    \n.\ngenerate\n(()\n \n=\n \nnew\n \nProtectedController\n());\n\n\n}\n\n\n\n\n\n\nInsert OAuth 2.0 clients into a database:\n\n\naqueduct auth add-client --id com.app.mobile --secret foobar --redirect-uri https://somewhereoutthere.com\n\n\n\n\n\nLogging\n\n\nAll requests are logged to an instance of \nLogger\n. Set up a listener for logger in \nRequestSink\n to print log messages to the console. (See also \nscribe\n for logging to rotating files.)\n\n\nclass\n \nWildfireSink\n \nextends\n \nRequestSink\n \n{\n\n  \nWildfireSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n    \nlogger\n.\nonRecord\n.\nlisten\n((\nrecord\n)\n \n{\n\n      \nprint\n(\n$\nrecord\n);\n\n    \n});\n\n  \n}\n\n\n}\n\n\n\n\n\n\nTesting\n\n\nTests are run by starting the Aqueduct application and verifying responses in a test file. A test harness is included in projects generated from \naqueduct create\n that starts and stops a test instance of your application and uploads your database schema to a temporary, local database.\n\n\nimport\n \nharness/app.dart\n;\n\n\n\nvoid\n \nmain\n()\n \n{\n\n  \nvar\n \napp\n \n=\n \nnew\n \nTestApplication\n();\n\n\n  \nsetUpAll\n(()\n \nasync\n \n{\n\n    \nawait\n \napp\n.\nstart\n();\n\n  \n});\n\n\n  \ntest\n(\n...\n,\n \n()\n \nasync\n \n{\n\n    \nvar\n \nresponse\n \n=\n \nawait\n \napp\n.\nclient\n.\nrequest\n(\n/endpoint\n).\nget\n();\n\n    \n...\n\n  \n});\n\n\n}\n\n\n\n\n\n\nA \nTestClient\n executes requests configured for the locally running test instance of your application. Instances of \nTestResponse\n are returned and can be evaluated with matchers like any other Dart tests. There are special matchers specifically for Aqueduct.\n\n\ntest\n(\nPOST /users creates a user\n,\n \n()\n \nasync\n \n{\n\n  \nvar\n \nrequest\n \n=\n \napp\n.\nclient\n.\nrequest\n(\n/users\n)\n\n    \n..\njson\n \n=\n \n{\nemail\n:\n \nbob@stablekernel.com\n};\n\n  \nvar\n \nresponse\n \n=\n \nawait\n \nrequest\n.\npost\n();\n\n\n  \nexpect\n(\nresponse\n,\n \nhasResponse\n(\n200\n,\n \n{\n\n    \nid\n:\n \nisNumber\n,\n\n    \nemail\n:\n \nbob@stablekernel.com\n\n  \n}));\n\n\n});\n\n\n\ntest\n(\nGET /users/1 returns a user\n,\n \n()\n \nasync\n \n{\n\n  \nvar\n \nresponse\n \n=\n \nawait\n \napp\n.\nclient\n.\nauthenticatedRequest\n(\n/users/1\n).\nget\n();\n\n  \nexpect\n(\nresponse\n,\n \nhasResponse\n(\n200\n,\n \npartial\n({\n\n    \nemail\n:\n \nbob@stablekernel.com\n\n  \n})));\n\n\n});\n\n\n\n\n\n\nDocumentation\n\n\nGenerate OpenAPI specifications automatically:\n\n\naqueduct document", 
            "title": "Tour"
        }, 
        {
            "location": "/tour/#aqueduct-a-tour", 
            "text": "Create applications with the  aqueduct  tool:  aqueduct create my_app  Run applications by using the  aqueduct  tool in a project directory:  aqueduct serve", 
            "title": "Aqueduct: A Tour"
        }, 
        {
            "location": "/tour/#structure", 
            "text": "An Aqueduct application is a series of controllers that form a  channel  for a request to flow through. Any of those controllers may respond to a request and take it out of the channel. Controllers in the middle of the channel often verify something, while the controller at the end fulfills the request. Fulfillment might mean returning the contents of a file or storing data from the request body in a database.", 
            "title": "Structure"
        }, 
        {
            "location": "/tour/#initialization", 
            "text": "An application's channel is created by subclassing  RequestSink . This type also performs any other application initialization, like creating database connections and defining how authorization occurs.  import   package:aqueduct/aqueduct.dart ;  class   AppRequestSink   extends   RequestSink   { \n   AppRequestSink ( ApplicationConfiguration   config )   :   super ( config )   { \n     databaseContext   =   contextFrom ( config ); \n   } \n\n   @ override \n   void   setupRouter ( Router   router )   { \n     router \n       . route ( /resource/[:id] ) \n       . generate (()   =   new   ResourceController ( databaseContext )); \n   }  }", 
            "title": "Initialization"
        }, 
        {
            "location": "/tour/#routing", 
            "text": "A  router  splits a channel into sub-channels based on the path of a request. A request with the path  /users  will be handled by a different controller than a request with the path  /posts , for example. Routes are defined by  route specification syntax . Routes can contain variables and optional segments, enabling routes to be grouped together.  @ override  void   setupRouter ( Router   router )   {     \n   router \n     . route ( /users/[:id] ) \n     . generate (()   =   new   UserController ()); \n\n   router \n     . route ( /file/* ) \n     . generate (()   =   new   HTTPFileController ()); \n\n   router \n     . route ( /health ) \n     . listen (( req )   async   =   new   Response . ok ( null ));  }", 
            "title": "Routing"
        }, 
        {
            "location": "/tour/#controllers", 
            "text": "HTTPController  are the controller that most often fulfill a request. An  HTTPController  subclass handles all operations for resource, e.g.  POST /users ,  GET /users  and  GET /users/1 .  Subclasses implement a  responder method  for each operation:  import   package:aqueduct/aqueduct.dart  class   ResourceController   extends   HTTPController   { \n   @ httpGet \n   Future Response   getAllResources ()   async   { \n     return   new   Response . ok ( await   fetchResources ()) ; \n   } \n\n   @ httpGet \n   Future Response   getResourceByID ( @ HTTPPath ( id )   int   id )   async   { \n     return   new   Response . ok ( await   fetchResource ( id )); \n   } \n\n   @ httpPost \n   Future Response   createResource ( @ HTTPBody ()   Resource   resource )   async   { \n     var   inserted   =   await   insertResource ( resource ); \n     return   new   Response . ok ( inserted ); \n   }  }   Properties of the request are bound to responder method arguments and controller properties:  class   ResourceController   extends   HTTPController   { \n   @ httpGet \n   Future Response   getAllResources ( \n       @ HTTPHeader ( x-request-id )   String   requestID , \n       { @ HTTPQuery ( limit )   int   limit })   async   { \n     return   new   Response . ok ( await   fetchResources ( limit   ??   0 )); \n   } \n\n   @ httpPost \n   Future Response   createResource ( @ HTTPBody ()   Resource   resource )   async   { \n     var   inserted   =   await   insertResourceIntoDatabase ( resource ); \n     return   new   Response . ok ( inserted ); \n   }  }   ManagedObjectController T s are  HTTPController s that automatically map a REST interface to database queries:  router \n   . route ( /users/[:id] ) \n   . generate (()   =   new   ManagedObjectController User ());   RequestController  is the base class for all controllers that form a channel. They only have a single method to handle the request, and must either return the request or a response. When a request controller returns a response, the request is taken out of the channel.  class   VerifyingController   extends   RequestController   { \n   @ override \n   Future RequestOrResponse   processRequest ( Request   request )   async   { \n     if   ( request . innerRequest . headers . value ( x-secret-key )   ==   secret! )   { \n       return   request ; \n     } \n\n     return   new   Response . badRequest (); \n   }  }   This behavior lets a channel prevent invalid requests from being fulfilled, or let's a controller be reused in multiple places to provide some preprocessing step.  Uncaught exceptions are caught by the controller and translated into an appropriate response, removing the request from the channel. Exceptions should only be caught when another response is desired or when the request should continue to the next controller in the channel.", 
            "title": "Controllers"
        }, 
        {
            "location": "/tour/#configuration", 
            "text": "Read YAML configuration data into type-safe and name-safe structures at startup:  // config.yaml\ndatabase:\n  host: ...\n  port: 5432\n  databaseName: foo\notherOption: hello\nnumberOfDoodads: 3    Subclass  ConfigurationItem  and declare a property for each key in the configuration file:  class   AppOptions   extends   ConfigurationItem   { \n   AppOptions ( String   path )   :   super . fromFile ( path ); \n\n   DatabaseConnectionInfo   database ; \n   String   otherOption ; \n   int   numberOfDoodads ;  }   Read the configuration file identified by an  ApplicationConfiguration :  import   package:aqueduct/aqueduct.dart ;  class   AppRequestSink   extends   RequestSink   { \n   AppRequestSink ( ApplicationConfig   config )   :   super ( config )   { \n     var   options   =   new   AppOptions ( config . configurationFilePath ); \n     ... \n   }  }", 
            "title": "Configuration"
        }, 
        {
            "location": "/tour/#running-and-concurrency", 
            "text": "Aqueduct applications are run with the  aqueduct serve  command line tool, which can also open debugging and instrumentation tools and specify how many threads the application should run on:  aqueduct serve --observe --isolates 5  Run applications detached or still connected to the shell:  aqueduct serve --detached --port $PORT  Aqueduct applications are multi-isolate (multi-threaded). Each isolate runs a replica of the same web server with its own set of services like database connections. This makes behavior like database connection pooling implicit.", 
            "title": "Running and Concurrency"
        }, 
        {
            "location": "/tour/#querying-a-database", 
            "text": "Database operations are built and executed with instances of  Query T .  import   package:aqueduct/aqueduct.dart  class   ResourceController   extends   HTTPController   { \n   @ httpGet \n   Future Response   getAllResources ()   async   { \n     var   query   =   new   Query Resource () ; \n\n     var   results   =   await   query . fetch (); \n\n     return   new   Response . ok ( results ); \n   }  }   The results can be filtered by the  Query.where  property, which has the same properties as the object being queried.  var   query   =   new   Query Employee () \n   .. where . name   =   whereStartsWith ( Sa ) \n   .. where . salary   =   whereGreaterThan ( 50000 );  var   results   =   await   query . fetch ();   Values set on the properties of  Query.values  are sent to the database on insert and update operations. Like  Query.where ,  Query.values  has the same properties as the object being inserted or updated.  var   query   =   new   Query Employee () \n   .. values . name   =   Bob \n   .. values . salary   =   50000 ;  var   bob   =   await   query . insert ();    var   updateQuery   =   new   Query Employee () \n   .. where . id   =   bob . id \n   .. values . name   =   Bobby ;  bob   =   await   updateQuery . updateOne ();     Query T s can sort and page on a result set. It can also join tables and return objects and their relationships:  var   query   =   new   Query Employee () \n   .. where . name   =   Sue Gallagher \n   .. join ( object:   ( e )   =   e . manager ) \n   .. join ( set :   ( e )   =   e . directReports );  var   herAndHerManagerAndHerDirectReports   =   await   query . fetchOne ();   Exceptions thrown for queries are caught by a controller and translated into the appropriate status code. Unique constraint conflicts return 409,\nmissing required properties return 400 and database connection failure returns 503.", 
            "title": "Querying a Database"
        }, 
        {
            "location": "/tour/#defining-a-data-model", 
            "text": "ManagedObject T  instances represent a row in a database; each property is a column in the corresponding table. This class is always subclassed and is in fact made up of two classes:  class   Employee   extends   ManagedObject _Employee   implements   _Employee   { \n   bool   get   wasRecentlyHired   =   hireDate . difference ( new   DateTime . now ()). inDays     30 ;  }  class   _Employee    { \n   @ managedPrimaryKey \n   int   index ; \n\n   @ ManagedColumnAttributes ( indexed:   true ) \n   String   name ; \n\n   DateTime   hireDate ; \n   int   salary ;  }   ManagedObject T s have relationship properties for has-one, has-many and many-to-many references to other  ManagedObject T s. The property with  ManagedRelationship  metadata is a foreign key column.  class   Employee   extends   ManagedObject _Employee   implements   _Employee   {}  class   _Employee   { \n   ... \n\n   ManagedSet Initiative   initiatives ;  }  class   Initiative   extends   ManagedObject _Initiative   implements   _Initiative   {}  class   _Initiative   { \n   ... \n\n   @ ManagedRelationship ( # initiatives ) \n   Employee   leader ;  }   ManagedObject T s are easily read from and written to JSON (or any other format):  class   UserController   extends   HTTPController   { \n   @ httpPut \n   Future Response   updateUser ( @ HTTPPath ( id )   int   id ,   @ HTTPBody ()   User   user )   async   { \n     var   query   =   new   Query User () \n       .. where . id   =   id \n       .. values   =   user ; \n\n     var   updatedUser   =   await   query . updateOne (); \n\n     return   new   Response . ok ( updatedUser ); \n   }  }", 
            "title": "Defining a Data Model"
        }, 
        {
            "location": "/tour/#automatic-database-migration", 
            "text": "Generate and run database migrations with the  aqueduct db  tool:  aqueduct db generate\naqueduct db validate\naqueduct db upgrade --connect postgres@://...", 
            "title": "Automatic Database Migration"
        }, 
        {
            "location": "/tour/#oauth-20", 
            "text": "Authentication and authorization are enabled at application startup by creating an  AuthServer  with  ManagedAuthStorage :  import   package:aqueduct/aqueduct.dart ;  import   package:aqueduct/managed_auth.dart ;  class   AppRequestSink   extends   RequestSink   { \n   AppRequestSink ( ApplicationConfig   config )   :   super ( config )   { \n     var   storage   =   new   ManagedAuthStorage User ( ManagedContext . defaultContext ); \n     authServer   =   new   AuthServer ( storage ); \n   } \n\n   AuthServer   authServer ;  }   Set up routes to exchange credentials for tokens using  AuthController  and  AuthCodeController . Add  Authorizer s between routes and their controller to restrict access to authorized resource owners only:  void   setupRouter ( Router   router )   { \n   router \n     . route ( /auth/token ) \n     . generate (()   =   new   AuthController ( authServer )); \n\n   router \n     . route ( /auth/code ) \n     . generate (()   =   new   AuthCodeController ( authServer )); \n\n   router \n     . route ( /protected ) \n     . pipe ( new   Authorizer . bearer ( authServer )) \n     . generate (()   =   new   ProtectedController ());  }   Insert OAuth 2.0 clients into a database:  aqueduct auth add-client --id com.app.mobile --secret foobar --redirect-uri https://somewhereoutthere.com", 
            "title": "OAuth 2.0"
        }, 
        {
            "location": "/tour/#logging", 
            "text": "All requests are logged to an instance of  Logger . Set up a listener for logger in  RequestSink  to print log messages to the console. (See also  scribe  for logging to rotating files.)  class   WildfireSink   extends   RequestSink   { \n   WildfireSink ( ApplicationConfiguration   config )   :   super ( config )   { \n     logger . onRecord . listen (( record )   { \n       print ( $ record ); \n     }); \n   }  }", 
            "title": "Logging"
        }, 
        {
            "location": "/tour/#testing", 
            "text": "Tests are run by starting the Aqueduct application and verifying responses in a test file. A test harness is included in projects generated from  aqueduct create  that starts and stops a test instance of your application and uploads your database schema to a temporary, local database.  import   harness/app.dart ;  void   main ()   { \n   var   app   =   new   TestApplication (); \n\n   setUpAll (()   async   { \n     await   app . start (); \n   }); \n\n   test ( ... ,   ()   async   { \n     var   response   =   await   app . client . request ( /endpoint ). get (); \n     ... \n   });  }   A  TestClient  executes requests configured for the locally running test instance of your application. Instances of  TestResponse  are returned and can be evaluated with matchers like any other Dart tests. There are special matchers specifically for Aqueduct.  test ( POST /users creates a user ,   ()   async   { \n   var   request   =   app . client . request ( /users ) \n     .. json   =   { email :   bob@stablekernel.com }; \n   var   response   =   await   request . post (); \n\n   expect ( response ,   hasResponse ( 200 ,   { \n     id :   isNumber , \n     email :   bob@stablekernel.com \n   }));  });  test ( GET /users/1 returns a user ,   ()   async   { \n   var   response   =   await   app . client . authenticatedRequest ( /users/1 ). get (); \n   expect ( response ,   hasResponse ( 200 ,   partial ({ \n     email :   bob@stablekernel.com \n   })));  });", 
            "title": "Testing"
        }, 
        {
            "location": "/tour/#documentation", 
            "text": "Generate OpenAPI specifications automatically:  aqueduct document", 
            "title": "Documentation"
        }, 
        {
            "location": "/intellij/", 
            "text": "Aqueduct IntellIJ IDEA Templates\n\n\nFor convenience in creating files and writing common code, the following IntelliJ IDEA templates are available:\n\n\nFile Templates\n\n\n\n\nAqueduct HTTPController\n\n\nCreates a new file with the skeleton of an \nHTTPController\n.\n\n\n\n\n\n\nAqueduct ManagedObject\n\n\nCreates a new file with the skeleton of a \nManagedObject\n subclass\n\n\n\n\n\n\nAqueduct Test\n\n\nCreates a new file that imports a test harness and implementations for setting up and tearing down a \nTestApplication\n.\n\n\n\n\n\n\n\n\nAfter \ninstallation\n, file templates are available through any IntellIJ IDEA interface for creating files when the project has been enabled to use the Dart plugin.\n\n\nLive Templates\n\n\n\n\n'bindmethod'\n\n\nEnters a skeleton of an \nHTTPController\n responder method - after insertion, enter the HTTP method to finish the method declaration.\n\n\n\n\n\n\n'bindheader'  \n\n\nEnters an \n@HTTPHeader\n binding - after insertion, enter the name of the header.\n\n\n\n\n\n\n'bindquery'\n\n\nEnters an \n@HTTPQuery\n binding - after insertion, enter the name of the query parameter.\n\n\n\n\n\n\n\n\nInstallation\n\n\nDownload the \nthis file\n and import it into IntelliJ by selecting \nImport Settings...\n from the \nFile\n menu.", 
            "title": "IntelliJ IDEA Templates"
        }, 
        {
            "location": "/intellij/#aqueduct-intellij-idea-templates", 
            "text": "For convenience in creating files and writing common code, the following IntelliJ IDEA templates are available:", 
            "title": "Aqueduct IntellIJ IDEA Templates"
        }, 
        {
            "location": "/intellij/#file-templates", 
            "text": "Aqueduct HTTPController  Creates a new file with the skeleton of an  HTTPController .    Aqueduct ManagedObject  Creates a new file with the skeleton of a  ManagedObject  subclass    Aqueduct Test  Creates a new file that imports a test harness and implementations for setting up and tearing down a  TestApplication .     After  installation , file templates are available through any IntellIJ IDEA interface for creating files when the project has been enabled to use the Dart plugin.", 
            "title": "File Templates"
        }, 
        {
            "location": "/intellij/#live-templates", 
            "text": "'bindmethod'  Enters a skeleton of an  HTTPController  responder method - after insertion, enter the HTTP method to finish the method declaration.    'bindheader'    Enters an  @HTTPHeader  binding - after insertion, enter the name of the header.    'bindquery'  Enters an  @HTTPQuery  binding - after insertion, enter the name of the query parameter.", 
            "title": "Live Templates"
        }, 
        {
            "location": "/intellij/#installation", 
            "text": "Download the  this file  and import it into IntelliJ by selecting  Import Settings...  from the  File  menu.", 
            "title": "Installation"
        }, 
        {
            "location": "/tut/getting-started/", 
            "text": "1. Getting Started\n\n\nThe purpose of this tutorial series is to become familiar with how Aqueduct works by building an application. To get started, make sure you have the following software installed:\n\n\n\n\nDart (\nInstall Instructions\n)\n\n\nIntelliJ IDEA or any other Jetbrains IDE (\nInstall Instructions\n)\n\n\nThe IntelliJ IDEA Dart Plugin (\nInstall Instructions\n)\n\n\n\n\nIf at anytime you get stuck, hop on over to the \nAqueduct Slack channel\n.\n\n\nInstalling Aqueduct\n\n\naqueduct\n is a command-line utility for all things Aqueduct - including creating a new project. Install \naqueduct\n with the following command:\n\n\npub global activate aqueduct\n\n\n\n\n\n\n\nIf you get warning text about your \nPATH\n, make sure to read it before moving on.\n\n\n\n\nCreating a Project\n\n\nCreate a new project named \nquiz\n:\n\n\naqueduct create quiz\n\n\n\n\n\nThis creates a \nquiz\n project directory. Open this directory with IntelliJ IDEA. In IntelliJ's project view, locate the \nlib\n directory; this is where your project's code will go. This barebones project has two files - \nquiz.dart\n and \nquiz_sink.dart\n.\n\n\nOpen \nquiz_sink.dart\n and click on the \nEnable Dart Support\n button that will appear.\n\n\nThis file contains a \nRequestSink\n subclass. A subclass of this type initializes an application. Declaring exactly one subclass of this type is the only requirement an Aqueduct application must fulfill, and it defines which endpoints the application has by overriding its \nsetupRouter\n method. Once a request sink has finished initialization, it will start receiving requests.\n\n\nHandling Requests\n\n\nWhen a \nRequestSink\n gets a request, it sends it to its \nRouter\n. A router figures out the next object to send the request to based on the request's path. That next object might respond to the request, or it might send it to some other object. This goes on until some object responds to the request.\n\n\nEach of these objects are \nRequestController\ns; the type that can receive and respond to requests. An Aqueduct application creates and links together instances of existing \nRequestController\n subclasses (like \nRequestSink\n and \nRouter\n) and application-specific subclasses. These instances form a series of step that a request will go through before getting responded to. Each of these requests are instances of \nRequest\n. For every \nRequest\n, a \nResponse\n must be created.\n\n\nThese three types - \nRequest\n, \nResponse\n and \nRequestController\n - are the most important in Aqueduct.\n\n\n\n\nFor more details on these types, see \nHTTP Guides\n.\n\n\n\n\nIn this tutorial, we'll create a Quiz application. We'll start by writing code that responds to a request with a JSON list of questions. The code that handles this request will be written in \nQuestionController\n - a class that you will write. Create a new file \nlib/controller/question_controller.dart\n and add the following code:\n\n\nimport\n \n../quiz.dart\n;\n\n\n\nclass\n \nQuestionController\n \nextends\n \nHTTPController\n \n{\n\n  \nvar\n \nquestions\n \n=\n \n[\n\n    \nHow much wood can a woodchuck chuck?\n,\n\n    \nWhat\ns the tallest mountain in the world?\n\n  \n];\n\n\n  \n@\nhttpGet\n\n  \nFuture\nResponse\n \ngetAllQuestions\n()\n \nasync\n \n{\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\nquestions\n);\n\n  \n}\n\n\n}\n\n\n\n\n\n\nHTTPController\n - the superclass - is the most often used controller in Aqueduct because it has special behavior that \nbinds\n requests to its methods.\n\n\nThe method \ngetAllQuestions\n - with its \n@httpGet\n metadata - is bound to HTTP \nGET\n requests. This method is invoked anytime a \nQuestionController\n receives a \nGET\n request. A method with this metadata must return a \nFuture\nResponse\n that fulfills the request. These methods are called \nresponder methods\n.\n\n\n\n\nThere are \nhttpPut\n, \nhttpPost\n, \nhttpDelete\n, and \nHTTPMethod()\n, too. They all bind an HTTP method to a responder method.\n\n\n\n\nIn this case, a \nQuestionController\n will return a 200 OK response with a JSON list of question strings for all \nGET\n requests. For a \nQuestionController\n to receive requests, we have to add a route to it. In \nlib/quiz_sink.dart\n, add this import to the top of the file:\n\n\nimport\n \ncontroller/question_controller.dart\n;\n\n\n\n\n\n\nAnd then in \nQuizSink\n, add a new route in \nsetupRouter\n:\n\n\n@\noverride\n\n\nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n  \nrouter\n\n    \n.\nroute\n(\n/questions\n)\n\n    \n.\ngenerate\n(()\n \n=\n \nnew\n \nQuestionController\n());\n\n\n  \n/* This code was added by the template, you may delete it */\n\n  \nrouter\n\n    \n.\nroute\n(\n/example\n)\n\n    \n.\nlisten\n((\nrequest\n)\n \nasync\n \n{\n\n      \nreturn\n \nnew\n \nResponse\n.\nok\n({\nkey\n:\n \nvalue\n});\n\n    \n});\n\n\n}\n\n\n\n\n\n\nThis code routes the endpoint \n/questions\n to an instance of \nQuestionController\n. We can verify this rather quickly. In the project directory, run:\n\n\naqueduct serve\n\n\n\n\n\nThen, in a browser, enter \nhttp://localhost:8081/questions\n. You'll see the following text:\n\n\n[\nHow much wood can a woodchuck chuck?\n,\nWhat\ns the tallest mountain in the world?\n]\n\n\n\n\n\nTry another route that you didn't add - like \nhttp://localhost:8081/foobar\n - and you'll get a 404 Not Found page. If there is no route registered for the request, the \nRouter\n returns a response and no other controllers will receive the request.\n\n\n\n\nThe organization of an application's \nRequestController\ns is called a \nrequest channel\n. The channel always starts with \nRequestSink\n and is then split into sub-channels by \nRouter.route\n. More controllers are then added to these sub-channels.\n\n\nHere, the sub-channel for requests with path \n/questions\n starts (and ends) with an instance of \nQuestionController\n. We know that if a \nQuestionController\n receives a \nGET\n request, it will respond with a list of questions. Thus, \nGET /questions\n returns a list of questions.\n\n\nConstructing the channel should look familiar to to using higher-ordered functions on \nList\ns and \nStream\ns:\n\n\nvar\n \nadults\n \n=\n \npeople\n\n  \n.\nwhere\n((\np\n)\n \n=\n \np\n.\nage\n \n=\n \n18\n)\n  \n  \n.\ntoList\n();\n\n\n\n\n\n\nWith higher-ordered functions, each function takes an input and emits an output. When higher-ordered functions are chained together, their output is used as the input to the next function. This is exactly how Aqueduct's request channel works, except the methods are named things like \nroute\n and \ngenerate\n.\n\n\n\n\nRequest Channel Methods\n\n\nThere are three methods for constructing the request channel: \npipe\n, \ngenerate\n and \nlisten\n. A \nRouter\n has a special method, \nroute\n. Each has slightly different behavior and are covered in more detail \nhere\n. For now, understand that a new instance of \nQuestionController\n is created each time a \n/questions\n request is received.\n\n\n\n\nRouting and Another Route\n\n\nSo far, we've added a route that matches the constant string \n/questions\n. Routes can also have variables and optional segments; this allows us to form groups of routes with a simple syntax. We'll add an optional variable to the end of our existing route so that both \n/questions\n and \n/questions/1\n (or 2, or 3, ...) all match this route.\n\n\nIn \nquiz_sink.dart\n, modify the code in the \nQuizSink.setupRouter\n by adding \"/[:index]\" to the route.\n\n\n  \n@\noverride\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n    \nrouter\n\n        \n.\nroute\n(\n/questions/[:index]\n)\n\n        \n.\ngenerate\n(()\n \n=\n \nnew\n \nQuestionController\n());\n\n  \n}\n\n\n\n\n\n\nThe square brackets indicate that route segment is optional and the colon indicates that it is a variable named \nindex\n. A variable will match whatever the segment is in the request path and store it so \nQuestionController\n can use it.\n\n\nWith this optional variable, both \n/questions\n and \n/questions/:index\n will be received by \nQuestionController\n. We need to add a new responder method to \nQuestionController\n that gets called when a request for a specific question is made:\n\n\nclass\n \nQuestionController\n \nextends\n \nHTTPController\n \n{\n\n  \nvar\n \nquestions\n \n=\n \n[\n\n    \nHow much wood can a woodchuck chuck?\n,\n\n    \nWhat\ns the tallest mountain in the world?\n\n  \n];\n\n\n  \n@\nhttpGet\n\n  \nFuture\nResponse\n \ngetAllQuestions\n()\n \nasync\n \n{\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\nquestions\n);\n\n  \n}\n\n\n  \n@\nhttpGet\n\n  \nFuture\nResponse\n \ngetQuestionAtIndex\n(\n@\nHTTPPath\n(\nindex\n)\n \nint\n \nindex\n)\n \nasync\n \n{\n\n    \nif\n \n(\nindex\n \n \n0\n \n||\n \nindex\n \n=\n \nquestions\n.\nlength\n)\n \n{\n\n      \nreturn\n \nnew\n \nResponse\n.\nnotFound\n();\n\n    \n}\n\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\nquestions\n[\nindex\n]);\n  \n  \n}\n\n\n}\n\n\n\n\n\n\nReload the application by hitting Ctrl-C in the terminal that ran \naqueduct serve\n and then run \naqueduct serve\n again.\n\n\nIn your browser, enter \nhttp://localhost:8081/questions\n and you'll get the list of questions.\n\n\nThen, enter \nhttp://localhost:8081/questions/0\n and you'll get the first question. If you enter an index not within the list of questions or something other than an integer, you'll get an error response.\n\n\n\n\nClosing the Application\n\n\nOnce you're done running an application, stop it with \n^C\n. Otherwise, the next time you try and start an application, it will fail because your previous application is already listening for requests on the same port.\n\n\n\n\nWhen a request matches a route, the value for any path variables is stored in the \nRequest\n object. In the case of \n/questions/1\n, the path variable \nindex\n has a value of \n1\n; whereas \nindex\n is null if the path is \n/questions\n. When the request makes it to the \nQuestionController\n, it will select which responder method to run based on the HTTP method and the value of any path variables.\n\n\nThe \n@HTTPPath\n metadata for the argument to \ngetQuestionAtIndex\n is called a \npath binding\n. When a request for \nGET /questions/1\n is made, the path variable \nindex\n will be non-null and \ngetQuestionAtIndex\n will be invoked. The argument \nindex\n will be equal to the path variable's value.\n\n\n\n\nThe bound argument's name does not have to be the same as the path variable it is bound to.\n\n\n\n\nIf the path variable \nindex\n is null (i.e. \nGET /questions\n), \ngetAllQuestions\n is invoked. A responder method is only selected if both its path and method bindings match the incoming request. If there is no responder method for a request, an appropriate error response is returned.\n\n\n\n\nThis 'binding' behavior is specific to \nHTTPController\n. In addition to path variables, you can bind headers, query parameters and bodies. Check out \nHTTPControllers\n for more details.\n\n\n\n\nThe More You Know: Multi-threading and Application State\n\n\nIn this simple exercise, we used a constant list of question as the source of data for the questions endpoint. For a simple getting-your-feet-wet demo, this is fine.\n\n\nHowever, in a real application, it is important that we don't keep any mutable state in a \nRequestSink\n or any \nRequestController\ns. This is for three reasons. First, it's just bad practice - web servers should be stateless. They are facilitators between a client and a repository of data, not a repository of data themselves. A repository of data is typically a database.\n\n\nSecond, the way Aqueduct applications are structured makes it intentionally difficult to keep state. For example, \nHTTPController\n is instantiated each time a new request comes in. Any state they have is discarded after the request is finished processing. This is intentional - you won't run into an issue when scaling to multiple server instances in the future, because the code is already structured to be stateless.\n\n\nFinally, Aqueduct applications are set up to run on multiple isolates. An isolate is effectively a thread that shares no memory with other threads. If we were to keep track of state in some way, that state would not be reflected across all of the isolates running on this web server. So depending on which isolate grabbed a request, it may have different state than you might expect. Again, Aqueduct forces you into this model on purpose.\n\n\nIsolates will spread themselves out across CPUs on the host machine. Each isolate will have its own instance of your \nRequestSink\n subclass. Having multiple isolates running the same stateless web server on one machine allows for faster request handling. Each isolate also maintains its own set of services, like database connections.\n\n\nNext Chapter: Writing Tests", 
            "title": "1. Getting Started"
        }, 
        {
            "location": "/tut/getting-started/#1-getting-started", 
            "text": "The purpose of this tutorial series is to become familiar with how Aqueduct works by building an application. To get started, make sure you have the following software installed:   Dart ( Install Instructions )  IntelliJ IDEA or any other Jetbrains IDE ( Install Instructions )  The IntelliJ IDEA Dart Plugin ( Install Instructions )   If at anytime you get stuck, hop on over to the  Aqueduct Slack channel .", 
            "title": "1. Getting Started"
        }, 
        {
            "location": "/tut/getting-started/#installing-aqueduct", 
            "text": "aqueduct  is a command-line utility for all things Aqueduct - including creating a new project. Install  aqueduct  with the following command:  pub global activate aqueduct   If you get warning text about your  PATH , make sure to read it before moving on.", 
            "title": "Installing Aqueduct"
        }, 
        {
            "location": "/tut/getting-started/#creating-a-project", 
            "text": "Create a new project named  quiz :  aqueduct create quiz  This creates a  quiz  project directory. Open this directory with IntelliJ IDEA. In IntelliJ's project view, locate the  lib  directory; this is where your project's code will go. This barebones project has two files -  quiz.dart  and  quiz_sink.dart .  Open  quiz_sink.dart  and click on the  Enable Dart Support  button that will appear.  This file contains a  RequestSink  subclass. A subclass of this type initializes an application. Declaring exactly one subclass of this type is the only requirement an Aqueduct application must fulfill, and it defines which endpoints the application has by overriding its  setupRouter  method. Once a request sink has finished initialization, it will start receiving requests.", 
            "title": "Creating a Project"
        }, 
        {
            "location": "/tut/getting-started/#handling-requests", 
            "text": "When a  RequestSink  gets a request, it sends it to its  Router . A router figures out the next object to send the request to based on the request's path. That next object might respond to the request, or it might send it to some other object. This goes on until some object responds to the request.  Each of these objects are  RequestController s; the type that can receive and respond to requests. An Aqueduct application creates and links together instances of existing  RequestController  subclasses (like  RequestSink  and  Router ) and application-specific subclasses. These instances form a series of step that a request will go through before getting responded to. Each of these requests are instances of  Request . For every  Request , a  Response  must be created.  These three types -  Request ,  Response  and  RequestController  - are the most important in Aqueduct.   For more details on these types, see  HTTP Guides .   In this tutorial, we'll create a Quiz application. We'll start by writing code that responds to a request with a JSON list of questions. The code that handles this request will be written in  QuestionController  - a class that you will write. Create a new file  lib/controller/question_controller.dart  and add the following code:  import   ../quiz.dart ;  class   QuestionController   extends   HTTPController   { \n   var   questions   =   [ \n     How much wood can a woodchuck chuck? , \n     What s the tallest mountain in the world? \n   ]; \n\n   @ httpGet \n   Future Response   getAllQuestions ()   async   { \n     return   new   Response . ok ( questions ); \n   }  }   HTTPController  - the superclass - is the most often used controller in Aqueduct because it has special behavior that  binds  requests to its methods.  The method  getAllQuestions  - with its  @httpGet  metadata - is bound to HTTP  GET  requests. This method is invoked anytime a  QuestionController  receives a  GET  request. A method with this metadata must return a  Future Response  that fulfills the request. These methods are called  responder methods .   There are  httpPut ,  httpPost ,  httpDelete , and  HTTPMethod() , too. They all bind an HTTP method to a responder method.   In this case, a  QuestionController  will return a 200 OK response with a JSON list of question strings for all  GET  requests. For a  QuestionController  to receive requests, we have to add a route to it. In  lib/quiz_sink.dart , add this import to the top of the file:  import   controller/question_controller.dart ;   And then in  QuizSink , add a new route in  setupRouter :  @ override  void   setupRouter ( Router   router )   { \n   router \n     . route ( /questions ) \n     . generate (()   =   new   QuestionController ()); \n\n   /* This code was added by the template, you may delete it */ \n   router \n     . route ( /example ) \n     . listen (( request )   async   { \n       return   new   Response . ok ({ key :   value }); \n     });  }   This code routes the endpoint  /questions  to an instance of  QuestionController . We can verify this rather quickly. In the project directory, run:  aqueduct serve  Then, in a browser, enter  http://localhost:8081/questions . You'll see the following text:  [ How much wood can a woodchuck chuck? , What s the tallest mountain in the world? ]  Try another route that you didn't add - like  http://localhost:8081/foobar  - and you'll get a 404 Not Found page. If there is no route registered for the request, the  Router  returns a response and no other controllers will receive the request.   The organization of an application's  RequestController s is called a  request channel . The channel always starts with  RequestSink  and is then split into sub-channels by  Router.route . More controllers are then added to these sub-channels.  Here, the sub-channel for requests with path  /questions  starts (and ends) with an instance of  QuestionController . We know that if a  QuestionController  receives a  GET  request, it will respond with a list of questions. Thus,  GET /questions  returns a list of questions.  Constructing the channel should look familiar to to using higher-ordered functions on  List s and  Stream s:  var   adults   =   people \n   . where (( p )   =   p . age   =   18 )   \n   . toList ();   With higher-ordered functions, each function takes an input and emits an output. When higher-ordered functions are chained together, their output is used as the input to the next function. This is exactly how Aqueduct's request channel works, except the methods are named things like  route  and  generate .   Request Channel Methods  There are three methods for constructing the request channel:  pipe ,  generate  and  listen . A  Router  has a special method,  route . Each has slightly different behavior and are covered in more detail  here . For now, understand that a new instance of  QuestionController  is created each time a  /questions  request is received.", 
            "title": "Handling Requests"
        }, 
        {
            "location": "/tut/getting-started/#routing-and-another-route", 
            "text": "So far, we've added a route that matches the constant string  /questions . Routes can also have variables and optional segments; this allows us to form groups of routes with a simple syntax. We'll add an optional variable to the end of our existing route so that both  /questions  and  /questions/1  (or 2, or 3, ...) all match this route.  In  quiz_sink.dart , modify the code in the  QuizSink.setupRouter  by adding \"/[:index]\" to the route.     @ override \n   void   setupRouter ( Router   router )   { \n     router \n         . route ( /questions/[:index] ) \n         . generate (()   =   new   QuestionController ()); \n   }   The square brackets indicate that route segment is optional and the colon indicates that it is a variable named  index . A variable will match whatever the segment is in the request path and store it so  QuestionController  can use it.  With this optional variable, both  /questions  and  /questions/:index  will be received by  QuestionController . We need to add a new responder method to  QuestionController  that gets called when a request for a specific question is made:  class   QuestionController   extends   HTTPController   { \n   var   questions   =   [ \n     How much wood can a woodchuck chuck? , \n     What s the tallest mountain in the world? \n   ]; \n\n   @ httpGet \n   Future Response   getAllQuestions ()   async   { \n     return   new   Response . ok ( questions ); \n   } \n\n   @ httpGet \n   Future Response   getQuestionAtIndex ( @ HTTPPath ( index )   int   index )   async   { \n     if   ( index     0   ||   index   =   questions . length )   { \n       return   new   Response . notFound (); \n     } \n\n     return   new   Response . ok ( questions [ index ]);   \n   }  }   Reload the application by hitting Ctrl-C in the terminal that ran  aqueduct serve  and then run  aqueduct serve  again.  In your browser, enter  http://localhost:8081/questions  and you'll get the list of questions.  Then, enter  http://localhost:8081/questions/0  and you'll get the first question. If you enter an index not within the list of questions or something other than an integer, you'll get an error response.   Closing the Application  Once you're done running an application, stop it with  ^C . Otherwise, the next time you try and start an application, it will fail because your previous application is already listening for requests on the same port.   When a request matches a route, the value for any path variables is stored in the  Request  object. In the case of  /questions/1 , the path variable  index  has a value of  1 ; whereas  index  is null if the path is  /questions . When the request makes it to the  QuestionController , it will select which responder method to run based on the HTTP method and the value of any path variables.  The  @HTTPPath  metadata for the argument to  getQuestionAtIndex  is called a  path binding . When a request for  GET /questions/1  is made, the path variable  index  will be non-null and  getQuestionAtIndex  will be invoked. The argument  index  will be equal to the path variable's value.   The bound argument's name does not have to be the same as the path variable it is bound to.   If the path variable  index  is null (i.e.  GET /questions ),  getAllQuestions  is invoked. A responder method is only selected if both its path and method bindings match the incoming request. If there is no responder method for a request, an appropriate error response is returned.   This 'binding' behavior is specific to  HTTPController . In addition to path variables, you can bind headers, query parameters and bodies. Check out  HTTPControllers  for more details.", 
            "title": "Routing and Another Route"
        }, 
        {
            "location": "/tut/getting-started/#the-more-you-know-multi-threading-and-application-state", 
            "text": "In this simple exercise, we used a constant list of question as the source of data for the questions endpoint. For a simple getting-your-feet-wet demo, this is fine.  However, in a real application, it is important that we don't keep any mutable state in a  RequestSink  or any  RequestController s. This is for three reasons. First, it's just bad practice - web servers should be stateless. They are facilitators between a client and a repository of data, not a repository of data themselves. A repository of data is typically a database.  Second, the way Aqueduct applications are structured makes it intentionally difficult to keep state. For example,  HTTPController  is instantiated each time a new request comes in. Any state they have is discarded after the request is finished processing. This is intentional - you won't run into an issue when scaling to multiple server instances in the future, because the code is already structured to be stateless.  Finally, Aqueduct applications are set up to run on multiple isolates. An isolate is effectively a thread that shares no memory with other threads. If we were to keep track of state in some way, that state would not be reflected across all of the isolates running on this web server. So depending on which isolate grabbed a request, it may have different state than you might expect. Again, Aqueduct forces you into this model on purpose.  Isolates will spread themselves out across CPUs on the host machine. Each isolate will have its own instance of your  RequestSink  subclass. Having multiple isolates running the same stateless web server on one machine allows for faster request handling. Each isolate also maintains its own set of services, like database connections.", 
            "title": "The More You Know: Multi-threading and Application State"
        }, 
        {
            "location": "/tut/getting-started/#next-chapter-writing-tests", 
            "text": "", 
            "title": "Next Chapter: Writing Tests"
        }, 
        {
            "location": "/tut/writing-tests/", 
            "text": "2. Writing Tests\n\n\nOne of the core principles of Aqueduct is effective testing. While opening up your browser and typing in a URL can verify the code you just wrote succeeds, it's not a very reliable way of testing software. We'll also run into trouble when testing endpoints that use HTTP methods other than GET. Therefore, there are some helpful utilities for writing tests in Aqueduct.\n\n\nTesting in Dart is simple: create a file that ends with \n_test.dart\n, write a \nmain\n function and use the \ntest\n function register a test. Each test is a closure that runs some code and has expectations. For example, this code would test that 1 + 1 = 2:\n\n\nimport\n \npackage:test/test.dart\n;\n\n\n\nvoid\n \nmain\n()\n \n{\n\n  \ntest\n(\n1+1 = 2\n,\n \n()\n \n{\n\n    \nexpect\n(\n1\n \n+\n \n1\n,\n \nequals\n(\n2\n));\n\n  \n});\n\n\n}\n\n\n\n\n\n\nTests are made possible by the \ntest\n package which you'll need to claim as a dependency. Locate the file \nquiz/pubspec.yaml\n in your project. This file contains metadata about your application, including it's dependencies. At the bottom of this file, the following two lines specify that this project uses the \ntest\n package as a development dependency:\n\n\ndev_dependencies\n:\n\n  \ntest\n:\n \nany\n\n\n\n\n\n\nIn Dart, tests are stored in a top-level \ntest\n directory that has already been created from the template. Add a new file to it named \ntest/question_controller_test.dart\n. (Tests must end in \n_test.dart\n and live in the \ntest\n directory.) In this file, import your application's test harness:\n\n\nimport\n \nharness/app.dart\n;\n\n\n\n\n\n\nThe test harness exports the \ntest\n package and declares a class named \nTestApplication\n. Aqueduct's testing strategy is simple: run the application locally, execute requests and verify their responses. A \nTestApplication\n can run the application from your tests and has a \nclient\n property for executing requests against that application. In \ntest/question_controller_test.dart\n, add the following code to set up the test harness:\n\n\nFuture\n \nmain\n()\n \nasync\n \n{\n\n  \nTestApplication\n \napp\n \n=\n \nnew\n \nTestApplication\n();\n\n\n  \nsetUpAll\n(()\n \nasync\n \n{\n\n    \nawait\n \napp\n.\nstart\n();\n\n  \n});\n\n\n  \ntearDownAll\n(()\n \nasync\n \n{\n\n    \nawait\n \napp\n.\nstop\n();\n\n  \n});\n  \n\n}\n\n\n\n\n\n\n\n\nIt's really important that the application is stopped in \ntearDownAll\n, otherwise your tests won't exit because there is an HTTP server running!\n\n\n\n\nLet's add a test to verify that \nGET /questions\n returns a list of questions in a 200 OK response.\n\n\nvoid\n \nmain\n()\n \n{\n\n  \n...\n\n\n  \ntest\n(\n/questions returns list of questions\n,\n \n()\n \nasync\n \n{\n\n    \nvar\n \nrequest\n \n=\n \napp\n.\nclient\n.\nrequest\n(\n/questions\n);\n\n    \nexpectResponse\n(\n\n      \nawait\n \nrequest\n.\nget\n(),\n\n      \n200\n,\n\n      \nbody:\n \neveryElement\n(\nendsWith\n(\n?\n)));\n\n  \n});\n\n\n}\n\n\n\n\n\n\nThis test executes the request \nGET http://localhost/questions\n and ensures that the response's status code is 200 and the body is a list of strings that all end in '?'.\n\n\nThe method \nexpectResponse\n takes a \nTestResponse\n, status code and an optional \nbody matcher\n (it optionally takes a header matcher, too). It verifies that the response has the expected values and fails the test if it doesn't.\n\n\nA \nTestResponse\n is created by executing a \nTestRequest\n, which is created by with the \nrequest\n method of a \nTestApplication\n's \nclient\n. The execution methods for \nTestRequest\n are \nget()\n, \npost()\n, etc.\n\n\nRun this test by right-clicking anywhere on it and selecting \nRun\n from the pop-up menu. A test runner will appear on the bottom of the screen with the results of the test.\n\n\nThere's one little issue here: the \neveryElement\n matcher ensures each string in the response body passes the inner matcher \nendsWith\n. However, if the response body were an empty list, the inner matcher would never run and the test would still pass. Let's verify that there is at least one question, too:\n\n\ntest\n(\n/questions returns list of questions\n,\n \n()\n \nasync\n \n{\n\n  \nvar\n \nrequest\n \n=\n \napp\n.\nclient\n.\nrequest\n(\n/questions\n);\n\n  \nexpectResponse\n(\n\n    \nawait\n \nrequest\n.\nget\n(),\n\n    \n200\n,\n\n    \nbody:\n \nallOf\n([\n\n      \nhasLength\n(\ngreaterThan\n(\n0\n)),\n\n      \neveryElement\n(\nendsWith\n(\n?\n))\n\n    \n]));\n\n\n});\n\n\n\n\n\n\nWriting More Tests\n\n\nLet's write two more tests - first, a test that ensures \nGET /questions/:index\n returns a single question and then another that ensures an index outside of the range of questions will return a 404. Add the following two tests inside the main function:\n\n\ntest\n(\n/questions/index returns a single question\n,\n \n()\n \nasync\n \n{\n\n  \nexpectResponse\n(\n\n    \nawait\n \napp\n.\nclient\n.\nrequest\n(\n/questions/1\n).\nget\n(),\n\n    \n200\n,\n\n    \nbody:\n \nendsWith\n(\n?\n));\n\n\n});\n\n\n\ntest\n(\n/questions/index out of range returns 404\n,\n \n()\n \nasync\n \n{\n\n  \nexpectResponse\n(\n\n    \nawait\n \napp\n.\nclient\n.\nrequest\n(\n/questions/100\n).\nget\n(),\n\n    \n404\n);\n\n  \n});\n\n\n\n\n\n\nYou can run all of the tests in a test file by right-clicking the \nmain\n function and selecting \nRun\n from the popup menu.\n\n\nAll of your tests should pass - but what if they don't? If we, for example, went back into \nquestion_controller.dart\n and added a new question that didn't end with a \n?\n:\n\n\nvar\n \nquestions\n \n=\n \n[\n\n  \nHow much wood can a woodchuck chuck?\n,\n\n  \nWhat\ns the tallest mountain in the world?\n,\n\n  \nThis is a statement.\n\n\n];\n\n\n\n\n\n\nThe first test that gets all questions will fail and the following will be printed to the console:\n\n\nExpected\n:\n \n---\n \nHTTP\n \nResponse\n \n---\n\n          \n-\n \nStatus\n \ncode\n \nmust\n \nbe\n \n200\n\n          \n-\n \nHeaders\n \ncan\n \nbe\n \nanything\n\n          \n-\n \nBody\n \nafter\n \ndecoding\n \nmust\n \nbe\n:\n\n\n            \n(\nan\n \nobject\n \nwith\n \nlength\n \nof\n \na\n \nvalue\n \ngreater\n \nthan\n \n0\n \nand\n \nevery\n \nelement\n(\na\n \nstring\n \nending\n \nwith\n \n?\n))\n\n          \n---------------------\n\n  \nActual\n:\n \nTestResponse\n:\n-----------\n\n          \n-\n \nStatus\n \ncode\n \nis\n \n200\n\n          \n-\n \nHeaders\n \nare\n \nthe\n \nfollowing\n:\n\n            \n-\n \ncontent-encoding\n:\n \ngzip\n\n            \n-\n \ncontent-length\n:\n \n108\n\n            \n-\n \nx-frame-options\n:\n \nSAMEORIGIN\n\n            \n-\n \ncontent-type\n:\n \napplication\n/\njson\n;\n \ncharset\n=\nutf-8\n\n            \n-\n \nx-xss-protection\n:\n \n1\n;\n \nmode\n=\nblock\n\n            \n-\n \nx-content-type-options\n:\n \nnosniff\n\n            \n-\n \nserver\n:\n \naqueduct\n/\n1\n\n          \n-------------------------\n\n          \n\n   \nWhich\n:\n \nthe\n \nbody\n \ndiffers\n \nfor\n \nthe\n \nfollowing\n \nreasons\n:\n\n          \nhas\n \nvalue\n \nThis is a statement.\n \nwhich\n \ndoesn\nt match a string ending with \n?\n \nat\n \nindex\n \n2\n\n\n\n\n\n\nThe \nExpected:\n value tells what was expected, the \nActual:\n value tells you what you got, and \nWhich:\n tells you why they differ. Here, the body differs because 'This is a statement.' doesn't end with '?'.\n\n\nRemove \nThis is a statement.\n from the list of questions and your tests will pass again.\n\n\nNext Chapter: Executing Database Queries", 
            "title": "2. Writing Tests"
        }, 
        {
            "location": "/tut/writing-tests/#2-writing-tests", 
            "text": "One of the core principles of Aqueduct is effective testing. While opening up your browser and typing in a URL can verify the code you just wrote succeeds, it's not a very reliable way of testing software. We'll also run into trouble when testing endpoints that use HTTP methods other than GET. Therefore, there are some helpful utilities for writing tests in Aqueduct.  Testing in Dart is simple: create a file that ends with  _test.dart , write a  main  function and use the  test  function register a test. Each test is a closure that runs some code and has expectations. For example, this code would test that 1 + 1 = 2:  import   package:test/test.dart ;  void   main ()   { \n   test ( 1+1 = 2 ,   ()   { \n     expect ( 1   +   1 ,   equals ( 2 )); \n   });  }   Tests are made possible by the  test  package which you'll need to claim as a dependency. Locate the file  quiz/pubspec.yaml  in your project. This file contains metadata about your application, including it's dependencies. At the bottom of this file, the following two lines specify that this project uses the  test  package as a development dependency:  dev_dependencies : \n   test :   any   In Dart, tests are stored in a top-level  test  directory that has already been created from the template. Add a new file to it named  test/question_controller_test.dart . (Tests must end in  _test.dart  and live in the  test  directory.) In this file, import your application's test harness:  import   harness/app.dart ;   The test harness exports the  test  package and declares a class named  TestApplication . Aqueduct's testing strategy is simple: run the application locally, execute requests and verify their responses. A  TestApplication  can run the application from your tests and has a  client  property for executing requests against that application. In  test/question_controller_test.dart , add the following code to set up the test harness:  Future   main ()   async   { \n   TestApplication   app   =   new   TestApplication (); \n\n   setUpAll (()   async   { \n     await   app . start (); \n   }); \n\n   tearDownAll (()   async   { \n     await   app . stop (); \n   });    }    It's really important that the application is stopped in  tearDownAll , otherwise your tests won't exit because there is an HTTP server running!   Let's add a test to verify that  GET /questions  returns a list of questions in a 200 OK response.  void   main ()   { \n   ... \n\n   test ( /questions returns list of questions ,   ()   async   { \n     var   request   =   app . client . request ( /questions ); \n     expectResponse ( \n       await   request . get (), \n       200 , \n       body:   everyElement ( endsWith ( ? ))); \n   });  }   This test executes the request  GET http://localhost/questions  and ensures that the response's status code is 200 and the body is a list of strings that all end in '?'.  The method  expectResponse  takes a  TestResponse , status code and an optional  body matcher  (it optionally takes a header matcher, too). It verifies that the response has the expected values and fails the test if it doesn't.  A  TestResponse  is created by executing a  TestRequest , which is created by with the  request  method of a  TestApplication 's  client . The execution methods for  TestRequest  are  get() ,  post() , etc.  Run this test by right-clicking anywhere on it and selecting  Run  from the pop-up menu. A test runner will appear on the bottom of the screen with the results of the test.  There's one little issue here: the  everyElement  matcher ensures each string in the response body passes the inner matcher  endsWith . However, if the response body were an empty list, the inner matcher would never run and the test would still pass. Let's verify that there is at least one question, too:  test ( /questions returns list of questions ,   ()   async   { \n   var   request   =   app . client . request ( /questions ); \n   expectResponse ( \n     await   request . get (), \n     200 , \n     body:   allOf ([ \n       hasLength ( greaterThan ( 0 )), \n       everyElement ( endsWith ( ? )) \n     ]));  });", 
            "title": "2. Writing Tests"
        }, 
        {
            "location": "/tut/writing-tests/#writing-more-tests", 
            "text": "Let's write two more tests - first, a test that ensures  GET /questions/:index  returns a single question and then another that ensures an index outside of the range of questions will return a 404. Add the following two tests inside the main function:  test ( /questions/index returns a single question ,   ()   async   { \n   expectResponse ( \n     await   app . client . request ( /questions/1 ). get (), \n     200 , \n     body:   endsWith ( ? ));  });  test ( /questions/index out of range returns 404 ,   ()   async   { \n   expectResponse ( \n     await   app . client . request ( /questions/100 ). get (), \n     404 ); \n   });   You can run all of the tests in a test file by right-clicking the  main  function and selecting  Run  from the popup menu.  All of your tests should pass - but what if they don't? If we, for example, went back into  question_controller.dart  and added a new question that didn't end with a  ? :  var   questions   =   [ \n   How much wood can a woodchuck chuck? , \n   What s the tallest mountain in the world? , \n   This is a statement.  ];   The first test that gets all questions will fail and the following will be printed to the console:  Expected :   ---   HTTP   Response   --- \n           -   Status   code   must   be   200 \n           -   Headers   can   be   anything \n           -   Body   after   decoding   must   be : \n\n             ( an   object   with   length   of   a   value   greater   than   0   and   every   element ( a   string   ending   with   ? )) \n           --------------------- \n   Actual :   TestResponse : ----------- \n           -   Status   code   is   200 \n           -   Headers   are   the   following : \n             -   content-encoding :   gzip \n             -   content-length :   108 \n             -   x-frame-options :   SAMEORIGIN \n             -   content-type :   application / json ;   charset = utf-8 \n             -   x-xss-protection :   1 ;   mode = block \n             -   x-content-type-options :   nosniff \n             -   server :   aqueduct / 1 \n           ------------------------- \n           \n    Which :   the   body   differs   for   the   following   reasons : \n           has   value   This is a statement.   which   doesn t match a string ending with  ?   at   index   2   The  Expected:  value tells what was expected, the  Actual:  value tells you what you got, and  Which:  tells you why they differ. Here, the body differs because 'This is a statement.' doesn't end with '?'.  Remove  This is a statement.  from the list of questions and your tests will pass again.", 
            "title": "Writing More Tests"
        }, 
        {
            "location": "/tut/writing-tests/#next-chapter-executing-database-queries", 
            "text": "", 
            "title": "Next Chapter: Executing Database Queries"
        }, 
        {
            "location": "/tut/executing-queries/", 
            "text": "3. Executing Queries\n\n\nWe will continue to build on the last chapter's project, \nquiz\n, by storing questions in a database and retrieving them from the database.\n\n\nObject-Relational Mapping\n\n\nFor a quick prelude, let's make sure we understand what an object-relational mapping (ORM) is. There are different kinds of databases that have different use cases. A relational database management system - which pretty much means a 'SQL' database - stores its data in the form of tables. A table represents some sort of entity - like a person or a bank account - and has columns that describe the attributes of that entity - like a name or a balance. Every row in a table is an instance of that entity - like a single person named Bob or Bob's bank account.\n\n\nIn an object-oriented framework like Aqueduct, we have a similar concept: a class represents some sort of entity, and an object with that class is an instance of that entity. An ORM maps rows in a database to and from objects in an application. In Aqueduct, each database table-class pairing is called an \nentity\n. Collectively, an application's entities are called its data model.\n\n\nBuilding a Data Model\n\n\nIn our \nquiz\n application, we really have one type of entity - a \"question\". We will create an entity by declaring a class whose instances represent a question.\n\n\nCreate a new directory \nlib/model/\n and then add a new file to it named \nlib/model/question.dart\n. Add the following code:\n\n\nimport\n \n../quiz.dart\n;\n\n\n\nclass\n \nQuestion\n \nextends\n \nManagedObject\n_Question\n \nimplements\n \n_Question\n \n{}\n\n\n\nclass\n \n_Question\n \n{\n\n  \n@\nmanagedPrimaryKey\n\n  \nint\n \nindex\n;\n\n\n  \nString\n \ndescription\n;\n\n\n}\n\n\n\n\n\n\nThis declares a \"question\" entity. The class \n_Question\n is a one-to-one mapping to a database table with the same name. Each of its properties - \nindex\n and \ndescription\n - are columns in that table. When the ORM fetches rows from the \n_Question\n table, they will be returned as instances of \nQuestion\n.\n\n\n\n\nWhen declaring an entity, there must be two classes:\n\n\n\n\nA \npersistent type\n that starts with a \n_\n, has exactly one primary key property, and declares a property for each database column. In the above, the persistent type is \n_Question\n.\n\n\nAn \ninstance type\n that extends \nManagedObject\nT\n (where \nT\n is the persistent type). The instance type also implements the persistent type so that it has the same properties. In the above, the instance type is \nQuestion\n.\n\n\n\n\n\n\nWhy two classes?\n\n\nThe two classes allow for much of the powerful behavior of the Aqueduct ORM. Answering this question requires a understanding of those behaviors, which we won't cover much in this beginner's tutorial. Once you've got the basics down, check out \nthis guide\n for more detail.\n\n\n\n\nDuring application initialization, the data model gets compiled so that it information about these entities is available at runtime to the components that need it. In \nquiz_sink.dart\n, add the following code to the constructor:\n\n\nQuizSink\n(\nApplicationConfiguration\n \nappConfig\n)\n \n:\n \nsuper\n(\nappConfig\n)\n \n{\n\n  \n/* This line was added by the template */\n\n  \nlogger\n.\nonRecord\n.\nlisten\n((\nrec\n)\n \n=\n\n    \nprint\n(\n$\nrec\n \n${\nrec\n.\nerror\n \n??\n \n}\n \n${\nrec\n.\nstackTrace\n \n??\n \n}\n));\n\n\n  \nvar\n \ndataModel\n \n=\n \nnew\n \nManagedDataModel\n.\nfromCurrentMirrorSystem\n();\n\n\n}\n\n\n\n\n\n\nThe \nfromCurrentMirrorSystem\n constructor will be able to find all of the \nManagedObject\nT\n subclasses in your application. This data model will be one of the two necessary components for our next task, creating a \nManagedContext\n.\n\n\nDefining a Context\n\n\nA \nManagedContext\n is an application's gateway to a database. It maintains a connection to a database, executes queries on that database, and translates the results to and from \nManagedObject\nT\ns according to a \nManagedDataModel\n. We have the data model already, so now we must provide database connection information. In \nquiz_sink.dart\n, declare a new property and add a few more lines to the constructor:\n\n\nclass\n \nQuizSink\n \nextends\n \nRequestSink\n \n{\n\n  \nQuizSink\n(\nApplicationConfiguration\n \noptions\n)\n \n:\n \nsuper\n(\noptions\n)\n \n{\n\n    \nlogger\n.\nonRecord\n.\nlisten\n((\nrec\n)\n \n=\n\n      \nprint\n(\n$\nrec\n \n${\nrec\n.\nerror\n \n??\n \n}\n \n${\nrec\n.\nstackTrace\n \n??\n \n}\n));\n\n\n    \nvar\n \ndataModel\n \n=\n \nnew\n \nManagedDataModel\n.\nfromCurrentMirrorSystem\n();\n\n\n    \nvar\n \npersistentStore\n \n=\n \nnew\n \nPostgreSQLPersistentStore\n.\nfromConnectionInfo\n(\n\n      \ndart\n,\n \ndart\n,\n \nlocalhost\n,\n \n5432\n,\n \ndart_test\n);\n\n\n    \ncontext\n \n=\n \nnew\n \nManagedContext\n(\ndataModel\n,\n \npersistentStore\n);\n\n  \n}\n\n\n  \nManagedContext\n \ncontext\n;\n\n\n  \n...\n\n\n\n\n\n\nThis creates a context that connects to the database \npostgres://dart:dart@localhost:5432/dart_test\n that has a single \n_Question\n table. Before we create that database, let's finish writing the code that will fetch questions from a database.\n\n\nExecuting Queries\n\n\nOnce a context has been created, when can execute \nQuery\nT\ns on it. In \nlib/controller/question_controller.dart\n, replace the implementation of \ngetAllQuestions\n to fetch questions from a database and import the file that declares \nQuestion\n:\n\n\nimport\n \n../quiz.dart\n;\n\n\nimport\n \n../model/question.dart\n;\n\n\n\nclass\n \nQuestionController\n \nextends\n \nHttpController\n \n{\n\n  \nvar\n \nquestions\n \n=\n \n[\n\n    \nHow much wood can a woodchuck chuck?\n,\n\n    \nWhat\ns the tallest mountain in the world?\n\n  \n];\n\n\n  \n@\nhttpGet\n\n  \nFuture\nResponse\n \ngetAllQuestions\n()\n \nasync\n \n{\n\n    \nvar\n \nquestionQuery\n \n=\n \nnew\n \nQuery\nQuestion\n();\n\n    \nvar\n \ndatabaseQuestions\n \n=\n \nawait\n \nquestionQuery\n.\nfetch\n();\n\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\ndatabaseQuestions\n);\n\n  \n}\n\n\n\n...\n\n\n\n\n\n\nIn \ngetAllQuestions\n, we create an instance of \nQuery\nQuestion\n and then execute its \nfetch()\n method. This method will fetch every question from the database and returns them as a \nList\nQuestion\n into the variable \ndatabaseQuestions\n. This list is the body object to the \nResponse\n, which by default, encodes them as a list of JSON objects.\n\n\n\n\nWhere's the context?\n\n\nMost applications only talk to a single database and therefore have a single \nManagedContext\n. When creating a \nQuery\nT\n without specifying a context, the query defaults to executing on the last context created - which happens to be the one we instantiated in \nQuizSink\n.\n\n\n\n\nNow, let's update \ngetQuestionAtIndex\n to fetch a single question by its index from the database.\n\n\n@\nhttpGet\n\n\nFuture\nResponse\n \ngetQuestionAtIndex\n(\n@\nHTTPPath\n(\nindex\n)\n \nint\n \nindex\n)\n \nasync\n \n{\n\n  \nvar\n \nquestionQuery\n \n=\n \nnew\n \nQuery\nQuestion\n()\n\n    \n..\nwhere\n.\nindex\n \n=\n \nwhereEqualTo\n(\nindex\n);\n    \n\n  \nvar\n \nquestion\n \n=\n \nawait\n \nquestionQuery\n.\nfetchOne\n();\n\n\n  \nif\n \n(\nquestion\n \n==\n \nnull\n)\n \n{\n\n    \nreturn\n \nnew\n \nResponse\n.\nnotFound\n();\n\n  \n}\n\n  \nreturn\n \nnew\n \nResponse\n.\nok\n(\nquestion\n);\n\n\n}\n\n\n\n\n\n\nThis query does two interesting things: it filters the result of the query by the question's index and only fetches a single question with \nfetchOne()\n.\n\n\nFiltering is accomplished through the \nwhere\n property of a \nQuery\nQuestion\n; this property has all of the same properties as \nQuestion\n. \nMatchers\n like \nwhereEqualTo\n are set as the values of \nwhere\n's properties. This adds conditions to the where clause of the generated SQL query. In the above code, we constrain the query to only fetch questions whose \nindex\n is equal to the \nindex\n from the path variable. Since \nindex\n is the primary key of \nQuestion\n, this will either give us a single question or return null.\n\n\n\n\nMatching All the Things\n\n\nThere are a lot of matchers available to build different queries. All matchers start with the word \nwhere\n and can be found by searching the \nAPI reference\n.\n\n\n\n\nSetting Up a Database\n\n\nEnsure that your PostgreSQL installation is running and open the command-line tool \npsql\n. In this tool, create a new database and a user that can connect to it with the following SQL:\n\n\nCREATE\n \nDATABASE\n \ndart_test\n;\n\n\nCREATE\n \nUSER\n \ndart\n \nWITH\n \ncreatedb\n;\n\n\nALTER\n \nUSER\n \ndart\n \nWITH\n \npassword\n \ndart\n;\n\n\nGRANT\n \nall\n \nON\n \ndatabase\n \ndart_test\n \nTO\n \ndart\n;\n\n\n\n\n\n\nThe user, password and database name match those provided when creating our \nManagedContext\n in \nQuizSink\n. This database will be used during testing. Before the tests run, the test harness will create tables for all of your entities in this database. After the tests complete, the tables will be deleted from this database. Therefore, this database should only be used for running tests.\n\n\nWe must update our test harness to create these tables. In \ntest/harness/app.dart\n, add the following method to \nTestApplication\n:\n\n\nstatic\n \nFuture\n \ncreateDatabaseSchema\n(\nManagedContext\n \ncontext\n)\n \nasync\n \n{\n\n  \nvar\n \nbuilder\n \n=\n \nnew\n \nSchemaBuilder\n.\ntoSchema\n(\n\n      \ncontext\n.\npersistentStore\n,\n \nnew\n \nSchema\n.\nfromDataModel\n(\ncontext\n.\ndataModel\n),\n\n      \nisTemporary:\n \ntrue\n);\n\n\n  \nfor\n \n(\nvar\n \ncmd\n \nin\n \nbuilder\n.\ncommands\n)\n \n{\n\n    \nawait\n \ncontext\n.\npersistentStore\n.\nexecute\n(\ncmd\n);\n\n  \n}\n\n\n}\n\n\n\n\n\n\nThen, in \nTestApplication.start\n, add the line that calls this method after \nawait application.start()\n:\n\n\nFuture\n \nstart\n()\n \nasync\n \n{\n\n  \nRequestController\n.\nletUncaughtExceptionsEscape\n \n=\n \ntrue\n;\n\n  \napplication\n \n=\n \nnew\n \nApplication\nQuizSink\n();\n\n  \napplication\n.\nconfiguration\n.\nport\n \n=\n \n0\n;\n\n  \napplication\n.\nconfiguration\n.\nconfigurationFilePath\n \n=\n \nconfig.src.yaml\n;\n\n\n  \nawait\n \napplication\n.\nstart\n(\nrunOnMainIsolate:\n \ntrue\n);\n\n\n  \n/* Add this line */\n\n  \nawait\n \ncreateDatabaseSchema\n(\nsink\n.\ncontext\n);\n\n\n  \nclient\n \n=\n \nnew\n \nTestClient\n(\napplication\n);\n\n\n}\n\n\n\n\n\n\nNow when our tests in \nquestion_controller_test.dart\n go through their setup process, the SQL command to create the question table will be executed. The last thing left to do is populate this test database with questions during the test setup process. At the top of \nquestion_controller_test.dart\n, import \nquestion.dart\n:\n\n\nimport\n \npackage:quiz/model/question.dart\n;\n\n\n\n\n\n\nAnd then update the \nsetUpAll\n method:\n\n\nsetUpAll\n(()\n \nasync\n \n{\n\n  \nawait\n \napp\n.\nstart\n();\n\n\n  \nvar\n \nquestions\n \n=\n \n[\n\n    \nnew\n \nQuestion\n()\n\n      \n..\ndescription\n \n=\n \nHow much wood can a woodchuck chuck?\n,\n\n    \nnew\n \nQuestion\n()\n\n      \n..\ndescription\n \n=\n \nWhat\ns the tallest mountain in the world?\n,\n\n  \n];\n\n\n  \nawait\n \nFuture\n.\nforEach\n(\nquestions\n,\n \n(\nq\n)\n \n{\n\n    \nvar\n \nquery\n \n=\n \nnew\n \nQuery\nQuestion\n()\n\n        \n..\nvalues\n \n=\n \nq\n;\n\n    \nreturn\n \nquery\n.\ninsert\n();\n\n  \n});\n\n\n});\n\n\n\n\n\n\nThis inserts the two original questions as instances of \nQuestion\n into the database during setup. If we were to run our tests now, they would fail and the output would look like this:\n\n\nExpected\n:\n \n---\n \nHTTP\n \nResponse\n \n---\n\n          \n-\n \nStatus\n \ncode\n \nmust\n \nbe\n \n200\n\n          \n-\n \nHeaders\n \ncan\n \nbe\n \nanything\n\n          \n-\n \nBody\n \nafter\n \ndecoding\n \nmust\n \nbe\n:\n\n\n            \n(\nan\n \nobject\n \nwith\n \nlength\n \nof\n \na\n \nvalue\n \ngreater\n \nthan\n \n0\n \nand\n \nevery\n \nelement\n(\na\n \nstring\n \nending\n \nwith\n \n?\n))\n\n          \n---------------------\n\n\nActual\n:\n \nTestResponse\n:\n-----------\n\n        \n-\n \nStatus\n \ncode\n \nis\n \n200\n\n        \n-\n \nHeaders\n \nare\n \nthe\n \nfollowing\n:\n\n          \n-\n \ncontent-encoding\n:\n \ngzip\n\n          \n-\n \ncontent-length\n:\n \n124\n\n          \n-\n \nx-frame-options\n:\n \nSAMEORIGIN\n\n          \n-\n \ncontent-type\n:\n \napplication\n/\njson\n;\n \ncharset\n=\nutf-8\n\n          \n-\n \nx-xss-protection\n:\n \n1\n;\n \nmode\n=\nblock\n\n          \n-\n \nx-content-type-options\n:\n \nnosniff\n\n          \n-\n \nserver\n:\n \naqueduct\n/\n1\n\n        \n-------------------------\n\n        \n\n \nWhich\n:\n \nthe\n \nbody\n \ndiffers\n \nfor\n \nthe\n \nfollowing\n \nreasons\n:\n\n        \nhas\n \nvalue\n\n        \n{\nindex\n:\n \n1,\n \ndescription\n:\n \nHow\n \nmuch\n \nwood\n \ncan\n \na\n \nwoodchuck\n \nchuck?\n}\n\n          \nwhich\n\n        \n{\nindex\n:\n \n1,\n \ndescription\n:\n \nHow\n \nmuch\n \nwood\n \ncan\n \na\n \nwoodchuck\n \nchuck?\n}\n\n          \nnot\n \na\n \nstring\n \nat\n \nindex\n \n0\n\n\n\n\n\n\nOur tests currently verify that the response body contains a list of JSON strings that all end in \n?\n. However, now that we are returning a list of JSON objects that represent a question, each question has the following form:\n\n\n{\n\n  \nindex\n:\n \n1\n,\n\n  \ndescription\n:\n \nHow much wood can a woodchuck chuck?\n\n\n}\n\n\n\n\n\n\nWe'll need to update our tests so that we're verifying that the \ndescription\n of a question object ends with \n?\n. Wrap the \nendsWith(\"?\")\n matchers in the two tests that use them with \ncontainsPair\n.\n\n\ntest\n(\n/questions returns list of questions\n,\n \n()\n \nasync\n \n{\n\n  \nexpectResponse\n(\n\n    \nawait\n \napp\n.\nclient\n.\nrequest\n(\n/questions\n).\nget\n(),\n\n    \n200\n,\n\n    \nbody:\n \nallOf\n([\n\n      \nhasLength\n(\ngreaterThan\n(\n0\n)),\n\n      \neveryElement\n(\ncontainsPair\n(\ndescription\n,\n \nendsWith\n(\n?\n)))\n\n    \n]));\n\n\n});\n\n\n\ntest\n(\n/questions/index returns a single question\n,\n \n()\n \nasync\n \n{\n\n  \nexpectResponse\n(\n\n    \nawait\n \napp\n.\nclient\n.\nrequest\n(\n/questions/1\n).\nget\n(),\n\n    \n200\n,\n\n    \nbody:\n \ncontainsPair\n(\ndescription\n,\n \nendsWith\n(\n?\n)));\n\n\n});\n\n\n\n\n\n\nRun the tests again by right-clicking on the \nmain\n function and selecting \nRun\n. They should all pass.\n\n\nThe more you know: Query Parameters and HTTP Headers\n\n\nSo far, we have bound HTTP method and path parameters to responder methods in \nQuestionController\n. You can also bind query parameters, headers and request bodies, too.\n\n\nWe'll allow the \ngetAllQuestions\n method to take a query parameter named \ncontains\n. If this query parameter is part of the request, we'll filter the questions on whether or not that question contains some substring. In \nquestion_controller.dart\n, update this method by adding an optional parameter named \ncontainsSubstring\n:\n\n\n@\nhttpGet\n\n\nFuture\nResponse\n \ngetAllQuestions\n({\n@\nHTTPQuery\n(\ncontains\n)\n \nString\n \ncontainsSubstring\n})\n \nasync\n \n{\n\n  \nvar\n \nquestionQuery\n \n=\n \nnew\n \nQuery\nQuestion\n();\n\n  \nif\n \n(\ncontainsSubstring\n \n!=\n \nnull\n)\n \n{\n\n    \nquestionQuery\n.\nwhere\n.\ndescription\n \n=\n \nwhereContainsString\n(\ncontainsSubstring\n);\n\n  \n}\n\n  \nvar\n \ndatabaseQuestions\n \n=\n \nawait\n \nquestionQuery\n.\nfetch\n();\n\n  \nreturn\n \nnew\n \nResponse\n.\nok\n(\ndatabaseQuestions\n);\n\n\n}\n\n\n\n\n\n\nIf an HTTP request has a \ncontains\n query parameter, that value will be available in the \ncontainsSubstring\n variable when this method is invoked. Also, note that we first check \ncontainsSubstring\n to make sure it is not-null. If we simply assigned \nnull\n to \ndescription\n, we'd be creating a matcher that checked to see if the \ndescription\n \ncontained\n \nnull\n.\n\n\n\n\nHTTPController Binding\n\n\nFor more information on binding, see \nthis guide\n.\n\n\n\n\nThen, add a new test in \nquestion_controller_test.dart\n:\n\n\ntest\n(\n/questions returns list of questions filtered by contains\n,\n \n()\n \nasync\n \n{\n\n  \nvar\n \nrequest\n \n=\n \napp\n.\nclient\n.\nrequest\n(\n/questions?contains=mountain\n);\n\n  \nexpectResponse\n(\n\n    \nawait\n \nrequest\n.\nget\n(),\n\n    \n200\n,\n\n    \nbody:\n \n[{\n\n      \nindex\n \n:\n \ngreaterThanOrEqualTo\n(\n0\n),\n\n      \ndescription\n \n:\n \nWhat\ns the tallest mountain in the world?\n\n    \n}]);\n  \n\n});\n\n\n\n\n\n\nThis test expects that the body is a list of exactly one object whose description is the one question we know has the word 'mountain' in it.\n\n\nThis test will pass, along with the rest of them. It's important to note that GET \n/questions\n without a \ncontains\n query still yields the correct results. That is because the \nHTTPQuery\n argument was declared in the optional parameters portion of the responder method. If the parameter were in the required, positional set of parameters and the query string was not included, this request would respond with a 400. (The same positional vs. optional behavior is true of \nHTTPHeader\ns as well.)\n\n\nBinding query and header parameters in a responder method is a good way to make your code more intentional and avoid boilerplate parsing code. Additionally, Aqueduct is able to generate documentation from method signatures - by using bindings, the documentation generator can add that information to the documentation.\n\n\nNext: Relationships and Joins", 
            "title": "3. Executing Database Queries"
        }, 
        {
            "location": "/tut/executing-queries/#3-executing-queries", 
            "text": "We will continue to build on the last chapter's project,  quiz , by storing questions in a database and retrieving them from the database.", 
            "title": "3. Executing Queries"
        }, 
        {
            "location": "/tut/executing-queries/#object-relational-mapping", 
            "text": "For a quick prelude, let's make sure we understand what an object-relational mapping (ORM) is. There are different kinds of databases that have different use cases. A relational database management system - which pretty much means a 'SQL' database - stores its data in the form of tables. A table represents some sort of entity - like a person or a bank account - and has columns that describe the attributes of that entity - like a name or a balance. Every row in a table is an instance of that entity - like a single person named Bob or Bob's bank account.  In an object-oriented framework like Aqueduct, we have a similar concept: a class represents some sort of entity, and an object with that class is an instance of that entity. An ORM maps rows in a database to and from objects in an application. In Aqueduct, each database table-class pairing is called an  entity . Collectively, an application's entities are called its data model.", 
            "title": "Object-Relational Mapping"
        }, 
        {
            "location": "/tut/executing-queries/#building-a-data-model", 
            "text": "In our  quiz  application, we really have one type of entity - a \"question\". We will create an entity by declaring a class whose instances represent a question.  Create a new directory  lib/model/  and then add a new file to it named  lib/model/question.dart . Add the following code:  import   ../quiz.dart ;  class   Question   extends   ManagedObject _Question   implements   _Question   {}  class   _Question   { \n   @ managedPrimaryKey \n   int   index ; \n\n   String   description ;  }   This declares a \"question\" entity. The class  _Question  is a one-to-one mapping to a database table with the same name. Each of its properties -  index  and  description  - are columns in that table. When the ORM fetches rows from the  _Question  table, they will be returned as instances of  Question .   When declaring an entity, there must be two classes:   A  persistent type  that starts with a  _ , has exactly one primary key property, and declares a property for each database column. In the above, the persistent type is  _Question .  An  instance type  that extends  ManagedObject T  (where  T  is the persistent type). The instance type also implements the persistent type so that it has the same properties. In the above, the instance type is  Question .    Why two classes?  The two classes allow for much of the powerful behavior of the Aqueduct ORM. Answering this question requires a understanding of those behaviors, which we won't cover much in this beginner's tutorial. Once you've got the basics down, check out  this guide  for more detail.   During application initialization, the data model gets compiled so that it information about these entities is available at runtime to the components that need it. In  quiz_sink.dart , add the following code to the constructor:  QuizSink ( ApplicationConfiguration   appConfig )   :   super ( appConfig )   { \n   /* This line was added by the template */ \n   logger . onRecord . listen (( rec )   = \n     print ( $ rec   ${ rec . error   ??   }   ${ rec . stackTrace   ??   } )); \n\n   var   dataModel   =   new   ManagedDataModel . fromCurrentMirrorSystem ();  }   The  fromCurrentMirrorSystem  constructor will be able to find all of the  ManagedObject T  subclasses in your application. This data model will be one of the two necessary components for our next task, creating a  ManagedContext .", 
            "title": "Building a Data Model"
        }, 
        {
            "location": "/tut/executing-queries/#defining-a-context", 
            "text": "A  ManagedContext  is an application's gateway to a database. It maintains a connection to a database, executes queries on that database, and translates the results to and from  ManagedObject T s according to a  ManagedDataModel . We have the data model already, so now we must provide database connection information. In  quiz_sink.dart , declare a new property and add a few more lines to the constructor:  class   QuizSink   extends   RequestSink   { \n   QuizSink ( ApplicationConfiguration   options )   :   super ( options )   { \n     logger . onRecord . listen (( rec )   = \n       print ( $ rec   ${ rec . error   ??   }   ${ rec . stackTrace   ??   } )); \n\n     var   dataModel   =   new   ManagedDataModel . fromCurrentMirrorSystem (); \n\n     var   persistentStore   =   new   PostgreSQLPersistentStore . fromConnectionInfo ( \n       dart ,   dart ,   localhost ,   5432 ,   dart_test ); \n\n     context   =   new   ManagedContext ( dataModel ,   persistentStore ); \n   } \n\n   ManagedContext   context ; \n\n   ...   This creates a context that connects to the database  postgres://dart:dart@localhost:5432/dart_test  that has a single  _Question  table. Before we create that database, let's finish writing the code that will fetch questions from a database.", 
            "title": "Defining a Context"
        }, 
        {
            "location": "/tut/executing-queries/#executing-queries", 
            "text": "Once a context has been created, when can execute  Query T s on it. In  lib/controller/question_controller.dart , replace the implementation of  getAllQuestions  to fetch questions from a database and import the file that declares  Question :  import   ../quiz.dart ;  import   ../model/question.dart ;  class   QuestionController   extends   HttpController   { \n   var   questions   =   [ \n     How much wood can a woodchuck chuck? , \n     What s the tallest mountain in the world? \n   ]; \n\n   @ httpGet \n   Future Response   getAllQuestions ()   async   { \n     var   questionQuery   =   new   Query Question (); \n     var   databaseQuestions   =   await   questionQuery . fetch (); \n\n     return   new   Response . ok ( databaseQuestions ); \n   }  ...   In  getAllQuestions , we create an instance of  Query Question  and then execute its  fetch()  method. This method will fetch every question from the database and returns them as a  List Question  into the variable  databaseQuestions . This list is the body object to the  Response , which by default, encodes them as a list of JSON objects.   Where's the context?  Most applications only talk to a single database and therefore have a single  ManagedContext . When creating a  Query T  without specifying a context, the query defaults to executing on the last context created - which happens to be the one we instantiated in  QuizSink .   Now, let's update  getQuestionAtIndex  to fetch a single question by its index from the database.  @ httpGet  Future Response   getQuestionAtIndex ( @ HTTPPath ( index )   int   index )   async   { \n   var   questionQuery   =   new   Query Question () \n     .. where . index   =   whereEqualTo ( index );     \n\n   var   question   =   await   questionQuery . fetchOne (); \n\n   if   ( question   ==   null )   { \n     return   new   Response . notFound (); \n   } \n   return   new   Response . ok ( question );  }   This query does two interesting things: it filters the result of the query by the question's index and only fetches a single question with  fetchOne() .  Filtering is accomplished through the  where  property of a  Query Question ; this property has all of the same properties as  Question .  Matchers  like  whereEqualTo  are set as the values of  where 's properties. This adds conditions to the where clause of the generated SQL query. In the above code, we constrain the query to only fetch questions whose  index  is equal to the  index  from the path variable. Since  index  is the primary key of  Question , this will either give us a single question or return null.   Matching All the Things  There are a lot of matchers available to build different queries. All matchers start with the word  where  and can be found by searching the  API reference .", 
            "title": "Executing Queries"
        }, 
        {
            "location": "/tut/executing-queries/#setting-up-a-database", 
            "text": "Ensure that your PostgreSQL installation is running and open the command-line tool  psql . In this tool, create a new database and a user that can connect to it with the following SQL:  CREATE   DATABASE   dart_test ;  CREATE   USER   dart   WITH   createdb ;  ALTER   USER   dart   WITH   password   dart ;  GRANT   all   ON   database   dart_test   TO   dart ;   The user, password and database name match those provided when creating our  ManagedContext  in  QuizSink . This database will be used during testing. Before the tests run, the test harness will create tables for all of your entities in this database. After the tests complete, the tables will be deleted from this database. Therefore, this database should only be used for running tests.  We must update our test harness to create these tables. In  test/harness/app.dart , add the following method to  TestApplication :  static   Future   createDatabaseSchema ( ManagedContext   context )   async   { \n   var   builder   =   new   SchemaBuilder . toSchema ( \n       context . persistentStore ,   new   Schema . fromDataModel ( context . dataModel ), \n       isTemporary:   true ); \n\n   for   ( var   cmd   in   builder . commands )   { \n     await   context . persistentStore . execute ( cmd ); \n   }  }   Then, in  TestApplication.start , add the line that calls this method after  await application.start() :  Future   start ()   async   { \n   RequestController . letUncaughtExceptionsEscape   =   true ; \n   application   =   new   Application QuizSink (); \n   application . configuration . port   =   0 ; \n   application . configuration . configurationFilePath   =   config.src.yaml ; \n\n   await   application . start ( runOnMainIsolate:   true ); \n\n   /* Add this line */ \n   await   createDatabaseSchema ( sink . context ); \n\n   client   =   new   TestClient ( application );  }   Now when our tests in  question_controller_test.dart  go through their setup process, the SQL command to create the question table will be executed. The last thing left to do is populate this test database with questions during the test setup process. At the top of  question_controller_test.dart , import  question.dart :  import   package:quiz/model/question.dart ;   And then update the  setUpAll  method:  setUpAll (()   async   { \n   await   app . start (); \n\n   var   questions   =   [ \n     new   Question () \n       .. description   =   How much wood can a woodchuck chuck? , \n     new   Question () \n       .. description   =   What s the tallest mountain in the world? , \n   ]; \n\n   await   Future . forEach ( questions ,   ( q )   { \n     var   query   =   new   Query Question () \n         .. values   =   q ; \n     return   query . insert (); \n   });  });   This inserts the two original questions as instances of  Question  into the database during setup. If we were to run our tests now, they would fail and the output would look like this:  Expected :   ---   HTTP   Response   --- \n           -   Status   code   must   be   200 \n           -   Headers   can   be   anything \n           -   Body   after   decoding   must   be : \n\n             ( an   object   with   length   of   a   value   greater   than   0   and   every   element ( a   string   ending   with   ? )) \n           ---------------------  Actual :   TestResponse : ----------- \n         -   Status   code   is   200 \n         -   Headers   are   the   following : \n           -   content-encoding :   gzip \n           -   content-length :   124 \n           -   x-frame-options :   SAMEORIGIN \n           -   content-type :   application / json ;   charset = utf-8 \n           -   x-xss-protection :   1 ;   mode = block \n           -   x-content-type-options :   nosniff \n           -   server :   aqueduct / 1 \n         ------------------------- \n         \n  Which :   the   body   differs   for   the   following   reasons : \n         has   value \n         { index :   1,   description :   How   much   wood   can   a   woodchuck   chuck? } \n           which \n         { index :   1,   description :   How   much   wood   can   a   woodchuck   chuck? } \n           not   a   string   at   index   0   Our tests currently verify that the response body contains a list of JSON strings that all end in  ? . However, now that we are returning a list of JSON objects that represent a question, each question has the following form:  { \n   index :   1 , \n   description :   How much wood can a woodchuck chuck?  }   We'll need to update our tests so that we're verifying that the  description  of a question object ends with  ? . Wrap the  endsWith(\"?\")  matchers in the two tests that use them with  containsPair .  test ( /questions returns list of questions ,   ()   async   { \n   expectResponse ( \n     await   app . client . request ( /questions ). get (), \n     200 , \n     body:   allOf ([ \n       hasLength ( greaterThan ( 0 )), \n       everyElement ( containsPair ( description ,   endsWith ( ? ))) \n     ]));  });  test ( /questions/index returns a single question ,   ()   async   { \n   expectResponse ( \n     await   app . client . request ( /questions/1 ). get (), \n     200 , \n     body:   containsPair ( description ,   endsWith ( ? )));  });   Run the tests again by right-clicking on the  main  function and selecting  Run . They should all pass.", 
            "title": "Setting Up a Database"
        }, 
        {
            "location": "/tut/executing-queries/#the-more-you-know-query-parameters-and-http-headers", 
            "text": "So far, we have bound HTTP method and path parameters to responder methods in  QuestionController . You can also bind query parameters, headers and request bodies, too.  We'll allow the  getAllQuestions  method to take a query parameter named  contains . If this query parameter is part of the request, we'll filter the questions on whether or not that question contains some substring. In  question_controller.dart , update this method by adding an optional parameter named  containsSubstring :  @ httpGet  Future Response   getAllQuestions ({ @ HTTPQuery ( contains )   String   containsSubstring })   async   { \n   var   questionQuery   =   new   Query Question (); \n   if   ( containsSubstring   !=   null )   { \n     questionQuery . where . description   =   whereContainsString ( containsSubstring ); \n   } \n   var   databaseQuestions   =   await   questionQuery . fetch (); \n   return   new   Response . ok ( databaseQuestions );  }   If an HTTP request has a  contains  query parameter, that value will be available in the  containsSubstring  variable when this method is invoked. Also, note that we first check  containsSubstring  to make sure it is not-null. If we simply assigned  null  to  description , we'd be creating a matcher that checked to see if the  description   contained   null .   HTTPController Binding  For more information on binding, see  this guide .   Then, add a new test in  question_controller_test.dart :  test ( /questions returns list of questions filtered by contains ,   ()   async   { \n   var   request   =   app . client . request ( /questions?contains=mountain ); \n   expectResponse ( \n     await   request . get (), \n     200 , \n     body:   [{ \n       index   :   greaterThanOrEqualTo ( 0 ), \n       description   :   What s the tallest mountain in the world? \n     }]);    });   This test expects that the body is a list of exactly one object whose description is the one question we know has the word 'mountain' in it.  This test will pass, along with the rest of them. It's important to note that GET  /questions  without a  contains  query still yields the correct results. That is because the  HTTPQuery  argument was declared in the optional parameters portion of the responder method. If the parameter were in the required, positional set of parameters and the query string was not included, this request would respond with a 400. (The same positional vs. optional behavior is true of  HTTPHeader s as well.)  Binding query and header parameters in a responder method is a good way to make your code more intentional and avoid boilerplate parsing code. Additionally, Aqueduct is able to generate documentation from method signatures - by using bindings, the documentation generator can add that information to the documentation.", 
            "title": "The more you know: Query Parameters and HTTP Headers"
        }, 
        {
            "location": "/tut/executing-queries/#next-relationships-and-joins", 
            "text": "", 
            "title": "Next: Relationships and Joins"
        }, 
        {
            "location": "/tut/model-relationships-and-joins/", 
            "text": "4. Advanced Database Queries\n\n\nManaged objects can also have relationships to other managed objects. There are two types of relationships: to-one and to-many. Let's add an answer for each \nQuestion\n in the form of a to-one relationship. First, create a new file \nlib/model/answer.dart\n and define a new managed object to represent an answer:\n\n\nimport\n \n../quiz.dart\n;\n\n\n\nclass\n \nAnswer\n \nextends\n \nManagedObject\n_Answer\n \nimplements\n \n_Answer\n \n{}\n\n\nclass\n \n_Answer\n \n{\n\n  \n@\nmanagedPrimaryKey\n\n  \nint\n \nid\n;\n\n\n  \nString\n \ndescription\n;\n\n\n}\n\n\n\n\n\n\nNow that we have a managed object that represents both a question and answer, we will set up a relationship between them. It logically makes sense that a 'question \nhas an\n answer', so let's add that property to \n_Question\n, the persistent type of \nQuestion\n:\n\n\n// Don\nt miss this new import!\n\n\nimport\n \nanswer.dart\n;\n\n\n\nclass\n \n_Question\n \n{\n\n  \n@\nmanagedPrimaryKey\n\n  \nint\n \nindex\n;\n\n\n  \nString\n \ndescription\n;\n\n  \nAnswer\n \nanswer\n;\n\n\n}\n\n\n\n\n\n\nFor all relationships, we also must specify the \ninverse relationship\n. The inverse will be a property on \n_Answer\n that points back to the \nQuestion\n it is the answer for. In \n_Answer\n, add the inverse:\n\n\n// Don\nt miss this new import, either!\n\n\nimport\n \nquestion.dart\n;\n\n\n\nclass\n \n_Answer\n \n{\n\n  \n@\nmanagedPrimaryKey\n\n  \nint\n \nid\n;\n\n  \nString\n \ndescription\n;\n\n\n  \n@\nManagedRelationship\n(\n#\nanswer\n)\n\n  \nQuestion\n \nquestion\n;\n\n\n}\n\n\n\n\n\n\nNotice that we added \nManagedRelationship\n metadata to the property \nquestion\n. Since relationships are two-sided, only one side needs to have this metadata (and in fact, only one side \ncan\n have this metadata). The first argument is the name of the property on the other side of the relationship; this is what links the relationship together.\n\n\nThe property with \nManagedRelationship\n metadata maps to the foreign key column in the database table. The table \n_Answer\n has a foreign key column named \nquestion_index\n. (The name is derived by taking the name of the relationship property and name of the primary key property on the other side and joining it with a \n_\n.) The \n_Answer\n table now has three columns: \nid\n, \ndescription\n and \nquestion_index\n.\n\n\nThe relationship property \nwithout\n \nManagedRelationship\n metadata is \nnot\n a column in a table. Instead, it represents an \nentire row\n in another table. Thus, the table \n_Question\n only has two columns: \nindex\n and \ndescription\n.\n\n\nManagedRelationship\n also allows you to specify a delete rule and whether or not the property is required, i.e., not nullable. By default, the delete rule is \nManagedRelationshipDeleteRule.nullify\n and not required - this is the least destructive action. But, in this case, we want every question to always have an answer and if we delete the question, the answer gets deleted along with it:\n\n\nclass\n \n_Answer\n \n{\n\n  \n@\nprimaryKey\n\n  \nint\n \nid\n;\n\n  \nString\n \ndescription\n;\n\n\n  \n@\nManagedRelationship\n(\n\n    \n#\nanswer\n,\n \nonDelete:\n \nManagedRelationshipDeleteRule\n.\ncascade\n,\n \nisRequired:\n \ntrue\n)\n\n  \nQuestion\n \nquestion\n;\n\n\n}\n\n\n\n\n\n\nNow that we have defined this relationship, we can associate answers with questions and return them in our \n/questions\n endpoint. In \nquestion_controller.dart\n, let's update the queries to fetch the \nAnswer\n for each \nQuestion\n and include it in the response JSON. First, for \ngetAllQuestions\n, use \njoin()\n to include the answers:\n\n\n@\nhttpGet\n\n\nFuture\nResponse\n \ngetAllQuestions\n({\n@\nHTTPQuery\n(\ncontains\n)\n \nString\n \ncontainsSubstring:\n \nnull\n})\n \nasync\n \n{\n\n  \nvar\n \nquestionQuery\n \n=\n \nnew\n \nQuery\nQuestion\n()\n\n    \n..\njoin\n(\nobject:\n \n(\nquestion\n)\n \n=\n \nquestion\n.\nanswer\n);\n\n\n  \nif\n \n(\ncontainsSubstring\n \n!=\n \nnull\n)\n \n{\n\n    \nquestionQuery\n.\nwhere\n.\ndescription\n \n=\n \nwhereContainsString\n(\ncontainsSubstring\n);\n\n  \n}\n\n\n  \nvar\n \nquestions\n \n=\n \nawait\n \nquestionQuery\n.\nfetch\n();\n\n  \nreturn\n \nnew\n \nResponse\n.\nok\n(\nquestions\n);\n\n\n}\n\n\n\n\n\n\nAnd same for \ngetQuestionAtIndex\n:\n\n\n@\nhttpGet\n\n\nFuture\nResponse\n \ngetQuestionAtIndex\n(\n@\nHTTPPath\n(\nindex\n)\n \nint\n \nindex\n)\n \nasync\n \n{\n\n  \nvar\n \nquestionQuery\n \n=\n \nnew\n \nQuery\nQuestion\n()\n\n    \n..\njoin\n(\nobject:\n \n(\nquestion\n)\n \n=\n \nquestion\n.\nanswer\n)\n\n    \n..\nwhere\n.\nindex\n \n=\n \nwhereEqualTo\n(\nindex\n);\n\n\n  \nvar\n \nquestion\n \n=\n \nawait\n \nquestionQuery\n.\nfetchOne\n();\n\n\n  \nif\n \n(\nquestion\n \n==\n \nnull\n)\n \n{\n\n    \nreturn\n \nnew\n \nResponse\n.\nnotFound\n();\n\n  \n}\n\n  \nreturn\n \nnew\n \nResponse\n.\nok\n(\nquestion\n);\n\n\n}\n\n\n\n\n\n\nThe SQL that gets built for this \nQuery\nT\n will join on the underlying \n_Answer\n table. Therefore, each \nanswer\n property of every \nQuestion\n returned will have a valid \nAnswer\n instance from the database. Managed objects also know how to serialize their relationship properties, so you'll get the following JSON when fetching a question that has been joined with its answer:\n\n\n{\n\n  \nindex\n \n:\n \n1\n,\n\n  \ndescription\n \n:\n \nA question?\n,\n\n  \nanswer\n \n:\n \n{\n\n      \nid\n \n:\n \n1\n,\n\n      \ndescription\n \n:\n \nAn answer\n\n  \n}\n\n\n}\n\n\n\n\n\n\nLet's update our tests to ensure this works correctly. If you run your tests now, the two tests that get a list of \nQuestion\ns will fail because they don't expect an answer key in the JSON. Now, we don't really care about the 'id' of the answer at all, just its 'description'. That's why there is the \npartial\n matcher. A \npartial\n matcher will match a \nMap\n, but will only verify the values for the specified keys. Any other key-value pairs are just ignored. Update the following tests:\n\n\ntest\n(\n/questions returns list of questions\n,\n \n()\n \nasync\n \n{\n\n  \nexpectResponse\n(\n\n    \nawait\n \napp\n.\nclient\n.\nrequest\n(\n/questions\n).\nget\n(),\n\n    \n200\n,\n\n    \nbody:\n \nallOf\n([\n\n      \nhasLength\n(\ngreaterThan\n(\n0\n)),\n\n      \neveryElement\n(\npartial\n({\n\n        \ndescription\n:\n \nendsWith\n(\n?\n),\n\n        \nanswer\n:\n \npartial\n({\n\n          \ndescription\n:\n \nisString\n\n        \n})\n\n      \n}))]));\n\n\n});\n\n\n\ntest\n(\n/questions/index returns a single question\n,\n \n()\n \nasync\n \n{\n\n  \nexpectResponse\n(\n\n    \nawait\n \napp\n.\nclient\n.\nrequest\n(\n/questions/1\n).\nget\n(),\n\n    \n200\n,\n\n    \nbody:\n \npartial\n({\n\n      \ndescription\n:\n \nendsWith\n(\n?\n),\n\n      \nanswer\n:\n \npartial\n({\n\n        \ndescription\n:\n \nisString\n\n      \n})\n\n    \n}));\n\n\n});\n\n\n\ntest\n(\n/questions returns list of questions filtered by contains\n,\n \n()\n \nasync\n \n{\n\n  \nvar\n \nrequest\n \n=\n \napp\n.\nclient\n.\nrequest\n(\n/questions?contains=mountain\n);\n\n  \nexpectResponse\n(\n\n    \nawait\n \nrequest\n.\nget\n(),\n\n    \n200\n,\n\n    \nbody:\n \n[{\n\n      \nindex\n \n:\n \ngreaterThanOrEqualTo\n(\n0\n),\n\n      \ndescription\n \n:\n \nWhat\ns the tallest mountain in the world?\n,\n\n      \nanswer\n:\n \npartial\n({\n\n        \ndescription\n:\n \nMount Everest\n\n      \n})\n\n    \n}]);\n\n\n});\n\n\n\n\n\n\nIf you run the tests now, this test will still fail - 'answer' in the JSON is null because there are no answers in the test database. Let's insert some by adding to \nsetUpAll\n in \nquestion_controller_test.dart\n:\n\n\n// Don\nt forget to add this import, too!\n\n\nimport\n \npackage:quiz/model/answer.dart\n;\n\n\n\nvoid\n \nmain\n()\n \n{\n\n  \nsetUpAll\n(()\n \nasync\n \n{\n\n    \nawait\n \napp\n.\nstart\n();\n\n\n    \nvar\n \nquestions\n \n=\n \n[\n\n      \nnew\n \nQuestion\n()\n\n        \n..\ndescription\n \n=\n \nHow much wood can a woodchuck chuck?\n\n        \n..\nanswer\n \n=\n \n(\nnew\n \nAnswer\n()..\ndescription\n \n=\n \nDepends\n),\n\n      \nnew\n \nQuestion\n()\n\n        \n..\ndescription\n \n=\n \nWhat\ns the tallest mountain in the world?\n\n        \n..\nanswer\n \n=\n \n(\nnew\n \nAnswer\n()..\ndescription\n \n=\n \nMount Everest\n),\n\n    \n];\n\n\n    \nawait\n \nFuture\n.\nforEach\n(\nquestions\n,\n \n(\nQuestion\n \nq\n)\n \nasync\n \n{\n\n      \nvar\n \nquery\n \n=\n \nnew\n \nQuery\nQuestion\n()\n\n          \n..\nvalues\n \n=\n \nq\n;\n\n      \nvar\n \ninsertedQuestion\n \n=\n \nawait\n \nquery\n.\ninsert\n();\n\n\n      \nvar\n \nanswerQuery\n \n=\n \nnew\n \nQuery\nAnswer\n()\n\n        \n..\nvalues\n.\ndescription\n \n=\n \nq\n.\nanswer\n.\ndescription\n\n        \n..\nvalues\n.\nquestion\n \n=\n \ninsertedQuestion\n;\n\n      \nawait\n \nanswerQuery\n.\ninsert\n();\n\n      \nreturn\n \ninsertedQuestion\n;\n\n    \n});\n\n  \n});\n\n\n\n\n\n\nNotice that we accumulated all of the questions and answers into a list of questions where each has an answer (\nquestions\n). Managed objects can be used just like normal objects, too.\n\n\nThen, for each question, we inserted it and got a reference to the \ninsertedQuestion\n back. The difference between each \nQuestion\n in \nquestions\n and \ninsertedQuestion\n is that the \ninsertedQuestion\n will have its primary key value (\nindex\n) set by the database. This allows the \nAnswer\ns - which have to be inserted separately, because they are different tables - to specify which question they are the answer for.\n\n\nRecall that a property with \nManagedRelationship\n - like \nAnswer.question\n - is actually a foreign key column. When setting this property with a \nManagedObject\nT\n, the primary key value of that instance is sent as the value for the foreign key column. In this case, the \ninsertedQuestion\n has valid values for both \ndescription\n and \nindex\n. Setting the query's \nvalues.question\n to this instance ignores the \ndescription\n - it's not going to store it anyway - and sets the \nindex\n of the answer being inserted.\n\n\nNote, also, that the query to insert a question has \nvalues\n that contain an answer. These answers will be ignored during that insertion, because only the question is being inserted. Inserting or updating values will only operate on one table at a table - this is intentional explicit to avoid unintended consequences.\n\n\nThe tests will now pass.\n\n\nMore on Joins and Relationships\n\n\nRelationships can also be 'has-many'. For example, if you wanted many answers for a question, we'd use a \nManagedSet\nT\n:\n\n\nclass\n \n_Question\n \n{\n\n  \n@\nmanagedPrimaryKey\n\n  \nint\n \nindex\n;\n\n\n  \nString\n \ndescription\n;\n\n  \nManagedSet\nAnswer\n \nanswers\n;\n\n\n}\n\n\n\n\n\n\nThe inverse relationship doesn't have to be updated. For \nManagedSet\nT\n, \nT\n must be a subclass of \nManagedObject\nT\n. A \nManagedSet\n acts just like a \nList\n - it has methods like \nmap\n and \nwhere\n - but also has special behavior that allows it to be used in building \nQuery\nT\ns. If you wish to join on \nManagedSet\nT\n properties, the syntax is similar:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nQuestion\n()\n\n  \n..\njoin\n(\nset\n:\n \n(\nquestion\n)\n \n=\n \nquestion\n.\nanswers\n);\n  \n\n\n\n\n\nEach returned \nQuestion\n would also have a \nManagedSet\n of \nAnswer\ns in its \nanswers\n property. You may also filter which answers are returned for each \nQuestion\n. A \njoin\n creates a new \nQuery\nT\n that has its own \nwhere\n property.\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nQuestion\n();\n\n\n\nquery\n.\njoin\n(\nset\n:\n \n(\nquestion\n)\n \n=\n \nquestion\n.\nanswers\n)\n\n  \n..\nwhere\n.\nisCorrect\n \n=\n \nwhereEqualTo\n(\ntrue\n);\n  \n\n\nvar\n \nquestionsWithCorrectAnswers\n \n=\n \nawait\n \nquery\n.\nfetch\n();\n\n\n\n\n\n\nAn \nManagedSet\n is serialized into a \nList\n of \nMap\ns, and therefore the encoded JSON will be an array of objects.\n\n\nNext: Deployment", 
            "title": "4. Advanced Database Queries"
        }, 
        {
            "location": "/tut/model-relationships-and-joins/#4-advanced-database-queries", 
            "text": "Managed objects can also have relationships to other managed objects. There are two types of relationships: to-one and to-many. Let's add an answer for each  Question  in the form of a to-one relationship. First, create a new file  lib/model/answer.dart  and define a new managed object to represent an answer:  import   ../quiz.dart ;  class   Answer   extends   ManagedObject _Answer   implements   _Answer   {}  class   _Answer   { \n   @ managedPrimaryKey \n   int   id ; \n\n   String   description ;  }   Now that we have a managed object that represents both a question and answer, we will set up a relationship between them. It logically makes sense that a 'question  has an  answer', so let's add that property to  _Question , the persistent type of  Question :  // Don t miss this new import!  import   answer.dart ;  class   _Question   { \n   @ managedPrimaryKey \n   int   index ; \n\n   String   description ; \n   Answer   answer ;  }   For all relationships, we also must specify the  inverse relationship . The inverse will be a property on  _Answer  that points back to the  Question  it is the answer for. In  _Answer , add the inverse:  // Don t miss this new import, either!  import   question.dart ;  class   _Answer   { \n   @ managedPrimaryKey \n   int   id ; \n   String   description ; \n\n   @ ManagedRelationship ( # answer ) \n   Question   question ;  }   Notice that we added  ManagedRelationship  metadata to the property  question . Since relationships are two-sided, only one side needs to have this metadata (and in fact, only one side  can  have this metadata). The first argument is the name of the property on the other side of the relationship; this is what links the relationship together.  The property with  ManagedRelationship  metadata maps to the foreign key column in the database table. The table  _Answer  has a foreign key column named  question_index . (The name is derived by taking the name of the relationship property and name of the primary key property on the other side and joining it with a  _ .) The  _Answer  table now has three columns:  id ,  description  and  question_index .  The relationship property  without   ManagedRelationship  metadata is  not  a column in a table. Instead, it represents an  entire row  in another table. Thus, the table  _Question  only has two columns:  index  and  description .  ManagedRelationship  also allows you to specify a delete rule and whether or not the property is required, i.e., not nullable. By default, the delete rule is  ManagedRelationshipDeleteRule.nullify  and not required - this is the least destructive action. But, in this case, we want every question to always have an answer and if we delete the question, the answer gets deleted along with it:  class   _Answer   { \n   @ primaryKey \n   int   id ; \n   String   description ; \n\n   @ ManagedRelationship ( \n     # answer ,   onDelete:   ManagedRelationshipDeleteRule . cascade ,   isRequired:   true ) \n   Question   question ;  }   Now that we have defined this relationship, we can associate answers with questions and return them in our  /questions  endpoint. In  question_controller.dart , let's update the queries to fetch the  Answer  for each  Question  and include it in the response JSON. First, for  getAllQuestions , use  join()  to include the answers:  @ httpGet  Future Response   getAllQuestions ({ @ HTTPQuery ( contains )   String   containsSubstring:   null })   async   { \n   var   questionQuery   =   new   Query Question () \n     .. join ( object:   ( question )   =   question . answer ); \n\n   if   ( containsSubstring   !=   null )   { \n     questionQuery . where . description   =   whereContainsString ( containsSubstring ); \n   } \n\n   var   questions   =   await   questionQuery . fetch (); \n   return   new   Response . ok ( questions );  }   And same for  getQuestionAtIndex :  @ httpGet  Future Response   getQuestionAtIndex ( @ HTTPPath ( index )   int   index )   async   { \n   var   questionQuery   =   new   Query Question () \n     .. join ( object:   ( question )   =   question . answer ) \n     .. where . index   =   whereEqualTo ( index ); \n\n   var   question   =   await   questionQuery . fetchOne (); \n\n   if   ( question   ==   null )   { \n     return   new   Response . notFound (); \n   } \n   return   new   Response . ok ( question );  }   The SQL that gets built for this  Query T  will join on the underlying  _Answer  table. Therefore, each  answer  property of every  Question  returned will have a valid  Answer  instance from the database. Managed objects also know how to serialize their relationship properties, so you'll get the following JSON when fetching a question that has been joined with its answer:  { \n   index   :   1 , \n   description   :   A question? , \n   answer   :   { \n       id   :   1 , \n       description   :   An answer \n   }  }   Let's update our tests to ensure this works correctly. If you run your tests now, the two tests that get a list of  Question s will fail because they don't expect an answer key in the JSON. Now, we don't really care about the 'id' of the answer at all, just its 'description'. That's why there is the  partial  matcher. A  partial  matcher will match a  Map , but will only verify the values for the specified keys. Any other key-value pairs are just ignored. Update the following tests:  test ( /questions returns list of questions ,   ()   async   { \n   expectResponse ( \n     await   app . client . request ( /questions ). get (), \n     200 , \n     body:   allOf ([ \n       hasLength ( greaterThan ( 0 )), \n       everyElement ( partial ({ \n         description :   endsWith ( ? ), \n         answer :   partial ({ \n           description :   isString \n         }) \n       }))]));  });  test ( /questions/index returns a single question ,   ()   async   { \n   expectResponse ( \n     await   app . client . request ( /questions/1 ). get (), \n     200 , \n     body:   partial ({ \n       description :   endsWith ( ? ), \n       answer :   partial ({ \n         description :   isString \n       }) \n     }));  });  test ( /questions returns list of questions filtered by contains ,   ()   async   { \n   var   request   =   app . client . request ( /questions?contains=mountain ); \n   expectResponse ( \n     await   request . get (), \n     200 , \n     body:   [{ \n       index   :   greaterThanOrEqualTo ( 0 ), \n       description   :   What s the tallest mountain in the world? , \n       answer :   partial ({ \n         description :   Mount Everest \n       }) \n     }]);  });   If you run the tests now, this test will still fail - 'answer' in the JSON is null because there are no answers in the test database. Let's insert some by adding to  setUpAll  in  question_controller_test.dart :  // Don t forget to add this import, too!  import   package:quiz/model/answer.dart ;  void   main ()   { \n   setUpAll (()   async   { \n     await   app . start (); \n\n     var   questions   =   [ \n       new   Question () \n         .. description   =   How much wood can a woodchuck chuck? \n         .. answer   =   ( new   Answer ().. description   =   Depends ), \n       new   Question () \n         .. description   =   What s the tallest mountain in the world? \n         .. answer   =   ( new   Answer ().. description   =   Mount Everest ), \n     ]; \n\n     await   Future . forEach ( questions ,   ( Question   q )   async   { \n       var   query   =   new   Query Question () \n           .. values   =   q ; \n       var   insertedQuestion   =   await   query . insert (); \n\n       var   answerQuery   =   new   Query Answer () \n         .. values . description   =   q . answer . description \n         .. values . question   =   insertedQuestion ; \n       await   answerQuery . insert (); \n       return   insertedQuestion ; \n     }); \n   });   Notice that we accumulated all of the questions and answers into a list of questions where each has an answer ( questions ). Managed objects can be used just like normal objects, too.  Then, for each question, we inserted it and got a reference to the  insertedQuestion  back. The difference between each  Question  in  questions  and  insertedQuestion  is that the  insertedQuestion  will have its primary key value ( index ) set by the database. This allows the  Answer s - which have to be inserted separately, because they are different tables - to specify which question they are the answer for.  Recall that a property with  ManagedRelationship  - like  Answer.question  - is actually a foreign key column. When setting this property with a  ManagedObject T , the primary key value of that instance is sent as the value for the foreign key column. In this case, the  insertedQuestion  has valid values for both  description  and  index . Setting the query's  values.question  to this instance ignores the  description  - it's not going to store it anyway - and sets the  index  of the answer being inserted.  Note, also, that the query to insert a question has  values  that contain an answer. These answers will be ignored during that insertion, because only the question is being inserted. Inserting or updating values will only operate on one table at a table - this is intentional explicit to avoid unintended consequences.  The tests will now pass.", 
            "title": "4. Advanced Database Queries"
        }, 
        {
            "location": "/tut/model-relationships-and-joins/#more-on-joins-and-relationships", 
            "text": "Relationships can also be 'has-many'. For example, if you wanted many answers for a question, we'd use a  ManagedSet T :  class   _Question   { \n   @ managedPrimaryKey \n   int   index ; \n\n   String   description ; \n   ManagedSet Answer   answers ;  }   The inverse relationship doesn't have to be updated. For  ManagedSet T ,  T  must be a subclass of  ManagedObject T . A  ManagedSet  acts just like a  List  - it has methods like  map  and  where  - but also has special behavior that allows it to be used in building  Query T s. If you wish to join on  ManagedSet T  properties, the syntax is similar:  var   query   =   new   Query Question () \n   .. join ( set :   ( question )   =   question . answers );     Each returned  Question  would also have a  ManagedSet  of  Answer s in its  answers  property. You may also filter which answers are returned for each  Question . A  join  creates a new  Query T  that has its own  where  property.  var   query   =   new   Query Question ();  query . join ( set :   ( question )   =   question . answers ) \n   .. where . isCorrect   =   whereEqualTo ( true );    var   questionsWithCorrectAnswers   =   await   query . fetch ();   An  ManagedSet  is serialized into a  List  of  Map s, and therefore the encoded JSON will be an array of objects.", 
            "title": "More on Joins and Relationships"
        }, 
        {
            "location": "/tut/model-relationships-and-joins/#next-deployment", 
            "text": "", 
            "title": "Next: Deployment"
        }, 
        {
            "location": "/tut/deploying-and-other-fun-things/", 
            "text": "5. Deploying an Aqueduct Application\n\n\nThe last chapter is a quick one - we'll get our application and its database running locally. When writing tests, the harness creates temporary tables that are destroyed when the tests end. Those tables are created in a database named \ndart_test\n that is exclusively used for this purpose. All of your projects will use this same database for running tests.\n\n\nTo run the application outside of the tests, you'll need another database. Run the \npsql\n command-line tool and enter the following SQL:\n\n\nCREATE\n \nDATABASE\n \nquiz\n;\n\n\nCREATE\n \nUSER\n \nquiz_user\n \nWITH\n \ncreatedb\n;\n\n\nALTER\n \nUSER\n \nquiz_user\n \nWITH\n \npassword\n \nquizzy\n;\n\n\nGRANT\n \nall\n \nON\n \ndatabase\n \nquiz\n \nTO\n \nquiz_user\n;\n\n\n\n\n\n\nThis creates a database \nquiz\n that \nquiz_user\n has access to. Now, add \nquiz\n's data model to this database by running the following commands in the project directory:\n\n\naqueduct db generate\naqueduct db upgrade --connect postgres://quiz_user:quizzy@localhost:5432/quiz\n\n\n\n\n\nThe first command generates a migration file in \nmigrations/\n that adds tables \n_Question\n and \n_Answer\n, and the second command executes that migration file on the newly created database.\n\n\nAfter adding the data model to the \nquiz\n database, run the following commands in \npsql\n to insert a question and answer:\n\n\n\\\nc\n \nquiz\n\n\nINSERT\n \nINTO\n \n_question\n \n(\ndescription\n)\n \nVALUES\n \n(\nWhat is 1+1?\n);\n\n\nINSERT\n \nINTO\n \n_answer\n \n(\ndescription\n,\n \nquestion_index\n)\n \nVALUES\n \n(\n2\n,\n \n1\n);\n\n\n\n\n\n\nThe application is currently hard-coded to connect to the test database. We'll write a bit of code to read connection info from a YAML configuration file instead. At the bottom of \nquiz_sink.dart\n, create a \nConfigurationItem\n subclass:\n\n\nclass\n \nQuizConfig\n \nextends\n \nConfigurationItem\n \n{\n\n  \nQuizConfig\n(\nString\n \nfilename\n)\n \n:\n \nsuper\n.\nfromFile\n(\nfilename\n);\n\n\n  \nDatabaseConnectionConfiguration\n \ndatabase\n;\n\n\n}\n\n\n\n\n\n\nUpdate \nQuizSink\n's constructor to create its persistent store from configuration values:\n\n\nQuizSink\n(\nApplicationConfiguration\n \nappConfig\n)\n \n:\n \nsuper\n(\nappConfig\n)\n \n{\n\n  \nlogger\n.\nonRecord\n.\nlisten\n((\nrec\n)\n \n=\n\n    \nprint\n(\n$\nrec\n \n${\nrec\n.\nerror\n \n??\n \n}\n \n${\nrec\n.\nstackTrace\n \n??\n \n}\n));\n\n  \nvar\n \ndataModel\n \n=\n \nnew\n \nManagedDataModel\n.\nfromCurrentMirrorSystem\n();\n\n\n  \nvar\n \nconfigValues\n \n=\n \nnew\n \nQuizConfig\n(\nappConfig\n.\nconfigurationFilePath\n);\n\n\n  \nvar\n \npersistentStore\n \n=\n \nnew\n \nPostgreSQLPersistentStore\n.\nfromConnectionInfo\n(\n\n      \nconfigValues\n.\ndatabase\n.\nusername\n,\n\n      \nconfigValues\n.\ndatabase\n.\npassword\n,\n\n      \nconfigValues\n.\ndatabase\n.\nhost\n,\n\n      \nconfigValues\n.\ndatabase\n.\nport\n,\n\n      \nconfigValues\n.\ndatabase\n.\ndatabaseName\n);\n\n  \ncontext\n \n=\n \nnew\n \nManagedContext\n(\ndataModel\n,\n \npersistentStore\n);\n\n\n}\n\n\n\n\n\n\nFinally, create the file \nconfig.yaml\n in the root of the project directory and add the following key-values pairs:\n\n\ndatabase\n:\n\n \nusername\n:\n \nquiz_user\n\n \npassword\n:\n \nquizzy\n\n \nhost\n:\n \nlocalhost\n\n \nport\n:\n \n5432\n\n \ndatabaseName\n:\n \nquiz\n\n\n\n\n\n\nRun \naqueduct serve\n and open a browser to \nhttp://localhost:8081/questions\n - you'll see the question in your database. For other ways of running an Aqueduct application (and tips for running them remotely), see \nthis guide\n.\n\n\n\n\nTest and Deployment Configuration\n\n\nThe \nconfigurationFilePath\n defaults to \nconfig.yaml\n when using \naqueduct serve\n. In the test harness, the \nconfigurationFilePath\n is set to \nconfig.src.yaml\n. To continue running the tests, add the database connection configuration for \ndart_test\n database to the file \nconfig.src.yaml\n. For more details on configuration, see \nthis guide\n.\n\n\n\n\nOnward\n\n\nWe've only touched on a small part of Aqueduct, but we've hit the fundamentals pretty well. The rest of the guides on this site will take you deeper on these topics, and topics we haven't covered like OAuth 2.0.\n\n\nIt's very important that you get comfortable using the \nAPI reference\n in addition to these guides. If you are looking to solve a problem, start by looking at the API reference for all of the objects you have access to (including the type you are writing the method for). The properties and methods you have access to will lead you to more properties and methods that'll eventually do what you want done.\n\n\nUsers of the documentation viewer \nDash\n can add Aqueduct through the \nPreferences\n pane, under \nDownloads\n.\n\n\nThere are IntelliJ IDEA file and code templates available for Aqueduct. See \nthis guide\n for installation instructions and usage. It takes 10 seconds and it'll save you a ton of time overall.\n\n\nAnd lastly, remember to create a new project:\n\n\naqueduct create my_next_big_idea", 
            "title": "5. Deploying"
        }, 
        {
            "location": "/tut/deploying-and-other-fun-things/#5-deploying-an-aqueduct-application", 
            "text": "The last chapter is a quick one - we'll get our application and its database running locally. When writing tests, the harness creates temporary tables that are destroyed when the tests end. Those tables are created in a database named  dart_test  that is exclusively used for this purpose. All of your projects will use this same database for running tests.  To run the application outside of the tests, you'll need another database. Run the  psql  command-line tool and enter the following SQL:  CREATE   DATABASE   quiz ;  CREATE   USER   quiz_user   WITH   createdb ;  ALTER   USER   quiz_user   WITH   password   quizzy ;  GRANT   all   ON   database   quiz   TO   quiz_user ;   This creates a database  quiz  that  quiz_user  has access to. Now, add  quiz 's data model to this database by running the following commands in the project directory:  aqueduct db generate\naqueduct db upgrade --connect postgres://quiz_user:quizzy@localhost:5432/quiz  The first command generates a migration file in  migrations/  that adds tables  _Question  and  _Answer , and the second command executes that migration file on the newly created database.  After adding the data model to the  quiz  database, run the following commands in  psql  to insert a question and answer:  \\ c   quiz  INSERT   INTO   _question   ( description )   VALUES   ( What is 1+1? );  INSERT   INTO   _answer   ( description ,   question_index )   VALUES   ( 2 ,   1 );   The application is currently hard-coded to connect to the test database. We'll write a bit of code to read connection info from a YAML configuration file instead. At the bottom of  quiz_sink.dart , create a  ConfigurationItem  subclass:  class   QuizConfig   extends   ConfigurationItem   { \n   QuizConfig ( String   filename )   :   super . fromFile ( filename ); \n\n   DatabaseConnectionConfiguration   database ;  }   Update  QuizSink 's constructor to create its persistent store from configuration values:  QuizSink ( ApplicationConfiguration   appConfig )   :   super ( appConfig )   { \n   logger . onRecord . listen (( rec )   = \n     print ( $ rec   ${ rec . error   ??   }   ${ rec . stackTrace   ??   } )); \n   var   dataModel   =   new   ManagedDataModel . fromCurrentMirrorSystem (); \n\n   var   configValues   =   new   QuizConfig ( appConfig . configurationFilePath ); \n\n   var   persistentStore   =   new   PostgreSQLPersistentStore . fromConnectionInfo ( \n       configValues . database . username , \n       configValues . database . password , \n       configValues . database . host , \n       configValues . database . port , \n       configValues . database . databaseName ); \n   context   =   new   ManagedContext ( dataModel ,   persistentStore );  }   Finally, create the file  config.yaml  in the root of the project directory and add the following key-values pairs:  database : \n  username :   quiz_user \n  password :   quizzy \n  host :   localhost \n  port :   5432 \n  databaseName :   quiz   Run  aqueduct serve  and open a browser to  http://localhost:8081/questions  - you'll see the question in your database. For other ways of running an Aqueduct application (and tips for running them remotely), see  this guide .   Test and Deployment Configuration  The  configurationFilePath  defaults to  config.yaml  when using  aqueduct serve . In the test harness, the  configurationFilePath  is set to  config.src.yaml . To continue running the tests, add the database connection configuration for  dart_test  database to the file  config.src.yaml . For more details on configuration, see  this guide .", 
            "title": "5. Deploying an Aqueduct Application"
        }, 
        {
            "location": "/tut/deploying-and-other-fun-things/#onward", 
            "text": "We've only touched on a small part of Aqueduct, but we've hit the fundamentals pretty well. The rest of the guides on this site will take you deeper on these topics, and topics we haven't covered like OAuth 2.0.  It's very important that you get comfortable using the  API reference  in addition to these guides. If you are looking to solve a problem, start by looking at the API reference for all of the objects you have access to (including the type you are writing the method for). The properties and methods you have access to will lead you to more properties and methods that'll eventually do what you want done.  Users of the documentation viewer  Dash  can add Aqueduct through the  Preferences  pane, under  Downloads .  There are IntelliJ IDEA file and code templates available for Aqueduct. See  this guide  for installation instructions and usage. It takes 10 seconds and it'll save you a ton of time overall.  And lastly, remember to create a new project:  aqueduct create my_next_big_idea", 
            "title": "Onward"
        }, 
        {
            "location": "/http/overview/", 
            "text": "Tasks\n\n\nAqueduct applications respond to HTTP requests. The main concepts and tasks are:\n\n\n\n\nUsing a \nRouter\n to determine which code is run for an HTTP request\n\n\nBinding\n the values from an HTTP request to the parameters of a method with \nHTTPController\n\n\nSetting up routes and initializing an application by subclassing \nRequestSink\n\n\nStarting and stopping Aqueduct Applications with \naqueduct serve\n\n\nBinding an REST interface to a database table with \nManagedObjectController\nT\n\n\nEncoding and Decoding HTTP request and response bodies according to \nHTTPCodecRepository\n\n\nBuilding pipelines with middleware\n\n\n\n\nGuides\n\n\n\n\nArchitecture and Organization of Aqueduct Applications\n\n\nRequest and Response Objects\n\n\nHandling Requests\n\n\nThe RequestSink\n\n\nRouting\n\n\nHTTPControllers\n\n\nConfiguration Files, CORS and SSL\n\n\nServing Files and Caching\n\n\nWebsockets\n\n\nMulti-threading", 
            "title": "Overview"
        }, 
        {
            "location": "/http/overview/#tasks", 
            "text": "Aqueduct applications respond to HTTP requests. The main concepts and tasks are:   Using a  Router  to determine which code is run for an HTTP request  Binding  the values from an HTTP request to the parameters of a method with  HTTPController  Setting up routes and initializing an application by subclassing  RequestSink  Starting and stopping Aqueduct Applications with  aqueduct serve  Binding an REST interface to a database table with  ManagedObjectController T  Encoding and Decoding HTTP request and response bodies according to  HTTPCodecRepository  Building pipelines with middleware", 
            "title": "Tasks"
        }, 
        {
            "location": "/http/overview/#guides", 
            "text": "Architecture and Organization of Aqueduct Applications  Request and Response Objects  Handling Requests  The RequestSink  Routing  HTTPControllers  Configuration Files, CORS and SSL  Serving Files and Caching  Websockets  Multi-threading", 
            "title": "Guides"
        }, 
        {
            "location": "/http/structure/", 
            "text": "Aqueduct Application Architecture\n\n\nThe most important object in Aqueduct is a \nRequestController\n. A \nRequestController\n is the only thing that can respond to HTTP requests. The logic for an application is written in the methods of this type and its subclasses.\n\n\nRequest controllers are chained together to create a \nrequest channel\n. A request channel is a series of request controllers that a request flows through to be verified, modified and responded to.\n\n\n\n\nA request channel always starts at an instance of \nRequestSink\n (a subclass of \nRequestController\n). When an application receives an HTTP request, it adds it to the \nRequestSink\n. A \nRequestSink\n has a \nRouter\n (also a subclass of \nRequestController\n) that splits the channel based on the path of the request. For example, a request with the path \n/users\n will go down one part of the channel, while a \n/things\n request will go down another.\n\n\nAn application has exactly one \nRequestSink\n subclass; it is responsible for defining its application's request channel by overriding the method \nsetupRouter\n. For example, the diagram above looks like this in code:\n\n\nclass\n \nMyRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nMyRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n);\n\n\n  \n@\noverride\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n    \nrouter\n\n      \n.\nroute\n(\n/a\n)\n\n      \n.\ngenerate\n(()\n \n=\n \nnew\n \nAController\n());\n\n\n    \nrouter\n\n      \n.\nroute\n(\n/b\n)\n\n      \n.\npipe\n(\nnew\n \nAuthorizer\n(...))\n\n      \n.\ngenerate\n(()\n \n=\n \nnew\n \nBController\n());\n\n\n    \nrouter\n\n      \n.\nroute\n(\n/c\n)\n\n      \n.\npipe\n(\nnew\n \nAuthorizer\n(...))\n\n      \n.\ngenerate\n(()\n \n=\n \nnew\n \nCController\n());\n      \n  \n}\n\n\n}\n\n\n\n\n\n\nIn the above code, all of the objects created are instances of a \nRequestController\n subclass. Each of these controllers can either respond to the request, or send it to the next controller in the channel. For example, an \nAuthorizer\n will respond with a 401 Unauthorized response if a request's authorization isn't valid - but if it is valid, the request is passed to the next controller in the channel.\n\n\nFor more details on the object that handles requests, see \nRequest Controllers\n.\n\n\nIsolates: Multi-threaded Applications\n\n\nAn Aqueduct application may run its request channel on multiple isolates. The number of isolates is configured when running \naqueduct serve\n.\n\n\naqueduct serve --isolates 3\n\n\n\n\n\nAn isolate is a thread with its own memory heap, thus each isolate has its own isolated replica of the request channel. Database connections and other services created in a \nRequestSink\n are also replicated in each isolate.\n\n\nWhen the application receives an HTTP request, only one of its isolates receives and responds to the request. This structure spreads computation across multiple CPUs/cores and makes patterns like connection pooling implicit, i.e. each isolate has its own database connection.\n\n\nAqueduct Project Structure and Organization\n\n\nAn Aqueduct project is a directory that contains, at minimum, the following file structure:\n\n\npubspec.yaml\nlib/\n  application_name.dart\n\n\n\n\n\nThe name of any Dart application is defined by the \nname\n key in \npubspec.yaml\n. In order for \naqueduct serve\n to run your application, there must be a \n.dart\n file in \nlib/\n with that same name. This is your application library file and it must declare a \nRequestSink\n subclass or import another file that does. This is the bare minimum requirement to run an Aqueduct application. (See \nDeploying\n for more details on running applications.)\n\n\nFor organizing applications of reasonable size, we recommend the following structure:\n\n\npubspec.yaml\nconfig.src.yaml\nconfig.yaml\nlib/\n  application_name.dart\n  application_name_sink.dart  \n  controller/\n    user_controller.dart\n  model/\n    user.dart\ntest/\n  user_controller_test.dart\n  harness/\n    app.dart\n\n\n\n\n\nThe required \npubspec.yaml\n and \nlib/application_name.dart\n files are present alongside a few others:\n\n\n\n\nconfig.yaml\n: A \nconfiguration file\n for the running application.\n\n\nconfig.src.yaml\n: A \ntemplate for config.yaml\n.\n\n\napplication_name_sink.dart\n: A file solely for the \nRequestSink\n of an application. This file should be \nexported\n from \napplication_name.dart\n.\n\n\ncontroller/\n: A directory for \nRequestController\n subclass files.\n\n\nmodel/\n: A directory for \nManagedObject\nT\n subclass files.\n\n\ntest/harness/app.dart\n: A \ntest harness\n) for automated testing.\n\n\n\n\nFeel free to create other subdirectories in \nlib/\n for organizing other types of files.\n\n\nAqueduct and dart:io\n\n\nAqueduct runs on top of \ndart:io\n and relies on its \nHttpServer\n implementation. When an Aqueduct application is started, one or more \nHttpServer\n instances are bound to the port specified by \naqueduct serve\n. For each HTTP request, an instance of \nRequest\n is created to wrap the \nHttpRequest\n from \ndart:io\n. The \nRequest\n is added to a \nRequestSink\n, sending it through the channel of \nRequestController\ns until it is responded to.\n\n\nIn rare circumstances, you may choose to remove a \nRequest\n from the request channel and manipulate the request with \ndart:io\n only. Once removed, it is your responsibility to respond to the request by setting properties on and closing the \nHttpRequest.response\n. To take a request out of the channel, simply return \nnull\n from a \nRequestController\n:\n\n\n@\noverride\n\n\nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n  \nrouter\n\n    \n.\nroute\n(\n/bypass_aqueduct\n)\n\n    \n.\nlisten\n((\nreq\n)\n \nasync\n \n{\n\n      \nreq\n.\nresponse\n.\nstatusCode\n \n=\n \n200\n;\n\n      \nreq\n.\nresponse\n.\nclose\n();\n\n\n      \nreturn\n \nnull\n;\n\n    \n});\n\n\n}\n\n\n\n\n\n\n(In Aqueduct 2.2.1 and below, a \nRequestController\n that removes a request from the channel in this way must be the last request controller in the channel.)\n\n\nThis technique is valuable when Aqueduct can't do something you want it to do or when using \nwebsockets\n.", 
            "title": "Application Structure"
        }, 
        {
            "location": "/http/structure/#aqueduct-application-architecture", 
            "text": "The most important object in Aqueduct is a  RequestController . A  RequestController  is the only thing that can respond to HTTP requests. The logic for an application is written in the methods of this type and its subclasses.  Request controllers are chained together to create a  request channel . A request channel is a series of request controllers that a request flows through to be verified, modified and responded to.   A request channel always starts at an instance of  RequestSink  (a subclass of  RequestController ). When an application receives an HTTP request, it adds it to the  RequestSink . A  RequestSink  has a  Router  (also a subclass of  RequestController ) that splits the channel based on the path of the request. For example, a request with the path  /users  will go down one part of the channel, while a  /things  request will go down another.  An application has exactly one  RequestSink  subclass; it is responsible for defining its application's request channel by overriding the method  setupRouter . For example, the diagram above looks like this in code:  class   MyRequestSink   extends   RequestSink   { \n   MyRequestSink ( ApplicationConfiguration   config )   :   super ( config ); \n\n   @ override \n   void   setupRouter ( Router   router )   { \n     router \n       . route ( /a ) \n       . generate (()   =   new   AController ()); \n\n     router \n       . route ( /b ) \n       . pipe ( new   Authorizer (...)) \n       . generate (()   =   new   BController ()); \n\n     router \n       . route ( /c ) \n       . pipe ( new   Authorizer (...)) \n       . generate (()   =   new   CController ());       \n   }  }   In the above code, all of the objects created are instances of a  RequestController  subclass. Each of these controllers can either respond to the request, or send it to the next controller in the channel. For example, an  Authorizer  will respond with a 401 Unauthorized response if a request's authorization isn't valid - but if it is valid, the request is passed to the next controller in the channel.  For more details on the object that handles requests, see  Request Controllers .", 
            "title": "Aqueduct Application Architecture"
        }, 
        {
            "location": "/http/structure/#isolates-multi-threaded-applications", 
            "text": "An Aqueduct application may run its request channel on multiple isolates. The number of isolates is configured when running  aqueduct serve .  aqueduct serve --isolates 3  An isolate is a thread with its own memory heap, thus each isolate has its own isolated replica of the request channel. Database connections and other services created in a  RequestSink  are also replicated in each isolate.  When the application receives an HTTP request, only one of its isolates receives and responds to the request. This structure spreads computation across multiple CPUs/cores and makes patterns like connection pooling implicit, i.e. each isolate has its own database connection.", 
            "title": "Isolates: Multi-threaded Applications"
        }, 
        {
            "location": "/http/structure/#aqueduct-project-structure-and-organization", 
            "text": "An Aqueduct project is a directory that contains, at minimum, the following file structure:  pubspec.yaml\nlib/\n  application_name.dart  The name of any Dart application is defined by the  name  key in  pubspec.yaml . In order for  aqueduct serve  to run your application, there must be a  .dart  file in  lib/  with that same name. This is your application library file and it must declare a  RequestSink  subclass or import another file that does. This is the bare minimum requirement to run an Aqueduct application. (See  Deploying  for more details on running applications.)  For organizing applications of reasonable size, we recommend the following structure:  pubspec.yaml\nconfig.src.yaml\nconfig.yaml\nlib/\n  application_name.dart\n  application_name_sink.dart  \n  controller/\n    user_controller.dart\n  model/\n    user.dart\ntest/\n  user_controller_test.dart\n  harness/\n    app.dart  The required  pubspec.yaml  and  lib/application_name.dart  files are present alongside a few others:   config.yaml : A  configuration file  for the running application.  config.src.yaml : A  template for config.yaml .  application_name_sink.dart : A file solely for the  RequestSink  of an application. This file should be  exported  from  application_name.dart .  controller/ : A directory for  RequestController  subclass files.  model/ : A directory for  ManagedObject T  subclass files.  test/harness/app.dart : A  test harness ) for automated testing.   Feel free to create other subdirectories in  lib/  for organizing other types of files.", 
            "title": "Aqueduct Project Structure and Organization"
        }, 
        {
            "location": "/http/structure/#aqueduct-and-dartio", 
            "text": "Aqueduct runs on top of  dart:io  and relies on its  HttpServer  implementation. When an Aqueduct application is started, one or more  HttpServer  instances are bound to the port specified by  aqueduct serve . For each HTTP request, an instance of  Request  is created to wrap the  HttpRequest  from  dart:io . The  Request  is added to a  RequestSink , sending it through the channel of  RequestController s until it is responded to.  In rare circumstances, you may choose to remove a  Request  from the request channel and manipulate the request with  dart:io  only. Once removed, it is your responsibility to respond to the request by setting properties on and closing the  HttpRequest.response . To take a request out of the channel, simply return  null  from a  RequestController :  @ override  void   setupRouter ( Router   router )   { \n   router \n     . route ( /bypass_aqueduct ) \n     . listen (( req )   async   { \n       req . response . statusCode   =   200 ; \n       req . response . close (); \n\n       return   null ; \n     });  }   (In Aqueduct 2.2.1 and below, a  RequestController  that removes a request from the channel in this way must be the last request controller in the channel.)  This technique is valuable when Aqueduct can't do something you want it to do or when using  websockets .", 
            "title": "Aqueduct and dart:io"
        }, 
        {
            "location": "/http/request_and_response/", 
            "text": "Request and Response Objects\n\n\nIn Aqueduct, HTTP requests and responses are instances of \nRequest\n and \nResponse\n, respectively. For every HTTP request an application receives, an instance of \nRequest\n is created. A \nResponse\n must be created for each \nRequest\n. Requests pass through a channel of \nRequestControllers\n to be validated, modified and finally responded to.\n\n\nThe Request Object\n\n\nAn instance of \nRequest\n represents an HTTP request. They are automatically created when the application receives a request and are delivered to your application's \nRequestSink\n. A \nRequest\n is a wrapper around the Dart standard library \nHttpRequest\n and its values - such as headers - can be accessed through its \ninnerRequest\n. (Just don't write to its \nresponse\n - Aqueduct does that.)\n\n\nA \nRequest\n has a \nbody\n property. This property decodes the HTTP request body into Dart objects based on the request's content type. The mechanism to decode the body is determined by \nHTTPCodecRepository\n, which is covered in more detail in a later section. By default, decoders exist for text, JSON and form data. The size of a request body is limited to 10MB by default and can be changed by setting the value of \nHTTPRequestBody.maxSize\n during application initialization.\n\n\nA \nRequest\n may go through many \nRequestController\ns before it is finally responded to. These \nRequestController\ns may validate or add more information to the request as it passes through. For example, an \nAuthorizer\n - a subclass of \nRequestController\n - will validate the Authorization header of a request. Once validated, it will add authorization info to the request - like the user for an OAuth 2.0 bearer token - and pass it to the next \nRequestController\n. The next controller in the channel has access to the authorization info without having to fetch the information again.\n\n\nThese additional values are added to a \nRequest\n's \nattachments\n property. A \nRequest\n also has two built-in attachments, \nauthorization\n and \npath\n. \nauthorization\n contains authorization information created by an \nAuthorizer\n and \npath\n has request path information created by a \nRouter\n.\n\n\nRequest\ns are responded to by returning an instance of \nResponse\n from a \nRequestController\n (see \nRequestControllers\n). Controllers that don't create the eventual response can still modify that response by invoking \naddResponseModifier\n.\n\n\nResponse Objects and HTTP Body Encoding\n\n\nAn instance of \nResponse\n has a status code, HTTP headers and an HTTP body. There are a number of convenience constructors for \nResponse\n for commonly used status codes. For example, \nResponse.ok\n creates a 200 OK status code response.\n\n\nvar\n \nresponse\n \n=\n \nnew\n \nResponse\n.\nok\n({\nkey\n:\n \nvalue\n});\n\n\n\n\n\n\nWhen a \nResponse\n is returned from a request controller, Aqueduct handles sending the HTTP response back to the client.\n\n\nAn HTTP response often contains a \nbody\n. For example, the body in response to \nGET /users/1\n might be JSON object that represents a user. To ensure the client understands that the body is a JSON object, it includes the header \nContent-Type: application/json; charset=utf-8\n.\n\n\nWhen creating a \nResponse\n that has a body, you provide a \nbody object\n and a \ncontentType\n. The body object is passed to one of \nResponse\n's constructors. For example, this map` is the body object of a response:\n\n\nvar\n \nmap\n \n=\n \n{\nkey\n:\n \nvalue\n};\n\n\n\n// ContentType.JSON is the default, setting it may be omitted.\n\n\n// ContentType.JSON == `application/json; charset=utf-8\n\n\nvar\n \nresponse\n \n=\n \nnew\n \nResponse\n.\nok\n(\nmap\n)\n\n  \n..\ncontentType\n \n=\n \nContentType\n.\nJSON\n;\n\n\n\n\n\n\nBody objects are encoded according to their content-type. In the above, \nmap\n is first encoded as a JSON string and then to a list of UTF8 bytes.\n\n\n\n\nA \nContentType\n is made up of three components: a primary type, a subtype and an optional character set.\n\n\n\n\nThe primary and subtype determine the first conversion step and the charset determines the next. Each step is performed by an instance of \nCodec\n. For example, the content type \napplication/json\n selects \nJsonCodec\n, while charset \nutf-8\n selects \nUtf8Codec\n. These two codecs are run in succession to convert the \nMap\n to a list of bytes.\n\n\nThe body object must be a valid input for the initial encoder step. In the above example, a \nMap\nString, dynamic\n can be encoded by a \nJsonCodec\n. But if the body object was something silly - like an \nIsolate\n - encoding would fail at runtime and the client would be sent a 500 Server Error response. A valid input for one \nCodec\n may not be valid for another; it is up to you to ensure that the body object is valid for the \ncontentType\n of the response.\n\n\nNot all content types require two conversion steps. For example, when serving an HTML file, the body object is already an HTML \nString\n. It will only be converted by a charset encoder:\n\n\nvar\n \nhtml\n \n=\n \nhtml\n/html\n;\n\n\nvar\n \nresponse\n \n=\n \nnew\n \nResponse\n.\nok\n(\nhtml\n)\n\n  \n..\ncontentType\n \n=\n \nContentType\n.\nHTML\n;\n\n\n\n\n\n\nAnd an image body object needs no conversion at all, since it is already a list of bytes:\n\n\nvar\n \nimageFile\n \n=\n \nnew\n \nFile\n(\nimage.jpg\n);\n\n\nvar\n \nimageBytes\n \n=\n \nawait\n \nimageFile\n.\nreadAsBytes\n();\n\n\nvar\n \nresponse\n \n=\n \nnew\n \nResponse\n.\nok\n(\nimageBytes\n)\n\n  \n..\ncontentType\n \n=\n \nnew\n \nContentType\n(\nimage\n,\n \njpeg\n);\n\n\n\n\n\n\nSee a later section for more details on content type to codec mappings. Also, see the documentation for \nHTTPCodecRepository\n for details on built-in codecs and adding codecs.\n\n\nStreaming Response Bodies\n\n\nA body object may also be a \nStream\nT\n. \nStream\nT\n body objects are most often used when serving files. This allows the contents of the file to be streamed from disk to the HTTP client without having to load the whole file into memory first. (See also \nHTTPFileController\n.)\n\n\nvar\n \nimageFile\n \n=\n \nnew\n \nFile\n(\nimage.jpg\n);\n\n\nvar\n \nimageByteStream\n \n=\n \nimageFile\n.\nopenRead\n();\n\n\nvar\n \nresponse\n \n=\n \nnew\n \nResponse\n.\nok\n(\nimageByteStream\n)\n\n  \n..\ncontentType\n \n=\n \nnew\n \nContentType\n(\nimage\n,\n \njpeg\n);\n\n\n\n\n\n\nWhen a body object is a \nStream\nT\n, the response will not be sent until the stream is closed. For finite streams - like those from opened filed - this happens as soon as the entire file is read. For streams that you construct yourself, you must close the stream some time after the response has been returned.\n\n\nCustom Objects\n\n\nAny object may be the body object of a \nResponse\n if it implements \nHTTPSerializable\n. An object conforming to this type must implement \nasMap()\n, which gets invoked on the body object prior to it being sent to the first encoding step. For example, the following object can be used as a body object and is automatically converted into a \nMap\n and then JSON encoded:\n\n\nclass\n \nPerson\n \nimplements\n \nHTTPSerializable\n \n{\n\n  \nString\n \nname\n;\n\n  \nString\n \nemail\n;\n\n\n  \nMap\nString\n,\n \ndynamic\n \nasMap\n()\n \n{\n\n    \nreturn\n \n{\n\n      \nname\n:\n \nname\n,\n\n      \nemail\n:\n \nemail\n\n    \n};\n\n  \n}\n\n\n  \n...\n\n\n}\n\n\n\nvar\n \nperson\n \n=\n \nnew\n \nPerson\n();\n\n\nvar\n \nresponse\n \n=\n \nnew\n \nResponse\n.\nok\n(\nperson\n);\n\n\n\n\n\n\nManagedObject\nT\n, part of the Aqueduct ORM, implements \nHTTPSerializable\n so results from \nQuery\nT\n may be body objects:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nPerson\n()..\nwhere\n.\nid\n \n=\n \nwhereEqualTo\n(\n1\n);\n\n\nvar\n \nperson\n \n=\n \nawait\n \nquery\n.\nfetchOne\n();\n\n\nvar\n \nresponse\n \n=\n \nnew\n \nResponse\n.\nok\n(\nperson\n);\n\n\n\n// or List\nSerializable\n\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nPerson\n();\n\n\nvar\n \npeople\n \n=\n \nawait\n \nquery\n.\nfetch\n();\n\n\nvar\n \nresponse\n \n=\n \nnew\n \nResponse\n.\nok\n(\npeople\n);\n\n\n\n\n\n\nNote that a \nMap\n itself is not \nHTTPSerializable\n and can't have \nHTTPSerializable\n values. For example, the following wouldn't work:\n\n\nvar\n \nmap\n \n=\n \n{\n\n  \nperson\n:\n \nnew\n \nPerson\n()\n\n\n};\n\n\nvar\n \nresponse\n \n=\n \nnew\n \nResponse\n.\nok\n(\nmap\n);\n\n\n\n\n\n\nThe entire flow of a body object is shown in the following diagram. Each orange item is an allowed body object type and shows the steps it will go through when being encoded to the HTTP response body. For example, a \nHTTPSerializable\n goes through three steps, whereas a \nList\nint\n goes through zero steps and is added as-is to the HTTP response.\n\n\n\n\nCodecs and Content Types\n\n\nIn the above sections, we glossed over how a codec gets selected when preparing the response body. The common case of \nManagedObject\nT\n body objects that are sent as UTF8 encoded JSON 'just works' and is suitable for most applications. When serving assets for a web application or different data formats like XML, it becomes important to understand how Aqueduct's codec repository works.\n\n\nHTTPCodecRepository\n contains mappings from content types to \nCodec\ns. These codecs encode response bodies and decode request bodies. There are three built-in codecs for \napplication/json\n, \napplication/x-www-form-urlencoded\n and \ntext/*\n. When a response is being sent, the repository is searched for an entry that exactly matches the primary and subtype of the \nResponse.contentType\n. If an entry exists, the associated \nCodec\n starts the conversion. For example, if the content type is \napplication/json; charset=utf-8\n, the built-in \napplication/json\n codec encodes the body object. The character set is not evaluated at this stage.\n\n\nIf there isn't an exact match, but there is an entry for the primary type with the wildcard (\n*\n) subtype, that codec is used. For example, the built-in codec for \ntext/*\n will be selected for both \ntext/plain\n and \ntext/html\n. If there was something special that had to be done for \ntext/html\n, a more specific codec may be added for that type:\n\n\nclass\n \nMySink\n \nextends\n \nRequestSink\n \n{\n\n  \nMySink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n    \nHTTPCodecRepository\n.\ndefaultInstance\n.\nadd\n(\nnew\n \nContentType\n(\napplication\n,\n \nhtml\n),\n \nnew\n \nHTMLCodec\n());\n\n  \n}\n\n\n}\n\n\n\n\n\n\nContent type to codec mappings are added in the constructor of an application's \nRequestSink\n. The codec must implement \nCodec\n from \ndart:convert\n. In the above example, when a response's content type is \ntext/html\n, the \nHTMLCodec\n will encode the body object. This codec takes precedence over \ntext/*\n because it is more specific.\n\n\nWhen selecting a codec for a response body, the \nContentType.charset\n doesn't impact which codec is selected. For example, the following two lines are equivalent:\n\n\nHTTPCodecRepository\n.\ndefaultInstance\n.\nadd\n(\nnew\n \nContentType\n(\napplication\n,\n \nhtml\n),\n \nnew\n \nHTMLCodec\n());\n\n\nHTTPCodecRepository\n.\ndefaultInstance\n.\nadd\n(\nnew\n \nContentType\n(\napplication\n,\n \nhtml\n,\n \ncharset:\n \nutf-8\n),\n \nnew\n \nHTMLCodec\n());\n\n\n\n\n\n\nIf a response's content-type has a charset, then a charset encoder like \nUTF8\n will be applied as a last encoding step. For example, a response with content-type \napplication/json; charset=utf-8\n will encode the body object as a JSON string, which is then encoded as a list of UTF8 bytes. It is required that a response body's eventually encoded type is a list of bytes, so it follows that a codec that produces a string must have a charset.\n\n\nIf there is no codec in the repository for the content type of a \nResponse\n, the body object must be a \nList\nint\n or \nStream\nList\nint\n. If you find yourself converting data prior to setting it as a body object, it may make sense to add your own codec to \nHTTPCodecRepository\n.\n\n\nA request's body, on the other hand, always starts as a list of bytes. To decode a JSON request body, it first must be decoded from the list of UTF8 bytes into a string. It is possible that a client could omit the charset in its content-type header. Codecs added to \nHTTPCodecRepository\n may specify a default charset to interpret a charset-less content-type. When a codec is added to the repository, if content-type's charset is non-null, that is the default. For example, the JSON codec is added like this:\n\n\nHTTPCodecRepository\n.\ndefaultInstance\n.\nadd\n(\n\n  \nnew\n \nContentType\n(\napplication\n,\n \njson\n,\n \ncharset:\n \nutf-8\n),\n\n  \nconst\n \nJsonCodec\n(),\n\n  \nallowCompression:\n \ntrue\n);\n\n\n\n\n\n\nIf the charset is null, no charset decoding occurs on a request body if one doesn't exist. Content-types that are decoded from a \nString\n should not use a default charset because the repository would always attempt to decode the body as a string first.\n\n\nCompression with gzip\n\n\nBody objects may be compressed with \ngzip\n if the HTTP client allows it \nand\n the \nHTTPCodecRepository\n has been configured to compress the content type of the response. The three built-in codecs - \napplication/json\n, \napplication/x-www-form-urlencoded\n and \ntext/*\n - are all configured to allow compression. Compression occurs as the last step of conversion and only if the HTTP client sends the \nAccept-Encoding: gzip\n header.\n\n\nContent types that are not in the codec repository will not trigger compression, even if the HTTP client allows compression with the \nAccept-Encoding\n header. This is to prevent binary contents like images from being 'compressed', since they are likely already compressed by a content-specific algorithm. In order for Aqueduct to compress a content type other than the built-in types, you may add a codec to the repository with the \nallowCompression\n flag. (The default value is \ntrue\n.)\n\n\nMySink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n  \nHTTPCodecRepository\n.\nadd\n(\n\n    \nnew\n \nContentType\n(\napplication\n,\n \nx-special\n),\n\n    \nnew\n \nMyCodec\n(),\n\n    \nallowCompression:\n \ntrue\n);\n\n\n}\n\n\n\n\n\n\nYou may also set whether or not a content type uses compression without having to specify a codec if no conversion step needs to occur:\n\n\nHTTPCodecRepository\n.\nsetAllowsCompression\n(\nnew\n \nContentType\n(\napplication\n,\n \nx-special\n),\n \ntrue\n);", 
            "title": "Request and Response Objects"
        }, 
        {
            "location": "/http/request_and_response/#request-and-response-objects", 
            "text": "In Aqueduct, HTTP requests and responses are instances of  Request  and  Response , respectively. For every HTTP request an application receives, an instance of  Request  is created. A  Response  must be created for each  Request . Requests pass through a channel of  RequestControllers  to be validated, modified and finally responded to.", 
            "title": "Request and Response Objects"
        }, 
        {
            "location": "/http/request_and_response/#the-request-object", 
            "text": "An instance of  Request  represents an HTTP request. They are automatically created when the application receives a request and are delivered to your application's  RequestSink . A  Request  is a wrapper around the Dart standard library  HttpRequest  and its values - such as headers - can be accessed through its  innerRequest . (Just don't write to its  response  - Aqueduct does that.)  A  Request  has a  body  property. This property decodes the HTTP request body into Dart objects based on the request's content type. The mechanism to decode the body is determined by  HTTPCodecRepository , which is covered in more detail in a later section. By default, decoders exist for text, JSON and form data. The size of a request body is limited to 10MB by default and can be changed by setting the value of  HTTPRequestBody.maxSize  during application initialization.  A  Request  may go through many  RequestController s before it is finally responded to. These  RequestController s may validate or add more information to the request as it passes through. For example, an  Authorizer  - a subclass of  RequestController  - will validate the Authorization header of a request. Once validated, it will add authorization info to the request - like the user for an OAuth 2.0 bearer token - and pass it to the next  RequestController . The next controller in the channel has access to the authorization info without having to fetch the information again.  These additional values are added to a  Request 's  attachments  property. A  Request  also has two built-in attachments,  authorization  and  path .  authorization  contains authorization information created by an  Authorizer  and  path  has request path information created by a  Router .  Request s are responded to by returning an instance of  Response  from a  RequestController  (see  RequestControllers ). Controllers that don't create the eventual response can still modify that response by invoking  addResponseModifier .", 
            "title": "The Request Object"
        }, 
        {
            "location": "/http/request_and_response/#response-objects-and-http-body-encoding", 
            "text": "An instance of  Response  has a status code, HTTP headers and an HTTP body. There are a number of convenience constructors for  Response  for commonly used status codes. For example,  Response.ok  creates a 200 OK status code response.  var   response   =   new   Response . ok ({ key :   value });   When a  Response  is returned from a request controller, Aqueduct handles sending the HTTP response back to the client.  An HTTP response often contains a  body . For example, the body in response to  GET /users/1  might be JSON object that represents a user. To ensure the client understands that the body is a JSON object, it includes the header  Content-Type: application/json; charset=utf-8 .  When creating a  Response  that has a body, you provide a  body object  and a  contentType . The body object is passed to one of  Response 's constructors. For example, this map` is the body object of a response:  var   map   =   { key :   value };  // ContentType.JSON is the default, setting it may be omitted.  // ContentType.JSON == `application/json; charset=utf-8  var   response   =   new   Response . ok ( map ) \n   .. contentType   =   ContentType . JSON ;   Body objects are encoded according to their content-type. In the above,  map  is first encoded as a JSON string and then to a list of UTF8 bytes.   A  ContentType  is made up of three components: a primary type, a subtype and an optional character set.   The primary and subtype determine the first conversion step and the charset determines the next. Each step is performed by an instance of  Codec . For example, the content type  application/json  selects  JsonCodec , while charset  utf-8  selects  Utf8Codec . These two codecs are run in succession to convert the  Map  to a list of bytes.  The body object must be a valid input for the initial encoder step. In the above example, a  Map String, dynamic  can be encoded by a  JsonCodec . But if the body object was something silly - like an  Isolate  - encoding would fail at runtime and the client would be sent a 500 Server Error response. A valid input for one  Codec  may not be valid for another; it is up to you to ensure that the body object is valid for the  contentType  of the response.  Not all content types require two conversion steps. For example, when serving an HTML file, the body object is already an HTML  String . It will only be converted by a charset encoder:  var   html   =   html /html ;  var   response   =   new   Response . ok ( html ) \n   .. contentType   =   ContentType . HTML ;   And an image body object needs no conversion at all, since it is already a list of bytes:  var   imageFile   =   new   File ( image.jpg );  var   imageBytes   =   await   imageFile . readAsBytes ();  var   response   =   new   Response . ok ( imageBytes ) \n   .. contentType   =   new   ContentType ( image ,   jpeg );   See a later section for more details on content type to codec mappings. Also, see the documentation for  HTTPCodecRepository  for details on built-in codecs and adding codecs.", 
            "title": "Response Objects and HTTP Body Encoding"
        }, 
        {
            "location": "/http/request_and_response/#streaming-response-bodies", 
            "text": "A body object may also be a  Stream T .  Stream T  body objects are most often used when serving files. This allows the contents of the file to be streamed from disk to the HTTP client without having to load the whole file into memory first. (See also  HTTPFileController .)  var   imageFile   =   new   File ( image.jpg );  var   imageByteStream   =   imageFile . openRead ();  var   response   =   new   Response . ok ( imageByteStream ) \n   .. contentType   =   new   ContentType ( image ,   jpeg );   When a body object is a  Stream T , the response will not be sent until the stream is closed. For finite streams - like those from opened filed - this happens as soon as the entire file is read. For streams that you construct yourself, you must close the stream some time after the response has been returned.", 
            "title": "Streaming Response Bodies"
        }, 
        {
            "location": "/http/request_and_response/#custom-objects", 
            "text": "Any object may be the body object of a  Response  if it implements  HTTPSerializable . An object conforming to this type must implement  asMap() , which gets invoked on the body object prior to it being sent to the first encoding step. For example, the following object can be used as a body object and is automatically converted into a  Map  and then JSON encoded:  class   Person   implements   HTTPSerializable   { \n   String   name ; \n   String   email ; \n\n   Map String ,   dynamic   asMap ()   { \n     return   { \n       name :   name , \n       email :   email \n     }; \n   } \n\n   ...  }  var   person   =   new   Person ();  var   response   =   new   Response . ok ( person );   ManagedObject T , part of the Aqueduct ORM, implements  HTTPSerializable  so results from  Query T  may be body objects:  var   query   =   new   Query Person ().. where . id   =   whereEqualTo ( 1 );  var   person   =   await   query . fetchOne ();  var   response   =   new   Response . ok ( person );  // or List Serializable  var   query   =   new   Query Person ();  var   people   =   await   query . fetch ();  var   response   =   new   Response . ok ( people );   Note that a  Map  itself is not  HTTPSerializable  and can't have  HTTPSerializable  values. For example, the following wouldn't work:  var   map   =   { \n   person :   new   Person ()  };  var   response   =   new   Response . ok ( map );   The entire flow of a body object is shown in the following diagram. Each orange item is an allowed body object type and shows the steps it will go through when being encoded to the HTTP response body. For example, a  HTTPSerializable  goes through three steps, whereas a  List int  goes through zero steps and is added as-is to the HTTP response.", 
            "title": "Custom Objects"
        }, 
        {
            "location": "/http/request_and_response/#codecs-and-content-types", 
            "text": "In the above sections, we glossed over how a codec gets selected when preparing the response body. The common case of  ManagedObject T  body objects that are sent as UTF8 encoded JSON 'just works' and is suitable for most applications. When serving assets for a web application or different data formats like XML, it becomes important to understand how Aqueduct's codec repository works.  HTTPCodecRepository  contains mappings from content types to  Codec s. These codecs encode response bodies and decode request bodies. There are three built-in codecs for  application/json ,  application/x-www-form-urlencoded  and  text/* . When a response is being sent, the repository is searched for an entry that exactly matches the primary and subtype of the  Response.contentType . If an entry exists, the associated  Codec  starts the conversion. For example, if the content type is  application/json; charset=utf-8 , the built-in  application/json  codec encodes the body object. The character set is not evaluated at this stage.  If there isn't an exact match, but there is an entry for the primary type with the wildcard ( * ) subtype, that codec is used. For example, the built-in codec for  text/*  will be selected for both  text/plain  and  text/html . If there was something special that had to be done for  text/html , a more specific codec may be added for that type:  class   MySink   extends   RequestSink   { \n   MySink ( ApplicationConfiguration   config )   :   super ( config )   { \n     HTTPCodecRepository . defaultInstance . add ( new   ContentType ( application ,   html ),   new   HTMLCodec ()); \n   }  }   Content type to codec mappings are added in the constructor of an application's  RequestSink . The codec must implement  Codec  from  dart:convert . In the above example, when a response's content type is  text/html , the  HTMLCodec  will encode the body object. This codec takes precedence over  text/*  because it is more specific.  When selecting a codec for a response body, the  ContentType.charset  doesn't impact which codec is selected. For example, the following two lines are equivalent:  HTTPCodecRepository . defaultInstance . add ( new   ContentType ( application ,   html ),   new   HTMLCodec ());  HTTPCodecRepository . defaultInstance . add ( new   ContentType ( application ,   html ,   charset:   utf-8 ),   new   HTMLCodec ());   If a response's content-type has a charset, then a charset encoder like  UTF8  will be applied as a last encoding step. For example, a response with content-type  application/json; charset=utf-8  will encode the body object as a JSON string, which is then encoded as a list of UTF8 bytes. It is required that a response body's eventually encoded type is a list of bytes, so it follows that a codec that produces a string must have a charset.  If there is no codec in the repository for the content type of a  Response , the body object must be a  List int  or  Stream List int . If you find yourself converting data prior to setting it as a body object, it may make sense to add your own codec to  HTTPCodecRepository .  A request's body, on the other hand, always starts as a list of bytes. To decode a JSON request body, it first must be decoded from the list of UTF8 bytes into a string. It is possible that a client could omit the charset in its content-type header. Codecs added to  HTTPCodecRepository  may specify a default charset to interpret a charset-less content-type. When a codec is added to the repository, if content-type's charset is non-null, that is the default. For example, the JSON codec is added like this:  HTTPCodecRepository . defaultInstance . add ( \n   new   ContentType ( application ,   json ,   charset:   utf-8 ), \n   const   JsonCodec (), \n   allowCompression:   true );   If the charset is null, no charset decoding occurs on a request body if one doesn't exist. Content-types that are decoded from a  String  should not use a default charset because the repository would always attempt to decode the body as a string first.", 
            "title": "Codecs and Content Types"
        }, 
        {
            "location": "/http/request_and_response/#compression-with-gzip", 
            "text": "Body objects may be compressed with  gzip  if the HTTP client allows it  and  the  HTTPCodecRepository  has been configured to compress the content type of the response. The three built-in codecs -  application/json ,  application/x-www-form-urlencoded  and  text/*  - are all configured to allow compression. Compression occurs as the last step of conversion and only if the HTTP client sends the  Accept-Encoding: gzip  header.  Content types that are not in the codec repository will not trigger compression, even if the HTTP client allows compression with the  Accept-Encoding  header. This is to prevent binary contents like images from being 'compressed', since they are likely already compressed by a content-specific algorithm. In order for Aqueduct to compress a content type other than the built-in types, you may add a codec to the repository with the  allowCompression  flag. (The default value is  true .)  MySink ( ApplicationConfiguration   config )   :   super ( config )   { \n   HTTPCodecRepository . add ( \n     new   ContentType ( application ,   x-special ), \n     new   MyCodec (), \n     allowCompression:   true );  }   You may also set whether or not a content type uses compression without having to specify a codec if no conversion step needs to occur:  HTTPCodecRepository . setAllowsCompression ( new   ContentType ( application ,   x-special ),   true );", 
            "title": "Compression with gzip"
        }, 
        {
            "location": "/http/request_controller/", 
            "text": "Handling Requests: Fundamentals\n\n\nThis guide provides a deeper understanding of how a request object moves through an Aqueduct application to get responded to. For more use-case based guidance on handling requests, see \nHTTPController\n.\n\n\nAn Aqueduct application is a \nchannel of RequestController\n instances that HTTP requests go through to get responded to.\n\n\n\n\nThe above diagram shows a request entering a channel at a \nRequestSink\n and then being passed to a \nRouter\n. The \nRouter\n splits the channel. If a request's path matches a registered route, the request is passed down the split channel. If it doesn't, the \nRouter\n stops the request from continuing down the channel and responds to it with a 404 Not Found response. Likewise, \nAuthorizer\n might reject a request if it doesn't have valid authorization credentials or let it pass to the next controller in the channel. At the end of the channel, some controller must respond to the request.\n\n\nChannels always begin with a \nRequestSink\n and a \nRouter\n. This channel is built upon by invoking \nroute\n on the \nRouter\n, which splits the channel into subchannels. These subchannels are added to with the methods \npipe\n, \ngenerate\n, and \nlisten\n. Each of these \nRequestController\n methods attaches another \nRequestController\n to form a channel. Here's an example:\n\n\n@\noverride\n\n\nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n  \nrouter\n\n    \n.\nroute\n(\n/path\n)\n\n    \n.\nlisten\n((\nRequest\n \nrequest\n)\n \nasync\n \n{\n\n      \nlogger\n.\ninfo\n(\n$\nrequest\n);\n\n      \nreturn\n \nrequest\n;\n\n    \n})\n\n    \n.\npipe\n(\nnew\n \nFilteringController\n())\n\n    \n.\ngenerate\n(()\n \n=\n \nnew\n \nCRUDController\n());\n\n\n}\n\n\n\n\n\n\nFirst, notice that these methods can be chained together in this way because they always return the controller being added to the channel. For example, \nrouter.route\n creates a new \"route controller\" and returns it. The route controller then has \nlisten\n invoked on it, which creates a new \nRequestController\n from a closure and returns it, for which \npipe\n is invoked... you get the drift.\n\n\nThe \nlisten\n method is the easiest to understand: it takes an async closure that takes a \nRequest\n and may either return a \nResponse\n or that same \nRequest\n. If it returns the \nRequest\n, the request is passed to the next controller in the channel. If it returns a \nResponse\n, a response is sent to the HTTP client and no more controllers get the request.\n\n\nConcrete implementations of \nRequestController\n - like the mythical \nFilteringController\n and \nCRUDController\n above - override a method with the same signature as the \nlisten\n closure. This method is named \nprocessRequest\n and it looks like this:\n\n\nclass\n \nFilteringController\n \n{\n\n  \n@\noverride\n\n  \nFuture\nRequestOrResponse\n \nprocessRequest\n(\nRequest\n \nrequest\n)\n \nasync\n \n{\n\n    \nif\n \n(\nisThereSomethingWrongWith\n(\nrequest\n))\n \n{\n\n      \nreturn\n \nnew\n \nResponse\n.\nbadRequest\n();\n\n    \n}\n\n\n    \nreturn\n \nrequest\n;\n\n  \n}\n\n\n}\n\n\n\n\n\n\n(In fact, the \nlisten\n closure is simply wrapped by an instance of \nRequestController\n. The default behavior of \nprocessRequest\n is to invoke its closure.)\n\n\nThe difference between \npipe\n and \ngenerate\n is important. A \nRequestController\n like \nFilteringController\n doesn't have any properties that change when processing a request. But let's pretend \nCRUDController\n is defined like the following, where it reads the body into a property and then accesses that property later:\n\n\nclass\n \nCRUDController\n \n{\n\n  \nMap\nString\n,\n \ndynamic\n \nbody\n;\n\n\n  \n@\noverride\n\n  \nFuture\nRequestOrResponse\n \nprocessRequest\n(\nRequest\n \nrequest\n)\n \nasync\n \n{\n\n    \nbody\n \n=\n \nawait\n \nrequest\n.\nbody\n.\nasMap\n();\n\n\n    \nif\n \n(\nrequest\n.\ninnerRequest\n.\nmethod\n \n==\n \nPOST\n)\n \n{\n\n      \nreturn\n \nhandlePost\n();\n\n    \n}\n \nelse\n \nif\n \n(\nrequest\n.\ninnerMethod\n \n==\n \nPUT\n)\n \n{\n\n      \nreturn\n \nhandlePut\n();\n\n    \n}\n \n...\n\n  \n}\n\n\n  \nFuture\nRequestOrResponse\n \nhandlePost\n()\n \nasync\n \n{\n\n    \n...\n \ndo\n \nsomething\n \nwith\n \nbody\n \n...\n\n  \n}\n\n\n}\n\n\n\n\n\n\nIn the above, the \nbody\n property will be updated when \nCRUDController\n receives a request. It is possible that \nCRUDController\n is working on creating a response when it gets a new request, changing its \nbody\n property. This would change the \nbody\n for all requests that \nCRUDController\n is currently processing!\n\n\nTherefore, when a controller has state that changes for every request, a new instance should be created for each request. The \ngenerate\n method takes a closure that creates a new instance of a controller. The \npipe\n method on the other hand reuses the same controller for every request because the request passes right through it without changing any of its state.\n\n\nThe most common controller in an Aqueduct is an \nHTTPController\n. This controller handles all requests for an HTTP resource. For example, a controller of this type might handle \nPOST /users\n, \nPUT /users/1\n, \nGET /users\n, \nGET /users/1\n and \nDELETE /users/1\n. It has conveniences for organizing code such that each of these operations is bound to its own instance method. These conveniences require that the controller store parsed values from the request in it properties. Therefore, all \nHTTPController\ns must be added to a channel with \ngenerate\n.\n\n\nHTTPController\n - and any other controller that requires a new instance for each request - is marked with \n@cannotBeReused\n metadata. If you try and \npipe\n to a controller with this metadata, you'll get an error at startup with a helpful error message.\n\n\nException Handling\n\n\nRequestController\ns wrap \nprocessRequest\n in a try-catch block. If an exception is thrown during the processing of a request, it is caught and the controller will send a response on your behalf. The request is then removed from the channel and no more controllers will receive it.\n\n\nThere are two types of exceptions that a \nRequestController\n will interpret to return a meaningful status code: \nHTTPResponseException\n and \nQueryException\n. Any other uncaught exceptions will result in a 500 status code error.\n\n\nQueryException\ns are generated by the Aqueduct ORM. A request controller interprets these types of exceptions to return a suitable status code. The following reasons for the exception generate the following status codes:\n\n\n\n\n\n\n\n\nReason\n\n\nStatus Code\n\n\n\n\n\n\n\n\n\n\nA programmer error (bad query syntax)\n\n\n500\n\n\n\n\n\n\nUnique constraint violated\n\n\n409\n\n\n\n\n\n\nInvalid input\n\n\n400\n\n\n\n\n\n\nDatabase can't be reached\n\n\n503\n\n\n\n\n\n\n\n\nAn \nHTTPResponseException\n can be thrown at anytime to escape early from processing and return a response. Exceptions of these type allow you to specify the status code and a message. The message is encoded in a JSON object for the key \"error\". Some classes in Aqueduct will throw an exception of this kind if some precondition isn't met.\n\n\nIf you have code that can throw for legitimate reasons, you may catch those exceptions to return a response with an appropriate status code:\n\n\nclass\n \nController\n \nextends\n \nRequestController\n \n{\n\n  \nFuture\nRequestOrResponse\n \nprocessRequest\n(\nRequest\n \nrequest\n)\n \nasync\n \n{\n\n    \ntry\n \n{\n\n      \nawait\n \nsomeFailableOperation\n();\n\n    \n}\n \non\n \nOperationException\n \ncatch\n \n(\ne\n)\n \n{\n\n      \nreturn\n \nnew\n \nResponse\n(\n503\n,\n \nnull\n,\n \n{\nerror\n:\n \ne\n.\nerrorMessage\n});\n\n    \n}\n\n\n    \n...\n\n  \n}\n\n\n}\n\n\n\n\n\n\nYou may also catch \nQuery\n or \nHTTPResponseException\ns to reinterpret them, but the default behavior is pretty reasonable.\n\n\nOther than \nHTTPResponseException\ns, exceptions are written to the \nLogger\n along with some details of the request that generated the exception. \nHTTPResponseException\ns are not logged, as they are used for control flow and are considered \"normal\" operation.\n\n\nSubclassing RequestController\n\n\nUsing existing subclasses of \nRequestController\n like \nRouter\n, \nAuthorizer\n and \nHTTPController\n cover the majority of Aqueduct use cases. There are times where creating your own \nRequestController\n subclass may make sense.\n\n\nTo pass a request on to the next controller in the channel, a controller must return the same instance of \nRequest\n it receives. It may, however, attach additional information by adding key-value pairs to the request's \nattachments\n.\n\n\nFor example, an \nAuthorizer\n's pseudo code looks like this:\n\n\nFuture\nRequestOrResponse\n \nprocessRequest\n(\nRequest\n \nrequest\n)\n \nasync\n \n{\n\n    \nif\n \n(\n!\nisAuthorized\n(\nrequest\n))\n \n{\n\n      \nreturn\n \nnew\n \nResponse\n.\nunauthorized\n();\n\n    \n}\n\n\n    \nrequest\n.\nattachments\n[\nauthInfo\n]\n \n=\n \nauthInfoFromRequest\n(\nrequest\n);\n\n    \nreturn\n \nrequest\n;\n\n\n}\n\n\n\n\n\n\nThe next controller in the channel can look up the value of \nauthInfo\n:\n\n\nFuture\nRequestOrResponse\n \nprocessRequest\n(\nRequest\n \nrequest\n)\n \nasync\n \n{\n\n    \nvar\n \nauthInfo\n \n=\n \nrequest\n.\nattachments\n[\nauthInfo\n];\n\n\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\nYou are user: \n${\nauthInfo\n.\nusername\n}\n);\n\n\n}\n\n\n\n\n\n\nMiddleware Modifying a Response\n\n\nA \nRequestController\n that doesn't respond to a request can still modify the eventual response. This is valuable for \"middleware\" \nRequestController\ns that have some information to return back to the client, but aren't responsible for generating the response. For example, the following shows a request channel where \nUserController\n will create the response, but middleware will add a header to the response:\n\n\nrouter\n\n  \n.\nroute\n(\n/path\n)\n\n  \n.\nlisten\n((\nreq\n)\n \nasync\n \n{\n\n    \nreturn\n \nreq\n\n      \n..\naddResponseModifier\n((\nresponse\n)\n \n{\n\n        \nresponse\n.\nheaders\n[\nx-api-version\n]\n \n=\n \n2.1\n;\n\n      \n});\n\n  \n})\n\n  \n.\ngenerate\n(()\n \n=\n \nnew\n \nUserController\n());\n\n\n\n\n\n\nModifiers are run in the order they are added to a request and are run before any body data is encoded or any values are written to the network socket.\n\n\nCORS Headers and Preflight Requests\n\n\nRequestController\ns have built-in behavior for handling CORS requests. They will automatically respond to \nOPTIONS\n preflight requests and attach CORS headers to any other response. See \nthe chapter on CORS\n for more details.", 
            "title": "Handling Requests: Fundamentals"
        }, 
        {
            "location": "/http/request_controller/#handling-requests-fundamentals", 
            "text": "This guide provides a deeper understanding of how a request object moves through an Aqueduct application to get responded to. For more use-case based guidance on handling requests, see  HTTPController .  An Aqueduct application is a  channel of RequestController  instances that HTTP requests go through to get responded to.   The above diagram shows a request entering a channel at a  RequestSink  and then being passed to a  Router . The  Router  splits the channel. If a request's path matches a registered route, the request is passed down the split channel. If it doesn't, the  Router  stops the request from continuing down the channel and responds to it with a 404 Not Found response. Likewise,  Authorizer  might reject a request if it doesn't have valid authorization credentials or let it pass to the next controller in the channel. At the end of the channel, some controller must respond to the request.  Channels always begin with a  RequestSink  and a  Router . This channel is built upon by invoking  route  on the  Router , which splits the channel into subchannels. These subchannels are added to with the methods  pipe ,  generate , and  listen . Each of these  RequestController  methods attaches another  RequestController  to form a channel. Here's an example:  @ override  void   setupRouter ( Router   router )   { \n   router \n     . route ( /path ) \n     . listen (( Request   request )   async   { \n       logger . info ( $ request ); \n       return   request ; \n     }) \n     . pipe ( new   FilteringController ()) \n     . generate (()   =   new   CRUDController ());  }   First, notice that these methods can be chained together in this way because they always return the controller being added to the channel. For example,  router.route  creates a new \"route controller\" and returns it. The route controller then has  listen  invoked on it, which creates a new  RequestController  from a closure and returns it, for which  pipe  is invoked... you get the drift.  The  listen  method is the easiest to understand: it takes an async closure that takes a  Request  and may either return a  Response  or that same  Request . If it returns the  Request , the request is passed to the next controller in the channel. If it returns a  Response , a response is sent to the HTTP client and no more controllers get the request.  Concrete implementations of  RequestController  - like the mythical  FilteringController  and  CRUDController  above - override a method with the same signature as the  listen  closure. This method is named  processRequest  and it looks like this:  class   FilteringController   { \n   @ override \n   Future RequestOrResponse   processRequest ( Request   request )   async   { \n     if   ( isThereSomethingWrongWith ( request ))   { \n       return   new   Response . badRequest (); \n     } \n\n     return   request ; \n   }  }   (In fact, the  listen  closure is simply wrapped by an instance of  RequestController . The default behavior of  processRequest  is to invoke its closure.)  The difference between  pipe  and  generate  is important. A  RequestController  like  FilteringController  doesn't have any properties that change when processing a request. But let's pretend  CRUDController  is defined like the following, where it reads the body into a property and then accesses that property later:  class   CRUDController   { \n   Map String ,   dynamic   body ; \n\n   @ override \n   Future RequestOrResponse   processRequest ( Request   request )   async   { \n     body   =   await   request . body . asMap (); \n\n     if   ( request . innerRequest . method   ==   POST )   { \n       return   handlePost (); \n     }   else   if   ( request . innerMethod   ==   PUT )   { \n       return   handlePut (); \n     }   ... \n   } \n\n   Future RequestOrResponse   handlePost ()   async   { \n     ...   do   something   with   body   ... \n   }  }   In the above, the  body  property will be updated when  CRUDController  receives a request. It is possible that  CRUDController  is working on creating a response when it gets a new request, changing its  body  property. This would change the  body  for all requests that  CRUDController  is currently processing!  Therefore, when a controller has state that changes for every request, a new instance should be created for each request. The  generate  method takes a closure that creates a new instance of a controller. The  pipe  method on the other hand reuses the same controller for every request because the request passes right through it without changing any of its state.  The most common controller in an Aqueduct is an  HTTPController . This controller handles all requests for an HTTP resource. For example, a controller of this type might handle  POST /users ,  PUT /users/1 ,  GET /users ,  GET /users/1  and  DELETE /users/1 . It has conveniences for organizing code such that each of these operations is bound to its own instance method. These conveniences require that the controller store parsed values from the request in it properties. Therefore, all  HTTPController s must be added to a channel with  generate .  HTTPController  - and any other controller that requires a new instance for each request - is marked with  @cannotBeReused  metadata. If you try and  pipe  to a controller with this metadata, you'll get an error at startup with a helpful error message.", 
            "title": "Handling Requests: Fundamentals"
        }, 
        {
            "location": "/http/request_controller/#exception-handling", 
            "text": "RequestController s wrap  processRequest  in a try-catch block. If an exception is thrown during the processing of a request, it is caught and the controller will send a response on your behalf. The request is then removed from the channel and no more controllers will receive it.  There are two types of exceptions that a  RequestController  will interpret to return a meaningful status code:  HTTPResponseException  and  QueryException . Any other uncaught exceptions will result in a 500 status code error.  QueryException s are generated by the Aqueduct ORM. A request controller interprets these types of exceptions to return a suitable status code. The following reasons for the exception generate the following status codes:     Reason  Status Code      A programmer error (bad query syntax)  500    Unique constraint violated  409    Invalid input  400    Database can't be reached  503     An  HTTPResponseException  can be thrown at anytime to escape early from processing and return a response. Exceptions of these type allow you to specify the status code and a message. The message is encoded in a JSON object for the key \"error\". Some classes in Aqueduct will throw an exception of this kind if some precondition isn't met.  If you have code that can throw for legitimate reasons, you may catch those exceptions to return a response with an appropriate status code:  class   Controller   extends   RequestController   { \n   Future RequestOrResponse   processRequest ( Request   request )   async   { \n     try   { \n       await   someFailableOperation (); \n     }   on   OperationException   catch   ( e )   { \n       return   new   Response ( 503 ,   null ,   { error :   e . errorMessage }); \n     } \n\n     ... \n   }  }   You may also catch  Query  or  HTTPResponseException s to reinterpret them, but the default behavior is pretty reasonable.  Other than  HTTPResponseException s, exceptions are written to the  Logger  along with some details of the request that generated the exception.  HTTPResponseException s are not logged, as they are used for control flow and are considered \"normal\" operation.", 
            "title": "Exception Handling"
        }, 
        {
            "location": "/http/request_controller/#subclassing-requestcontroller", 
            "text": "Using existing subclasses of  RequestController  like  Router ,  Authorizer  and  HTTPController  cover the majority of Aqueduct use cases. There are times where creating your own  RequestController  subclass may make sense.  To pass a request on to the next controller in the channel, a controller must return the same instance of  Request  it receives. It may, however, attach additional information by adding key-value pairs to the request's  attachments .  For example, an  Authorizer 's pseudo code looks like this:  Future RequestOrResponse   processRequest ( Request   request )   async   { \n     if   ( ! isAuthorized ( request ))   { \n       return   new   Response . unauthorized (); \n     } \n\n     request . attachments [ authInfo ]   =   authInfoFromRequest ( request ); \n     return   request ;  }   The next controller in the channel can look up the value of  authInfo :  Future RequestOrResponse   processRequest ( Request   request )   async   { \n     var   authInfo   =   request . attachments [ authInfo ]; \n\n\n     return   new   Response . ok ( You are user:  ${ authInfo . username } );  }", 
            "title": "Subclassing RequestController"
        }, 
        {
            "location": "/http/request_controller/#middleware-modifying-a-response", 
            "text": "A  RequestController  that doesn't respond to a request can still modify the eventual response. This is valuable for \"middleware\"  RequestController s that have some information to return back to the client, but aren't responsible for generating the response. For example, the following shows a request channel where  UserController  will create the response, but middleware will add a header to the response:  router \n   . route ( /path ) \n   . listen (( req )   async   { \n     return   req \n       .. addResponseModifier (( response )   { \n         response . headers [ x-api-version ]   =   2.1 ; \n       }); \n   }) \n   . generate (()   =   new   UserController ());   Modifiers are run in the order they are added to a request and are run before any body data is encoded or any values are written to the network socket.", 
            "title": "Middleware Modifying a Response"
        }, 
        {
            "location": "/http/request_controller/#cors-headers-and-preflight-requests", 
            "text": "RequestController s have built-in behavior for handling CORS requests. They will automatically respond to  OPTIONS  preflight requests and attach CORS headers to any other response. See  the chapter on CORS  for more details.", 
            "title": "CORS Headers and Preflight Requests"
        }, 
        {
            "location": "/http/routing/", 
            "text": "Routing\n\n\nWhat is routing?\n\n\nEvery HTTP request has a URL. A URL identifies a \nresource\n on a computer. In the early days of the Internet, a resource was a file. For example, the URL \nhttp://www.geocities.com/my_page/image.jpg\n would return the file \nimage.jpg\n from the folder \nmy_page\n on the webserver located at \nwww.geocities.com\n. In a web application today, resources come from many other sources of data, like a database or a connected device. The job of a web application is to provide, create or modify a resource for a URL, wherever that resource might come from.\n\n\nA URL is made up of many parts, some of which are optional. The typical URL we see as humans looks like this: \nhttp://stablekernel.com/about\n. Most people recognize that typing this URL into a browser would take them to our company's \"About\" page. In other words, our \"About\" page is a resource and the URL identifies it.\n\n\nMore generally, the \"About\" page URL has the three required components of a URL: a \nscheme\n (\nhttp\n), a \nhost\n (\nstablekernel.com\n) and a \npath\n (\n/about\n). The host specifies the computer responsible for providing the resource, the path indicates the 'name' of the resource and the scheme lets both the requester and the host know how they should exchange information.\n\n\nAn Aqueduct application receives requests when the scheme is \nhttp\n (or \nhttps\n) and the host refers to a machine where the application is running. Therefore, once the application gets the request, it only cares about the remaining component: the path.\n\n\nIn Aqueduct, a \nRouter\n routes \nRequest\ns to a \nRequestController\n based on the request path. This process is known as \nrouting\n. When an application starts up, routes are registered in a subclass of \nRequestSink\n. Each registered route creates a new \nchannel\n of \nRequestController\ns that the request will be passed through.\n\n\nRoute Specifications Match HTTP Request Paths\n\n\nA route is registered by invoking \nRouter.route\n. This method takes a \nroute specification\n - a \nString\n with some syntax rules that will match the path of a request. This registration occurs when an application first starts by overriding \nRequestSink.setupRouter\n. For example:\n\n\nclass\n \nMyRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nMyRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n:\n \nsuper\n(\nconfig\n);\n\n\n  \n@\noverride\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n    \nrouter\n\n      \n.\nroute\n(\n/users\n)\n\n      \n.\nlisten\n((\nreq\n)\n \nasync\n \n=\n \nnew\n \nResponse\n.\nok\n(\nawait\n \ngetAllUsers\n());\n\n  \n}\n\n\n}\n\n\n\n\n\n\nThe argument to \nroute\n is the route specification string. This particular route matches the path \n/users\n. That is, a request for the URL \nhttp://myserver.com/users\n will be sent to the \nlisten\n closure. (Leading and trailing slashes are stripped out when routes are compiled, so including them has no effect, but it is good style to show a leading slash.)\n\n\nA path can have multiple segments (the characters between slashes). For example, the path \n/users/foo\n has two path segments: \nusers\n and \nfoo\n. A route specification matches each segment of a path against each of its segments. The path and the route must also have the same number of segments. Thus, the route specification \n/users/foo\n would match the path \n/users/foo\n, but it would not match the paths \n/users\n, \n/users/7\n or \n/users/foo/1\n.\n\n\nPath Variables\n\n\nA route specification may have \npath variables\n. A path variable is a route segment that always succeeds in matching a path segment. The value of the segment is stored so that down-channel controllers can use it. In a route specification, a path variable starts with a colon (\n:\n). The name of the variable follows this colon. For example, consider the following route that declares a path variable named \nuserID\n:\n\n\nrouter\n.\nroute\n(\n/users/:userID\n)\n\n\n\n\n\n\nThis route specification will match \n/users/1\n, \n/users/2\n, \n/users/foo\n, etc. The value of \nuserID\n is \n1\n, \n2\n and \nfoo\n, respectively. This route won't match \n/users\n or \n/users/1/2\n.\n\n\nOptional Path Segments\n\n\nRoutes may have optional path segments. This allows a group of routes that all refer to a resource collection to go to the same controller. For example, the requests \n/users\n and \n/users/1\n can both be covered by a single route specification.\n\n\nAn optional path segment has square brackets (\n[]\n) around it. The brackets can go before or after slashes. For example, the following two syntaxes register a route that accepts both \n/users\n and \n/users/:userID\n:\n\n\nroute\n(\n/users/[:userID]\n)\n\n\nroute\n(\n/users[/:userID]\n)\n\n\n\n\n\n\nConceptually, a request with a path of \n/users/1\n identifies a single user, where \n/users\n identifies all users. Optional segments are used to create better code structure by forwarding requests that deal with a specific type of resource to the same request controller. Therefore, the code to handle one user or multiple users is written in the same place.\n\n\nYou may have any number of optional segments in a route specification. Each optional segment must be nested. The following route would match \n/a\n, \n/a/b\n and \n/a/b/c\n. It would not match \n/a/c\n.\n\n\nroute\n(\n/a/[b/[c]]\n)\n\n\n\n\n\n\nIt's pretty rare to have more than one optional segment in a route. For example, consider the route:\n\n\nroute\n(\n/users/[:id/[:subresource/[:subresourceid]]]\n);\n\n\n\n\n\n\nThe code to handle one or more users is likely very different than the code to handle one of its subresources - different database tables will be queried and different authorization may be needed. Thus, a better approach is to split subresources into their own routes to keep controller logic modular:\n\n\nroute\n(\n/users/[:id]\n)...;\n\n\nroute\n(\n/users/:id/posts/[:id]\n)...;\n\n\nroute\n(\n/users/:id/notes/[:id]\n)...;\n\n\n\n\n\n\nRestricting Path Variable Values\n\n\nPath variables may restrict their possible values with a regular expression. The expression comes in parentheses following the path variable name. For example, the following route specification limits \nuserID\n to numbers only:\n\n\nroute\n(\n/users/:userID([0-9]+)\n)\n\n\n\n\n\n\nThis regular expression would only apply to the \n:userID\n segment. Note that capture groups and parentheses in general can't be included in a route's regular expression.\n\n\nEverything in between the parentheses is evaluated as the regular expression. Therefore, any additional spaces or characters will be a part of the regular expression. Since spaces aren't valid in a URL, you don't want spaces in a regular expression.\n\n\nMatching the Remaining Path\n\n\nFinally, a route specification may have a special 'match-all' token, the asterisk (\n*\n). This token allows for any remaining request path to be matched, regardless of its contents or length. For example, the route specification \n/users/*\n would match the following paths:\n\n\n/users\n/users/1\n/users/foo\n/users/foo/bar\n/users/foo/bar/something/else/and/this/goes/on/forever\n\n\n\n\n\nThis token is used when another medium is going to interpret the URL. For example, \nHTTPFileController\n - which reads a file from the filesystem - might have a route \n/file/*\n. It uses everything after \n/file\n to figure out the path on the filesystem.\n\n\nAccessing Path Variables\n\n\nInformation that a router parses from a request path - like path variables - are stored in a \nRequest\n's \npath\n. As a \nRequest\n passes through a \nRouter\n, its \npath\n is set to an instance of this type. Later controllers access the \npath\n of a \nRequest\n to help determine which resource the request is referring to. The \npath\n is an instance of \nHTTPRequestPath\n.\n\n\nThe \nvariables\n of an \nHTTPRequestPath\n are a \nMap\nString, String\n, where the key is the name of the variable in the route specification and the value is the matching path segment in an incoming request. For example, consider a route specification \n/users/:id\n. When a request with path \n/users/1\n is routed, this specification will match. So, a controller would access it like so:\n\n\nvar\n \nidentifier\n \n=\n \nrequest\n.\npath\n.\nvariables\n[\nid\n];\n\n\n// identifier = \n1\n\n\n\n\n\n\nThe values in \nvariables\n are always \nString\ns, since a request path is a \nString\n. \nRequestController\ns may parse path variables into types like \nint\n.\n\n\nHTTPController\n uses path variables to select a responder method to handle a request.\n\n\nFailed Matches Return 404\n\n\nA \nRouter\n will return a \nResponse.notFound\n - a response with status code 404 - if it receives a request that no route is registered for. The router will not send this request downstream to further listeners. This behavior may be overridden by providing a closure to \nRouter.unhandledRequestController\n to provide a 404 HTML page if the request allows it.", 
            "title": "Routing"
        }, 
        {
            "location": "/http/routing/#routing", 
            "text": "", 
            "title": "Routing"
        }, 
        {
            "location": "/http/routing/#what-is-routing", 
            "text": "Every HTTP request has a URL. A URL identifies a  resource  on a computer. In the early days of the Internet, a resource was a file. For example, the URL  http://www.geocities.com/my_page/image.jpg  would return the file  image.jpg  from the folder  my_page  on the webserver located at  www.geocities.com . In a web application today, resources come from many other sources of data, like a database or a connected device. The job of a web application is to provide, create or modify a resource for a URL, wherever that resource might come from.  A URL is made up of many parts, some of which are optional. The typical URL we see as humans looks like this:  http://stablekernel.com/about . Most people recognize that typing this URL into a browser would take them to our company's \"About\" page. In other words, our \"About\" page is a resource and the URL identifies it.  More generally, the \"About\" page URL has the three required components of a URL: a  scheme  ( http ), a  host  ( stablekernel.com ) and a  path  ( /about ). The host specifies the computer responsible for providing the resource, the path indicates the 'name' of the resource and the scheme lets both the requester and the host know how they should exchange information.  An Aqueduct application receives requests when the scheme is  http  (or  https ) and the host refers to a machine where the application is running. Therefore, once the application gets the request, it only cares about the remaining component: the path.  In Aqueduct, a  Router  routes  Request s to a  RequestController  based on the request path. This process is known as  routing . When an application starts up, routes are registered in a subclass of  RequestSink . Each registered route creates a new  channel  of  RequestController s that the request will be passed through.", 
            "title": "What is routing?"
        }, 
        {
            "location": "/http/routing/#route-specifications-match-http-request-paths", 
            "text": "A route is registered by invoking  Router.route . This method takes a  route specification  - a  String  with some syntax rules that will match the path of a request. This registration occurs when an application first starts by overriding  RequestSink.setupRouter . For example:  class   MyRequestSink   extends   RequestSink   { \n   MyRequestSink ( ApplicationConfiguration   config ) :   super ( config ); \n\n   @ override \n   void   setupRouter ( Router   router )   { \n     router \n       . route ( /users ) \n       . listen (( req )   async   =   new   Response . ok ( await   getAllUsers ()); \n   }  }   The argument to  route  is the route specification string. This particular route matches the path  /users . That is, a request for the URL  http://myserver.com/users  will be sent to the  listen  closure. (Leading and trailing slashes are stripped out when routes are compiled, so including them has no effect, but it is good style to show a leading slash.)  A path can have multiple segments (the characters between slashes). For example, the path  /users/foo  has two path segments:  users  and  foo . A route specification matches each segment of a path against each of its segments. The path and the route must also have the same number of segments. Thus, the route specification  /users/foo  would match the path  /users/foo , but it would not match the paths  /users ,  /users/7  or  /users/foo/1 .", 
            "title": "Route Specifications Match HTTP Request Paths"
        }, 
        {
            "location": "/http/routing/#path-variables", 
            "text": "A route specification may have  path variables . A path variable is a route segment that always succeeds in matching a path segment. The value of the segment is stored so that down-channel controllers can use it. In a route specification, a path variable starts with a colon ( : ). The name of the variable follows this colon. For example, consider the following route that declares a path variable named  userID :  router . route ( /users/:userID )   This route specification will match  /users/1 ,  /users/2 ,  /users/foo , etc. The value of  userID  is  1 ,  2  and  foo , respectively. This route won't match  /users  or  /users/1/2 .", 
            "title": "Path Variables"
        }, 
        {
            "location": "/http/routing/#optional-path-segments", 
            "text": "Routes may have optional path segments. This allows a group of routes that all refer to a resource collection to go to the same controller. For example, the requests  /users  and  /users/1  can both be covered by a single route specification.  An optional path segment has square brackets ( [] ) around it. The brackets can go before or after slashes. For example, the following two syntaxes register a route that accepts both  /users  and  /users/:userID :  route ( /users/[:userID] )  route ( /users[/:userID] )   Conceptually, a request with a path of  /users/1  identifies a single user, where  /users  identifies all users. Optional segments are used to create better code structure by forwarding requests that deal with a specific type of resource to the same request controller. Therefore, the code to handle one user or multiple users is written in the same place.  You may have any number of optional segments in a route specification. Each optional segment must be nested. The following route would match  /a ,  /a/b  and  /a/b/c . It would not match  /a/c .  route ( /a/[b/[c]] )   It's pretty rare to have more than one optional segment in a route. For example, consider the route:  route ( /users/[:id/[:subresource/[:subresourceid]]] );   The code to handle one or more users is likely very different than the code to handle one of its subresources - different database tables will be queried and different authorization may be needed. Thus, a better approach is to split subresources into their own routes to keep controller logic modular:  route ( /users/[:id] )...;  route ( /users/:id/posts/[:id] )...;  route ( /users/:id/notes/[:id] )...;", 
            "title": "Optional Path Segments"
        }, 
        {
            "location": "/http/routing/#restricting-path-variable-values", 
            "text": "Path variables may restrict their possible values with a regular expression. The expression comes in parentheses following the path variable name. For example, the following route specification limits  userID  to numbers only:  route ( /users/:userID([0-9]+) )   This regular expression would only apply to the  :userID  segment. Note that capture groups and parentheses in general can't be included in a route's regular expression.  Everything in between the parentheses is evaluated as the regular expression. Therefore, any additional spaces or characters will be a part of the regular expression. Since spaces aren't valid in a URL, you don't want spaces in a regular expression.", 
            "title": "Restricting Path Variable Values"
        }, 
        {
            "location": "/http/routing/#matching-the-remaining-path", 
            "text": "Finally, a route specification may have a special 'match-all' token, the asterisk ( * ). This token allows for any remaining request path to be matched, regardless of its contents or length. For example, the route specification  /users/*  would match the following paths:  /users\n/users/1\n/users/foo\n/users/foo/bar\n/users/foo/bar/something/else/and/this/goes/on/forever  This token is used when another medium is going to interpret the URL. For example,  HTTPFileController  - which reads a file from the filesystem - might have a route  /file/* . It uses everything after  /file  to figure out the path on the filesystem.", 
            "title": "Matching the Remaining Path"
        }, 
        {
            "location": "/http/routing/#accessing-path-variables", 
            "text": "Information that a router parses from a request path - like path variables - are stored in a  Request 's  path . As a  Request  passes through a  Router , its  path  is set to an instance of this type. Later controllers access the  path  of a  Request  to help determine which resource the request is referring to. The  path  is an instance of  HTTPRequestPath .  The  variables  of an  HTTPRequestPath  are a  Map String, String , where the key is the name of the variable in the route specification and the value is the matching path segment in an incoming request. For example, consider a route specification  /users/:id . When a request with path  /users/1  is routed, this specification will match. So, a controller would access it like so:  var   identifier   =   request . path . variables [ id ];  // identifier =  1   The values in  variables  are always  String s, since a request path is a  String .  RequestController s may parse path variables into types like  int .  HTTPController  uses path variables to select a responder method to handle a request.", 
            "title": "Accessing Path Variables"
        }, 
        {
            "location": "/http/routing/#failed-matches-return-404", 
            "text": "A  Router  will return a  Response.notFound  - a response with status code 404 - if it receives a request that no route is registered for. The router will not send this request downstream to further listeners. This behavior may be overridden by providing a closure to  Router.unhandledRequestController  to provide a 404 HTML page if the request allows it.", 
            "title": "Failed Matches Return 404"
        }, 
        {
            "location": "/http/request_sink/", 
            "text": "Application Initialization and the RequestSink\n\n\nThe only requirement of an Aqueduct application is that it has exactly one \nRequestSink\n subclass. This subclass handles the initialization of an application, including setting up routes, authorization and database connections.\n\n\nA \nRequestSink\n subclass is declared in its own file named \nlib/\napplication_name\n_request_sink.dart\n, which is exported from the application library file. (For example if the application is named \nwildfire\n, the application library file is \nlib/wildfire.dart\n.)\n\n\nwildfire/\n  lib/\n    wildfire.dart\n    wildfire_request_sink.dart\n    controllers/\n      user_controller.dart      \n    ...\n\n\n\n\n\nApplications are run with the command line tool \naqueduct serve\n. This tool create multiple \nIsolate\ns (a thread managed by the VM) and creates an instance of the \nRequestSink\n for each. Therefore, the channel of \nRequestController\ns created by the \nRequestSink\n is replicated for each isolate. Requests are divvied up among each isolate to maximize CPU and resource usage.\n\n\nSubclassing RequestSink\n\n\nThe responsibility of a \nRequestSink\n is to set up routes and initialize services it will use to fulfill requests. There are five initialization methods in \nRequestSink\n, each with its own purpose. The methods and the order they are executed in are as follows:\n\n\n\n\nRequestSink.initializeApplication\n: this \nstatic\n method is called once at the very beginning of an application's startup, before any instances of \nRequestSink\n are created.\n\n\nAn instance of \nRequestSink\n is created with its default constructor.\n\n\nThe method \nsetupRouter\n is invoked on the \nRequestSink\n instance; initialized properties from the constructor are injected into controllers created here.\n\n\nThe method \nwillOpen\n is invoked on the \nRequestSink\n instance.\n\n\nThe method \ndidOpen\n is invoked on the \nRequestSink\n instance.\n\n\n\n\nOnly \nsetupRouter\n is required, but it is extremely common to provide a constructor and implement \nRequestSink.initializeApplication\n. It is rare to use \nwillOpen\n and rarer still to use \ndidOpen\n.\n\n\nAqueduct applications will create more than one instance of \nRequestSink\n and repeat steps 2-5 for each instance. See a later section on multi-threading in Aqueduct applications.\n\n\nThe usage and details for each of these initialization methods is detailed in the following sections.\n\n\nUse RequestSink.initializeApplication for One-Time Initialization\n\n\nSince many instances of \nRequestSink\n will be created in an Aqueduct application, its instance methods and constructor will be called multiple times. An Aqueduct application may often have to execute one-time startup tasks that must not occur more than once. For this purpose, you may implement a \nstatic\n method with the following name and signature in an application's \nRequestSink\n:\n\n\nstatic\n \nFuture\n \ninitializeApplication\n(\nApplicationConfiguration\n \nconfig\n)\n \nasync\n \n{\n\n  \n...\n \ndo\n \none\n \ntime\n \nsetup\n \n...\n\n\n}\n\n\n\n\n\n\nA common use of this method is to set up services that will be shared across isolates or unique persistent connections to remote services. This method can modify \nApplicationConfiguration\n prior to \nRequestSink\n instances being created. This allows services allocated during this one-time setup method can be accessible by each \nRequestSink\n instance.\n\n\nFor example:\n\n\nstatic\n \nFuture\n \ninitializeApplication\n(\nApplicationConfiguration\n \nconfig\n)\n \nasync\n \n{\n        \n  \nconfig\n.\noptions\n[\nspecial item\n]\n \n=\n \nxyz\n;\n\n\n}\n  \n\n\nRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n{\n\n  \nvar\n \nparsedConfigValues\n \n=\n \nconfig\n.\noptions\n[\nspecial item\n];\n \n// == xyz\n\n\n}\n\n\n\n\n\n\nIn a more complex example, Aqueduct applications that use \nscribe\n will implement \ninitializeApplication\n to optimize logging. \nscribe\n works by spawning a new isolate that writes log statements to disk or the console. Aqueduct isolates send their log messages to the logging isolate so that they can move on to more important things. The logging isolate is spawned in \ninitializeApplication\n and a reference is stored in \nApplicationConfiguration\n. Each \nRequestSink\n grabs the reference so that it can send messages to the logging isolate later.\n\n\nIt is important to note the behavior of isolates as it relates to Aqueduct and the initialization process. Each isolate has its own heap. \ninitializeApplication\n is executed in the main isolate, whereas each \nRequestSink\n is instantiated in its own isolate. This means that any values stored in \nApplicationConfiguration\n must be safe to pass across isolates - i.e., they can't contain references to closures.\n\n\nAdditionally, any static or global variables that are set in the main isolate \nwill not be set\n in other isolates. Configuration types like \nHTTPCodecRepository\n do not share values across isolates. Therefore, they must be set up in the \nRequestSink\n constructor.\n\n\n/// Do not do this!\n\n\nstatic\n \nFuture\n \ninitializeApplication\n(\nApplicationConfiguration\n \nconfig\n)\n \nasync\n \n{\n        \n  \nHTTPCodecRepository\n.\nadd\n(\nnew\n \nContentType\n(\napplication\n,\n \nxml\n),\n \nnew\n \nXMLCodec\n());\n\n\n}\n  \n\n\n\n\n\nAlso, because static methods cannot be overridden in Dart, it is important that you ensure the name and signature of \ninitializeApplication\n exactly matches what is shown in these code samples. The analyzer can't help you here, unfortunately.\n\n\nUse RequestSink's Constructor to Initialize Service and Isolate-Specific Configurations\n\n\nA service, in this context, is something your application will use to fulfill requests. A database connection is an example of a service. Services should be created the constructor of a \nRequestSink\n and stored as properties.\n\n\nThe constructor of a \nRequestSink\n must be unnamed and take a single argument of type \nApplicationConfiguration\n. This instance of \nApplicationConfiguration\n will have the same values as the instance in the previous initialization step (\ninitializeApplication\n). The configuration contains a path to the configuration file that the application was started with (this defaults to \nconfig.yaml\n). This file often contains values that set up things like database connections:\n\n\nThe values in \nApplicationConfiguration\n often contain the details for the services that should be created - like database connection information. Here is an example:\n\n\nclass\n \nMySink\n \nextends\n \nRequestSink\n \n{\n\n  \nMySink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n    \nvar\n \nappConfigValues\n \n=\n \nnew\n \nMyConfig\n(\nconfig\n.\nconfigurationFilePath\n);\n\n    \nvar\n \ndatabaseConnection\n \n=\n \nnew\n \nDatabaseConnection\n()\n\n      \n..\nconnectionInfo\n \n=\n \nappConfigValues\n.\ndatabase\n;\n\n  \n}\n\n\n}\n\n\n\n\n\n\n(See more details about using configuration files \nhere\n.)\n\n\nIsolate specific initialization should also be set in this method:\n\n\nclass\n \nMySink\n \nextends\n \nRequestSink\n \n{\n\n  \nMySink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n    \nHTTPCodecRepository\n.\nadd\n(\nnew\n \nContentType\n(\napplication\n,\n \nxml\n),\n \nnew\n \nXMLCodec\n());\n\n  \n}\n\n\n}\n  \n\n\n\n\n\nAll of the properties of a \nRequestSink\n should be initialized in its constructor. This allows the next phase of initialization - setting up routes - to inject these services into controllers. For example, a typical \nRequestSink\n will have some property that holds a database connection; this property should be initialized in the constructor.\n\n\nA constructor should never call asynchronous functions. Some services require asynchronous initialization - e.g., a database connection has to connect to a database - but those must be fully initialized later. (See a later section on Lazy Services.)\n\n\nSetting up Routes in setupRouter\n\n\nOnce a \nRequestSink\n is instantiated, its \nsetupRouter\n method is invoked. This method takes a \nRouter\n that you must configure with all of the routes your application will respond to. (See \nRouting\n for more details.)\n\n\nWhen setting up routes, you will create many instances of \nRequestController\n. Any services these controllers need should be injected in their constructor. For example, \nAuthorizer\ns need an instance of \nAuthServer\n to validate a request. The following code is an example of this:\n\n\nclass\n \nMySink\n \nextends\n \nRequestSink\n \n{\n\n  \nMySink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n  \n    \nauthServer\n \n=\n \nnew\n \nAuthServer\n(...);\n\n  \n}\n\n\n  \nAuthServer\n \nauthServer\n;\n\n\n  \n@\noverride\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n    \nrouter\n\n        \n.\nroute\n(\n/path\n)\n\n        \n.\npipe\n(\nnew\n \nAuthorizer\n(\nauthServer\n))\n\n        \n.\nlisten\n((\nreq\n)\n \n=\n \nnew\n \nResponse\n.\nok\n(\nAuthorized!\n));\n\n  \n}\n\n\n}\n\n\n\n\n\n\nThis is the only time routes may be set up in an application, as the \nRouter\n will restructure its registered routes into an optimized, immutable collection after this method is invoked.\n\n\nYou may not call any asynchronous functions in this this method.\n\n\nAfter \nsetupRouter\n has completed, the \nRequestSink.router\n property is set to the router this method configured.\n\n\nPerform Asynchronous Initialization with willOpen\n\n\nFor any initialization that needs to occur asynchronously, you may override \nRequestSink.willOpen\n. This method is asynchronous, and the application will wait for this method to complete before sending any HTTP requests to the request sink. In general, you should avoid using this method and read the later section on Lazy Services.\n\n\nStart Receiving Requests\n\n\nOnce an \nRequestSink\n sets up its routes and performs asynchronous initialization, the application will hook up the stream of HTTP requests to the \nRequestSink\n and data will start flowing. Just prior to this, one last method is invoked on \nRequestSink\n, \ndidOpen\n. This method is a final callback to the \nRequestSink\n that indicates all initialization has completed.\n\n\nLazy Services\n\n\nAn Aqueduct application will probably communicate to other servers and databases. A \nRequestSink\n will have properties to represent these connections. Services like these must open a persistent network connection, a process that is asynchronous by nature. Following the initialization process of a \nRequestSink\n, it may then make sense to create the services in a constructor and then open them in \nwillOpen\n.\n\n\nHowever, an Aqueduct application will run for a long time. It is probable that connections it uses will occasionally be interrupted. If these connections are only opened when the application first starts, the application will not be able to reopen these connections without restarting it. This would be catastrophic.\n\n\nFor that reason, asynchronous services should manage their own opening behavior. For example, a database connection should open it when it is asked to execute a query. If it has a valid connection, it will go ahead and execute the query. Otherwise, it will establish the connection and then execute the query. The caller doesn't care - they get a \nFuture\n with the desired data.\n\n\nThe pseudo-code looks something like this:\n\n\nFuture\n \nexecute\n(\nString\n \nsql\n)\n \nasync\n \n{\n\n  \nif\n \n(\nconnection\n \n==\n \nnull\n \n||\n \n!\nconnection\n.\nisAvailable\n)\n \n{\n\n    \nconnection\n \n=\n \nnew\n \nConnection\n(...);\n\n    \nawait\n \nconnection\n.\nopen\n();\n\n  \n}\n\n\n  \nreturn\n \nawait\n \nconnection\n.\nexecuteSQL\n(\nsql\n);\n\n\n}\n\n\n\n\n\n\nFrom the perspective of an \nHTTPController\n, it doesn't care about the underlying connection. It invokes \nexecute\n, and the connection object figures out if it needs to establish a connection first:\n\n\n@\nhttpGet\n \ngetThings\n()\n \nasync\n \n{\n\n  \n// May or may not create a new connection, but will either return\n\n  \n// some things or throw an error.\n\n  \nvar\n \nthings\n \n=\n \nawait\n \nconnection\n.\nexecute\n(\nselect * from things\n);\n\n\n  \n...\n\n\n}\n\n\n\n\n\n\nMulti-threaded Aqueduct Applications\n\n\nAqueduct applications can - and should - be spread across a number of threads. This allows an application to take advantage of multiple CPUs and serve requests faster. In Dart, threads are called \nisolates\n (and have some slight nuances to them that makes the different than a traditional thread). Spreading requests across isolates is an architectural tenet of Aqueduct applications.\n\n\nWhen an application is started with \naqueduct serve\n, a option indicates how many isolates the application should run on.\n\n\naqueduct serve --isolates 3\n\n\n\n\n\nThe number of isolates defaults to 3. An application will spawn that many isolates and create an instance of \nRequestSink\n for each. When an HTTP request is received, one of the isolates - and its \nRequestSink\n - will receive the request while the others will never see it. Each isolate works independently of each other, running as their own \"web server\" within a web server. Because a \nRequestSink\n initializes itself in the exact same way on each isolate, each isolate behaves exactly the same way.\n\n\nAn isolate can't share memory with another isolate. Therefore, each \nRequestSink\n instance has its own set of services, like database connections. This behavior also makes connection pooling implicit - the connections are effectively pooled by the fact that there is a pool of \nRequestSink\ns. If a \nRequestSink\n creates a database connection and an application is started with four isolates, there will be four database connections total.\n\n\nHowever, there are times where you want your own pool or you want to share a single service across multiple isolates. For example, an API that must register with some other server (like in a system with an event bus) or must maintain a single persistent connection (like the error pipe to Apple's Push Notification Service or a streaming connection to Nest). These types of services should be instantiated in \ninitializeApplication\n.\n\n\nThe Application Object\n\n\nHidden in all of this discussion is the \nApplication\nT\n object. Because the \naqueduct serve\n command manages creating \nApplication\nT\n instances, your code rarely concerns itself with this type.\n\n\nAn \nApplication\nT\n is the top-level object in an Aqueduct application; it setups up HTTP listeners and sends their requests to \nRequestSink\n instances. The \nApplication\nT\n itself is just a generic container for \nRequestSink\ns; it doesn't do much other than kick everything off.\n\n\nThe application's \nstart\n method will initialize at least one instance of the application's \nRequestSink\n. If something goes wrong during this initialization process, the application will throw an exception and halt starting the server. For example, setting up an invalid route in a \nRequestSink\n subclass would trigger this type of startup exception.\n\n\nAn \nApplication\nT\n has a number of options that determine how it will listen for HTTP requests, such as which port it is listening on or the SSL certificate it will use. These values are available in the application's \nconfiguration\n property, an instance of \nApplicationConfiguration\n.\n\n\nProperties of an \nApplicationConfiguration\n and \nApplication\nT\n are provided through the \naqueduct serve\n command-line options.", 
            "title": "Initialization and the RequestSink"
        }, 
        {
            "location": "/http/request_sink/#application-initialization-and-the-requestsink", 
            "text": "The only requirement of an Aqueduct application is that it has exactly one  RequestSink  subclass. This subclass handles the initialization of an application, including setting up routes, authorization and database connections.  A  RequestSink  subclass is declared in its own file named  lib/ application_name _request_sink.dart , which is exported from the application library file. (For example if the application is named  wildfire , the application library file is  lib/wildfire.dart .)  wildfire/\n  lib/\n    wildfire.dart\n    wildfire_request_sink.dart\n    controllers/\n      user_controller.dart      \n    ...  Applications are run with the command line tool  aqueduct serve . This tool create multiple  Isolate s (a thread managed by the VM) and creates an instance of the  RequestSink  for each. Therefore, the channel of  RequestController s created by the  RequestSink  is replicated for each isolate. Requests are divvied up among each isolate to maximize CPU and resource usage.", 
            "title": "Application Initialization and the RequestSink"
        }, 
        {
            "location": "/http/request_sink/#subclassing-requestsink", 
            "text": "The responsibility of a  RequestSink  is to set up routes and initialize services it will use to fulfill requests. There are five initialization methods in  RequestSink , each with its own purpose. The methods and the order they are executed in are as follows:   RequestSink.initializeApplication : this  static  method is called once at the very beginning of an application's startup, before any instances of  RequestSink  are created.  An instance of  RequestSink  is created with its default constructor.  The method  setupRouter  is invoked on the  RequestSink  instance; initialized properties from the constructor are injected into controllers created here.  The method  willOpen  is invoked on the  RequestSink  instance.  The method  didOpen  is invoked on the  RequestSink  instance.   Only  setupRouter  is required, but it is extremely common to provide a constructor and implement  RequestSink.initializeApplication . It is rare to use  willOpen  and rarer still to use  didOpen .  Aqueduct applications will create more than one instance of  RequestSink  and repeat steps 2-5 for each instance. See a later section on multi-threading in Aqueduct applications.  The usage and details for each of these initialization methods is detailed in the following sections.", 
            "title": "Subclassing RequestSink"
        }, 
        {
            "location": "/http/request_sink/#use-requestsinkinitializeapplication-for-one-time-initialization", 
            "text": "Since many instances of  RequestSink  will be created in an Aqueduct application, its instance methods and constructor will be called multiple times. An Aqueduct application may often have to execute one-time startup tasks that must not occur more than once. For this purpose, you may implement a  static  method with the following name and signature in an application's  RequestSink :  static   Future   initializeApplication ( ApplicationConfiguration   config )   async   { \n   ...   do   one   time   setup   ...  }   A common use of this method is to set up services that will be shared across isolates or unique persistent connections to remote services. This method can modify  ApplicationConfiguration  prior to  RequestSink  instances being created. This allows services allocated during this one-time setup method can be accessible by each  RequestSink  instance.  For example:  static   Future   initializeApplication ( ApplicationConfiguration   config )   async   {         \n   config . options [ special item ]   =   xyz ;  }    RequestSink ( ApplicationConfiguration   config )   { \n   var   parsedConfigValues   =   config . options [ special item ];   // == xyz  }   In a more complex example, Aqueduct applications that use  scribe  will implement  initializeApplication  to optimize logging.  scribe  works by spawning a new isolate that writes log statements to disk or the console. Aqueduct isolates send their log messages to the logging isolate so that they can move on to more important things. The logging isolate is spawned in  initializeApplication  and a reference is stored in  ApplicationConfiguration . Each  RequestSink  grabs the reference so that it can send messages to the logging isolate later.  It is important to note the behavior of isolates as it relates to Aqueduct and the initialization process. Each isolate has its own heap.  initializeApplication  is executed in the main isolate, whereas each  RequestSink  is instantiated in its own isolate. This means that any values stored in  ApplicationConfiguration  must be safe to pass across isolates - i.e., they can't contain references to closures.  Additionally, any static or global variables that are set in the main isolate  will not be set  in other isolates. Configuration types like  HTTPCodecRepository  do not share values across isolates. Therefore, they must be set up in the  RequestSink  constructor.  /// Do not do this!  static   Future   initializeApplication ( ApplicationConfiguration   config )   async   {         \n   HTTPCodecRepository . add ( new   ContentType ( application ,   xml ),   new   XMLCodec ());  }     Also, because static methods cannot be overridden in Dart, it is important that you ensure the name and signature of  initializeApplication  exactly matches what is shown in these code samples. The analyzer can't help you here, unfortunately.", 
            "title": "Use RequestSink.initializeApplication for One-Time Initialization"
        }, 
        {
            "location": "/http/request_sink/#use-requestsinks-constructor-to-initialize-service-and-isolate-specific-configurations", 
            "text": "A service, in this context, is something your application will use to fulfill requests. A database connection is an example of a service. Services should be created the constructor of a  RequestSink  and stored as properties.  The constructor of a  RequestSink  must be unnamed and take a single argument of type  ApplicationConfiguration . This instance of  ApplicationConfiguration  will have the same values as the instance in the previous initialization step ( initializeApplication ). The configuration contains a path to the configuration file that the application was started with (this defaults to  config.yaml ). This file often contains values that set up things like database connections:  The values in  ApplicationConfiguration  often contain the details for the services that should be created - like database connection information. Here is an example:  class   MySink   extends   RequestSink   { \n   MySink ( ApplicationConfiguration   config )   :   super ( config )   { \n     var   appConfigValues   =   new   MyConfig ( config . configurationFilePath ); \n     var   databaseConnection   =   new   DatabaseConnection () \n       .. connectionInfo   =   appConfigValues . database ; \n   }  }   (See more details about using configuration files  here .)  Isolate specific initialization should also be set in this method:  class   MySink   extends   RequestSink   { \n   MySink ( ApplicationConfiguration   config )   :   super ( config )   { \n     HTTPCodecRepository . add ( new   ContentType ( application ,   xml ),   new   XMLCodec ()); \n   }  }     All of the properties of a  RequestSink  should be initialized in its constructor. This allows the next phase of initialization - setting up routes - to inject these services into controllers. For example, a typical  RequestSink  will have some property that holds a database connection; this property should be initialized in the constructor.  A constructor should never call asynchronous functions. Some services require asynchronous initialization - e.g., a database connection has to connect to a database - but those must be fully initialized later. (See a later section on Lazy Services.)", 
            "title": "Use RequestSink's Constructor to Initialize Service and Isolate-Specific Configurations"
        }, 
        {
            "location": "/http/request_sink/#setting-up-routes-in-setuprouter", 
            "text": "Once a  RequestSink  is instantiated, its  setupRouter  method is invoked. This method takes a  Router  that you must configure with all of the routes your application will respond to. (See  Routing  for more details.)  When setting up routes, you will create many instances of  RequestController . Any services these controllers need should be injected in their constructor. For example,  Authorizer s need an instance of  AuthServer  to validate a request. The following code is an example of this:  class   MySink   extends   RequestSink   { \n   MySink ( ApplicationConfiguration   config )   :   super ( config )   {   \n     authServer   =   new   AuthServer (...); \n   } \n\n   AuthServer   authServer ; \n\n   @ override \n   void   setupRouter ( Router   router )   { \n     router \n         . route ( /path ) \n         . pipe ( new   Authorizer ( authServer )) \n         . listen (( req )   =   new   Response . ok ( Authorized! )); \n   }  }   This is the only time routes may be set up in an application, as the  Router  will restructure its registered routes into an optimized, immutable collection after this method is invoked.  You may not call any asynchronous functions in this this method.  After  setupRouter  has completed, the  RequestSink.router  property is set to the router this method configured.", 
            "title": "Setting up Routes in setupRouter"
        }, 
        {
            "location": "/http/request_sink/#perform-asynchronous-initialization-with-willopen", 
            "text": "For any initialization that needs to occur asynchronously, you may override  RequestSink.willOpen . This method is asynchronous, and the application will wait for this method to complete before sending any HTTP requests to the request sink. In general, you should avoid using this method and read the later section on Lazy Services.", 
            "title": "Perform Asynchronous Initialization with willOpen"
        }, 
        {
            "location": "/http/request_sink/#start-receiving-requests", 
            "text": "Once an  RequestSink  sets up its routes and performs asynchronous initialization, the application will hook up the stream of HTTP requests to the  RequestSink  and data will start flowing. Just prior to this, one last method is invoked on  RequestSink ,  didOpen . This method is a final callback to the  RequestSink  that indicates all initialization has completed.", 
            "title": "Start Receiving Requests"
        }, 
        {
            "location": "/http/request_sink/#lazy-services", 
            "text": "An Aqueduct application will probably communicate to other servers and databases. A  RequestSink  will have properties to represent these connections. Services like these must open a persistent network connection, a process that is asynchronous by nature. Following the initialization process of a  RequestSink , it may then make sense to create the services in a constructor and then open them in  willOpen .  However, an Aqueduct application will run for a long time. It is probable that connections it uses will occasionally be interrupted. If these connections are only opened when the application first starts, the application will not be able to reopen these connections without restarting it. This would be catastrophic.  For that reason, asynchronous services should manage their own opening behavior. For example, a database connection should open it when it is asked to execute a query. If it has a valid connection, it will go ahead and execute the query. Otherwise, it will establish the connection and then execute the query. The caller doesn't care - they get a  Future  with the desired data.  The pseudo-code looks something like this:  Future   execute ( String   sql )   async   { \n   if   ( connection   ==   null   ||   ! connection . isAvailable )   { \n     connection   =   new   Connection (...); \n     await   connection . open (); \n   } \n\n   return   await   connection . executeSQL ( sql );  }   From the perspective of an  HTTPController , it doesn't care about the underlying connection. It invokes  execute , and the connection object figures out if it needs to establish a connection first:  @ httpGet   getThings ()   async   { \n   // May or may not create a new connection, but will either return \n   // some things or throw an error. \n   var   things   =   await   connection . execute ( select * from things ); \n\n   ...  }", 
            "title": "Lazy Services"
        }, 
        {
            "location": "/http/request_sink/#multi-threaded-aqueduct-applications", 
            "text": "Aqueduct applications can - and should - be spread across a number of threads. This allows an application to take advantage of multiple CPUs and serve requests faster. In Dart, threads are called  isolates  (and have some slight nuances to them that makes the different than a traditional thread). Spreading requests across isolates is an architectural tenet of Aqueduct applications.  When an application is started with  aqueduct serve , a option indicates how many isolates the application should run on.  aqueduct serve --isolates 3  The number of isolates defaults to 3. An application will spawn that many isolates and create an instance of  RequestSink  for each. When an HTTP request is received, one of the isolates - and its  RequestSink  - will receive the request while the others will never see it. Each isolate works independently of each other, running as their own \"web server\" within a web server. Because a  RequestSink  initializes itself in the exact same way on each isolate, each isolate behaves exactly the same way.  An isolate can't share memory with another isolate. Therefore, each  RequestSink  instance has its own set of services, like database connections. This behavior also makes connection pooling implicit - the connections are effectively pooled by the fact that there is a pool of  RequestSink s. If a  RequestSink  creates a database connection and an application is started with four isolates, there will be four database connections total.  However, there are times where you want your own pool or you want to share a single service across multiple isolates. For example, an API that must register with some other server (like in a system with an event bus) or must maintain a single persistent connection (like the error pipe to Apple's Push Notification Service or a streaming connection to Nest). These types of services should be instantiated in  initializeApplication .", 
            "title": "Multi-threaded Aqueduct Applications"
        }, 
        {
            "location": "/http/request_sink/#the-application-object", 
            "text": "Hidden in all of this discussion is the  Application T  object. Because the  aqueduct serve  command manages creating  Application T  instances, your code rarely concerns itself with this type.  An  Application T  is the top-level object in an Aqueduct application; it setups up HTTP listeners and sends their requests to  RequestSink  instances. The  Application T  itself is just a generic container for  RequestSink s; it doesn't do much other than kick everything off.  The application's  start  method will initialize at least one instance of the application's  RequestSink . If something goes wrong during this initialization process, the application will throw an exception and halt starting the server. For example, setting up an invalid route in a  RequestSink  subclass would trigger this type of startup exception.  An  Application T  has a number of options that determine how it will listen for HTTP requests, such as which port it is listening on or the SSL certificate it will use. These values are available in the application's  configuration  property, an instance of  ApplicationConfiguration .  Properties of an  ApplicationConfiguration  and  Application T  are provided through the  aqueduct serve  command-line options.", 
            "title": "The Application Object"
        }, 
        {
            "location": "/http/http_controller/", 
            "text": "HTTPController\n\n\nMost Aqueduct code is written in subclasses of \nHTTPController\n. Instances of this class receive requests for a particular resource. For example, an \nHTTPController\n subclass named \nUserController\n might handle requests to create, update, delete, read and list users. \nHTTPController\n is subclassed to implement an instance method for each one of these operations.\n\n\nFor example, a \nPOST /users\n would trigger a \ncreateUser\n method, whereas a \nGET /users/1\n would trigger a \ngetUserByID\n method. The names of these methods are up to you; the method that gets called is determined by metadata on the method and its parameters.\n\n\nResponder Methods and Parameter Binding\n\n\nAn \nHTTPController\n method that responds to a request is called a \nresponder method\n. A responder method must return a \nFuture\nResponse\n and have \nHTTPMethod\n metadata. Here's an example:\n\n\nclass\n \nUserController\n \nextends\n \nHTTPController\n \n{\n\n  \n@\nhttpGet\n\n  \nFuture\nResponse\n \ngetAllUsers\n()\n \nasync\n \n{\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\nawait\n \ngetUsersFromDatabase\n());\n\n  \n}\n\n\n}\n\n\n\n\n\n\nThe constant \nhttpGet\n is an instance of \nHTTPMethod\n. When a \nGET\n request is sent to an instance of \nUserController\n, the method \ngetAllUsers\n is invoked and the \nResponse\n it returns is sent to the HTTP client. There exist \nHTTPMethod\n constants for the major HTTP methods: \nhttpPut\n, \nhttpGet\n, \nhttpPost\n and \nhttpDelete\n. You may use \nHTTPMethod\n for other types of HTTP methods:\n\n\n@\nHTTPMethod\n(\nPATCH\n)\n\n\nFuture\nResponse\n \npatch\n()\n \nasync\n \n{\n \n...\n \n}\n\n\n\n\n\n\nIf a request is sent to an \nHTTPController\n and there isn't a responder method with matching \nHTTPMethod\n metadata, a 405 response is sent and no method is invoked.\n\n\nEach responder method can \nbind\n values from the HTTP request to its arguments. The following responder method binds the value from the path variable \nid\n:\n\n\n@\nhttpGet\n\n\nFuture\nResponse\n \ngetOneUser\n(\n@\nHTTPPath\n(\nid\n)\n \nint\n \nid\n)\n \nasync\n \n{\n\n  \nreturn\n \nnew\n \nResponse\n.\nok\n(\nawait\n \ngetAUserFromDatabaseByID\n(\nid\n));\n\n\n}\n\n\n\n\n\n\nWhen a \nroute contains a path variable\n (like \n/users/:id\n), the value of that path variable will be available in this argument. It is often the case that a path variable is an optional part of a route (like \n/users/[:id]\n). Thus, the request \n/users\n and \n/users/:id\n get sent to the same controller. There must be a responder method for both variants. For example, the following controller may respond to both \nGET /users\n and \nGET /users/1\n:\n\n\nclass\n \nUserController\n \nextends\n \nHTTPController\n \n{\n\n  \n@\nhttpGet\n\n  \nFuture\nResponse\n \ngetAllUsers\n()\n \nasync\n \n{\n\n    \n// invoked with path is /users\n\n  \n}\n\n\n  \n@\nhttpGet\n\n  \nFuture\nResponse\n \ngetOneUser\n(\n@\nHTTPPath\n(\nid\n)\n \nint\n \nid\n)\n \nasync\n \n{\n\n    \n// invoked with path is /users/:id\n\n  \n}\n\n\n}\n\n\n\n\n\n\nThe argument to \nHTTPPath\n is the name of the path variable as it is declared in the route. For example, if the route is \n/thing/:abcdef\n, the argument must be \n\"abcdef\"\n.\n\n\nThe variable that \nHTTPPath\n is bound to can be named whatever you want - you don't have to name it the same as the path variable.\n\n\nThe \ntype\n of the bound variable can be a \nString\n or any type that has a \nparse\n method (like \nint\n, \ndouble\n, \nHttpDate\n and \nDateTime\n). If the bound variable's type is not \nString\n or a type that implements \nparse\n, a 500 Server Error is returned.\n\n\nIf the bound path variable's type does implement \nparse\n, but the value from the request is in an invalid format, a 404 Not Found response is returned.\n\n\nQuery string parameters and header values may also be bound to responder methods with \nHTTPQuery\n and \nHTTPHeader\n metadata. (Note that a failed parse for query or header binding return a 400 Bad Request response.) The following responder method will bind the query string parameters \nlimit\n and \noffset\n to \nnumberOfThings\n and \noffset\n:\n\n\n@\nhttpGet\n\n\nFuture\nResponse\n \ngetThing\n(\n\n  \n@\nHTTPQuery\n(\nlimit\n)\n \nint\n \nnumberOfThings\n,\n\n  \n@\nHTTPQuery\n(\noffset\n)\n \nint\n \noffset\n)\n \nasync\n \n{\n\n    \nvar\n \nthings\n \n=\n \nawait\n \ngetThingsBetween\n(\noffset\n,\n \noffset\n \n+\n \nnumberOfThings\n);\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\nthings\n);\n\n\n}\n\n\n\n\n\n\nFor example, if the request was \n/?limit=10\noffset=0\n, the values of \nnumberOfThings\n and \noffset\n are 10 and 1. In this above method, both \nlimit\n and \noffset\n are \nrequired\n. If one or both are values are missing from the query string in a request, the responder method is not called and a 400 Bad Request response is sent.\n\n\nQuery parameters can be made optional by moving them to the optional part of the method signature. Thus, the following method still requires \nlimit\n, but if \noffset\n is omitted, its value defaults to 0:\n\n\n@\nhttpGet\n\n\nFuture\nResponse\n \ngetThing\n(\n\n  \n@\nHTTPQuery\n(\nlimit\n)\n \nint\n \nnumberOfThings\n,\n\n  \n{\n@\nHTTPQuery\n(\noffset\n)\n \nint\n \noffset:\n \n0\n})\n \nasync\n \n{\n\n    \nvar\n \nthings\n \n=\n \nawait\n \ngetThingsBetween\n(\noffset\n,\n \noffset\n \n+\n \nnumberOfThings\n);\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\nthings\n);\n\n\n}\n\n\n\n\n\n\nThe argument to \nHTTPQuery\n is case-sensitive.\n\n\nHeaders are bound in the same way using \nHTTPHeader\n metadata. Unlike \nHTTPQuery\n, \nHTTPHeader\ns are compared case-insensitively. Here's an example of a responder method that takes an optional \nX-Timestamp\n header:\n\n\n@\nhttpGet\n\n\nFuture\nResponse\n \ngetThings\n(\n\n  \n{\n@\nHTTPHeader\n(\nx-timestamp\n)\n \nDateTime\n \ntimestamp\n})\n \nasync\n \n{\n\n    \n...\n\n\n}\n\n\n\n\n\n\nThe properties of an \nHTTPController\ns may also have \nHTTPQuery\n and \nHTTPHeader\n metadata. This binds values from the request to the \nHTTPController\n instance itself, making them accessible from \nall\n responder methods.\n\n\nclass\n \nThingController\n \nextends\n \nHTTPController\n \n{\n\n  \n@\nrequiredHTTPParameter\n\n  \n@\nHTTPHeader\n(\nx-timestamp\n)\n\n  \nDateTime\n \ntimestamp\n;\n\n\n  \n@\nHTTPQuery\n(\nlimit\n)\n\n  \nint\n \nlimit\n;\n\n\n  \n@\nhttpGet\n\n  \nFuture\nResponse\n \ngetThings\n()\n \nasync\n \n{\n\n      \n// can use both limit and timestamp\n\n  \n}\n\n\n  \n@\nhttpGet\n\n  \nFuture\nResponse\n \ngetThing\n(\n@\nHTTPPath\n(\nid\n)\n \nint\n \nid\n)\n \nasync\n \n{\n\n      \n// can use both limit and timestamp\n\n  \n}\n\n\n}\n\n\n\n\n\n\nIn the above, both \ntimestamp\n and \nlimit\n are bound prior to \ngetThing\n and \ngetThings\n being invoked. By default, a bound property is optional but can have additional \nrequiredHTTPParameter\n metadata. If required, any request without the required property fails with a 400 Bad Request status code and none of the responder methods are invoked.\n\n\nAqueduct treats \nPOST\n and \nPUT\n requests with \napplication/x-www-form-urlencoded\n content type as query strings, so the body of the request can be bound to \nHTTPQuery\n parameters.\n\n\nQuery strings can have repeating keys, i.e. \n/?x=1\nx=2\n. You may also bind a query parameter to a \nList\n:\n\n\n@\nhttpGet\n\n\nFuture\nResponse\n \ngetThing\n(\n@\nHTTPQuery\n(\nx\n)\n \nList\nString\n \nxs\n)\n \nasync\n \n{\n\n  \n// xs = [\n1\n, \n2\n]\n\n\n}\n\n\n\n\n\n\nQuery strings may also have no value, i.e. \n/?flag\n. You may bind a query parameter to a boolean that will be true if the bound key is present in the query string:\n\n\n@\nhttpGet\n\n\nFuture\nResponse\n \ngetThing\n(\n@\nHTTPQuery\n(\nflag\n)\n \nbool\n \nflag\n)\n \nasync\n \n{\n\n  \n// xs = true\n\n\n}\n\n\n\n\n\n\nBinding HTTP Request Bodies\n\n\nYou may also bind an HTTP request body to an object with \n@HTTPBody\n metadata:\n\n\n@\nhttpPost\n\n\nFuture\nResponse\n \ncreateUser\n(\n@\nHTTPBody\n()\n \nUser\n \nuser\n)\n \nasync\n \n{\n\n  \nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n    \n..\nvalues\n \n=\n \nuser\n;\n\n  \nvar\n \ninsertedUser\n \n=\n \nawait\n \nquery\n.\ninsert\n();\n\n  \nreturn\n \nnew\n \nResponse\n.\nok\n(\ninsertedUser\n);\n\n\n}\n\n\n\n\n\n\nBody binding is available for both properties and responder method parameters, just like query and header bindings.\n\n\nAn type must implement \nHTTPSerializable\n to be bound to a request body. This interface requires that the method \nreadFromMap\n be implemented:\n\n\nclass\n \nPerson\n \nimplements\n \nHTTPSerializable\n \n{\n\n  \nString\n \nname\n;\n\n  \nString\n \nemail\n;\n\n\n  \n@\noverride\n\n  \nvoid\n \nreadFromMap\n(\nMap\nString\n,\n \ndynamic\n \nrequestBody\n)\n \n{\n\n    \nname\n \n=\n \nrequestBody\n[\nname\n];\n\n    \nemail\n \n=\n \nrequestBody\n[\nemail\n];\n\n  \n}\n\n\n  \n@\noverride\n\n  \nMap\nString\n,\n \ndynamic\n \nasMap\n()\n \n{\n\n    \nreturn\n \n{\n\n      \nname\n:\n \nname\n,\n\n      \nemail\n:\n \nemail\n\n    \n};\n\n  \n}\n\n\n}\n\n\n\nclass\n \nPersonController\n \nextends\n \nHTTPController\n \n{\n\n  \n@\nhttpPost\n\n  \nFuture\nResponse\n \ncreatePerson\n(\n@\nHTTPBody\n()\n \nPerson\n \np\n)\n \n{\n\n    \n// p.name and p.email are read from body when body is {\nname\n: \n...\n, \nemail\n: \n...\n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nYou may also bind a \nList\nHTTPSerializable\n:\n\n\nclass\n \nPersonController\n \nextends\n \nHTTPController\n \n{\n\n  \n@\nhttpPost\n\n  \nFuture\nResponse\n \ncreatePerson\n(\n@\nHTTPBody\n()\n \nList\nPerson\n \npeople\n)\n \n{\n\n    \n// When body is [{\nname\n: \n...\n, \nemail\n: \n...\n}]\n\n  \n}\n\n\n}\n\n\n\n\n\n\nThe request body is decoded based on its content type prior to binding it to an \nHTTPSerializable\n. Thus, for data like \napplication/json\n, the bound body object is read from a \nMap\nString, dynamic\n.\n\n\nRequest and Response Bodies\n\n\nAn \nHTTPController\n can limit the content type of HTTP request bodies it accepts. By default, an \nHTTPController\n will accept both \napplication/json\n and \napplication/x-www-form-urlencoded\n request bodies for its \nPOST\n and \nPUT\n methods. This can be modified by setting the \nacceptedContentTypes\n property in the constructor.\n\n\nclass\n \nUserController\n \nextends\n \nHTTPController\n \n{\n\n  \nUserController\n()\n \n{\n\n    \nacceptedContentTypes\n \n=\n \n[\nContentType\n.\nJSON\n,\n \nContentType\n.\nXML\n];\n\n  \n}\n\n\n}\n\n\n\n\n\n\nIf a request is made with a content type other than the accepted content types, the controller automatically responds with a 415 Unsupported Media Type response.\n\n\nThe body of an HTTP request is decoded if the content type is accepted and there exists a responder method to handle the request. This means two things. First, the body is not decoded if the request is going to be discarded because no responder method was found.\n\n\nSecond, methods on \nHTTPRequestBody\n have two flavors: those that return the contents as a \nFuture\n or those that return the already decoded body. Responder methods can access the already decoded body without awaiting on the \nFuture\n-flavored variants of \nHTTPRequestBody\n:\n\n\n@\nhttpPost\n\n\nFuture\nResponse\n \ncreateThing\n()\n \nasync\n \n{\n\n  \n// do this:\n\n  \nvar\n \nbodyMap\n \n=\n \nrequest\n.\nbody\n.\nasMap\n();\n\n\n  \n// no need to do this:\n\n  \nvar\n \nbodyMap\n \n=\n \nawait\n \nrequest\n.\nbody\n.\ndecodeAsMap\n();\n\n\n  \nreturn\n \n...;\n\n\n}\n\n\n\n\n\n\nAn \nHTTPController\n can also have a default content type for its \nresponse\n bodies. By default, this is \napplication/json\n - any response body returned as JSON. This default can be changed by changing \nresponseContentType\n in the constructor:\n\n\nclass\n \nUserController\n \nextends\n \nHTTPController\n \n{\n\n  \nUserController\n()\n \n{\n\n    \nresponseContentType\n \n=\n \nContentType\n.\nXML\n;\n\n  \n}\n\n\n}\n\n\n\n\n\n\nThe \nresponseContentType\n is the \ndefault\n response content type. An individual \nResponse\n may set its own \ncontentType\n, which takes precedence over the \nresponseContentType\n. For example, the following controller returns JSON by default, but if the request specifically asks for XML, that's what it will return:\n\n\nclass\n \nUserController\n \nextends\n \nHTTPController\n \n{\n\n  \nUserController\n()\n \n{\n\n    \nresponseContentType\n \n=\n \nContentType\n.\nJSON\n;\n\n  \n}\n\n\n  \n@\nhttpGet\n\n  \nFuture\nResponse\n \ngetUserByID\n(\n@\nHTTPPath\n(\nid\n)\n \nint\n \nid\n)\n \nasync\n \n{\n\n    \nvar\n \nresponse\n \n=\n \nnew\n \nResponse\n.\nok\n(...);\n\n\n    \nif\n \n(\nrequest\n.\nheaders\n.\nvalue\n(\nHttpHeaders\n.\nACCEPT\n).\nstartsWith\n(\napplication/xml\n))\n \n{\n\n      \nresponse\n.\ncontentType\n \n=\n \nContentType\n.\nXML\n;\n\n    \n}\n\n\n    \nreturn\n \nresponse\n;\n\n  \n}\n\n\n}\n\n\n\n\n\n\nMore Specialized HTTPControllers\n\n\nBecause many \nHTTPController\n subclasses will execute \nqueries\n, there are helpful \nHTTPController\n subclasses for reducing boilerplate code.\n\n\nA \nQueryController\nT\n builds a \nQuery\nT\n based on the incoming request. If the request has a body, this \nQuery\nT\n's \nvalues\n property is read from that body. If the request has a path variable, the \nQuery\nT\n assigns a matcher to the primary key value of its \nwhere\n. For example, in a normal \nHTTPController\n that responds to a PUT request, you might write the following:\n\n\n@\nhttpPut\n\n\nFuture\nResponse\n \nupdateUser\n(\n@\nHTTPPath\n(\nid\n)\n \nint\n \nid\n)\n \nasync\n \n{\n\n  \nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n    \n..\nwhere\n.\nid\n \n=\n \nwhereEqualTo\n(\nid\n)\n\n    \n..\nvalues\n \n=\n \n(\nnew\n \nUser\n()..\nreadFromMap\n(\nrequest\n.\nbody\n.\nasMap\n());\n\n\n  \nreturn\n \nnew\n \nResponse\n.\nok\n(\nawait\n \nquery\n.\nupdateOne\n());\n\n\n}\n\n\n\n\n\n\nA \nQueryController\nT\n builds this query before a responder method is invoked, storing it in the inherited \nquery\n property. A \nManagedObject\nT\n subclass is the type argument to \nQueryController\nT\n.\n\n\nclass\n \nUserController\n \nextends\n \nQueryController\nUser\n \n{\n\n  \nFuture\nResponse\n \nupdateUser\n(\n@\nHTTPPath\n(\nid\n)\n \nint\n \nid\n)\n \nasync\n \n{\n\n    \n// query already exists and is identical to the snippet above\n\n    \nvar\n \nresult\n \n=\n \nawait\n \nquery\n.\nupdateOne\n();\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\nresult\n);\n\n  \n}\n\n\n}\n\n\n\n\n\n\nA \nManagedObjectController\nT\n is significantly more powerful; you don't even need to subclass it. It does all the things a CRUD endpoint does without any code. Here's an example usage:\n\n\nrouter\n\n  \n.\nroute\n(\n/users/[:id]\n)\n\n  \n.\ngenerate\n(()\n \n=\n \nnew\n \nManagedObjectController\nUser\n());\n\n\n\n\n\n\nThis controller has the following behavior:\n\n\n\n\n\n\n\n\nRequest\n\n\nAction\n\n\n\n\n\n\n\n\n\n\nPOST /users\n\n\nInserts a user into the database with values from the request body\n\n\n\n\n\n\nGET /users\n\n\nFetches all users in the database\n\n\n\n\n\n\nGET /users/:id\n\n\nFetches a single user by id\n\n\n\n\n\n\nDELETE /users/:id\n\n\nDeletes a single user by id\n\n\n\n\n\n\nPUT /users/:id\n\n\nUpdated a single user by id, using values from the request body\n\n\n\n\n\n\n\n\nThe objects returned from getting the collection - e.g, \nGET /users\n - can be modified with query parameters. For example, the following request will return the users sorted by their name in ascending order:\n\n\nGET /users?sortBy=name,asc\n\n\n\n\n\n\nThe results can be paged (see \nPaging in Advanced Queries\n) with query parameters \noffset\n, \ncount\n, \npageBy\n, \npageAfter\n and \npagePrior\n.\n\n\nA \nManagedObjectController\nT\n can also be subclassed. A subclass allows for callbacks to be overridden to adjust the query before execution, or the results before sending the respond. Each operation - fetch, update, delete, etc. - has a pair of methods to do this. For example, the following subclass alters the query and results before any update via \nPUT\n:\n\n\nclass\n \nUserController\n \nextends\n \nManagedObjectController\nUser\n \n{\n\n  \nFuture\nQuery\nUser\n \nwillUpdateObjectWithQuery\n(\n\n      \nQuery\nUser\n \nquery\n)\n \nasync\n \n{\n\n    \nquery\n.\nvalues\n.\nlastUpdatedAt\n \n=\n \nnew\n \nDateTime\n.\nnow\n().\ntoUtc\n();\n\n    \nreturn\n \nquery\n;\n\n  \n}\n\n\n  \nFuture\nResponse\n \ndidUpdateObject\n(\nUser\n \nobject\n)\n \nasync\n \n{\n\n    \nobject\n.\nremovePropertyFromBackingMap\n(\nprivate\n);\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\nobject\n);\n\n  \n}\n\n\n}\n\n\n\n\n\n\nSee also \nvalidations\n, which are powerful when combined with \nManagedObjectController\nT\n.\n\n\nAccessing the Request\n\n\nAny value from the request itself can be accessed through the \nrequest\n property of a controller.\n\n\nThis also means that an \nHTTPController\n instance cannot be reused to handle multiple requests; if it awaited on an operation, a new request could be assigned to the \nrequest\n property. Therefore, all \nHTTPController\ns must be added to a request processing pipeline with \ngenerate\n. If you add a controller with \npipe\n, an exception will be thrown immediately at startup.", 
            "title": "Handling Requests: HTTPController"
        }, 
        {
            "location": "/http/http_controller/#httpcontroller", 
            "text": "Most Aqueduct code is written in subclasses of  HTTPController . Instances of this class receive requests for a particular resource. For example, an  HTTPController  subclass named  UserController  might handle requests to create, update, delete, read and list users.  HTTPController  is subclassed to implement an instance method for each one of these operations.  For example, a  POST /users  would trigger a  createUser  method, whereas a  GET /users/1  would trigger a  getUserByID  method. The names of these methods are up to you; the method that gets called is determined by metadata on the method and its parameters.", 
            "title": "HTTPController"
        }, 
        {
            "location": "/http/http_controller/#responder-methods-and-parameter-binding", 
            "text": "An  HTTPController  method that responds to a request is called a  responder method . A responder method must return a  Future Response  and have  HTTPMethod  metadata. Here's an example:  class   UserController   extends   HTTPController   { \n   @ httpGet \n   Future Response   getAllUsers ()   async   { \n     return   new   Response . ok ( await   getUsersFromDatabase ()); \n   }  }   The constant  httpGet  is an instance of  HTTPMethod . When a  GET  request is sent to an instance of  UserController , the method  getAllUsers  is invoked and the  Response  it returns is sent to the HTTP client. There exist  HTTPMethod  constants for the major HTTP methods:  httpPut ,  httpGet ,  httpPost  and  httpDelete . You may use  HTTPMethod  for other types of HTTP methods:  @ HTTPMethod ( PATCH )  Future Response   patch ()   async   {   ...   }   If a request is sent to an  HTTPController  and there isn't a responder method with matching  HTTPMethod  metadata, a 405 response is sent and no method is invoked.  Each responder method can  bind  values from the HTTP request to its arguments. The following responder method binds the value from the path variable  id :  @ httpGet  Future Response   getOneUser ( @ HTTPPath ( id )   int   id )   async   { \n   return   new   Response . ok ( await   getAUserFromDatabaseByID ( id ));  }   When a  route contains a path variable  (like  /users/:id ), the value of that path variable will be available in this argument. It is often the case that a path variable is an optional part of a route (like  /users/[:id] ). Thus, the request  /users  and  /users/:id  get sent to the same controller. There must be a responder method for both variants. For example, the following controller may respond to both  GET /users  and  GET /users/1 :  class   UserController   extends   HTTPController   { \n   @ httpGet \n   Future Response   getAllUsers ()   async   { \n     // invoked with path is /users \n   } \n\n   @ httpGet \n   Future Response   getOneUser ( @ HTTPPath ( id )   int   id )   async   { \n     // invoked with path is /users/:id \n   }  }   The argument to  HTTPPath  is the name of the path variable as it is declared in the route. For example, if the route is  /thing/:abcdef , the argument must be  \"abcdef\" .  The variable that  HTTPPath  is bound to can be named whatever you want - you don't have to name it the same as the path variable.  The  type  of the bound variable can be a  String  or any type that has a  parse  method (like  int ,  double ,  HttpDate  and  DateTime ). If the bound variable's type is not  String  or a type that implements  parse , a 500 Server Error is returned.  If the bound path variable's type does implement  parse , but the value from the request is in an invalid format, a 404 Not Found response is returned.  Query string parameters and header values may also be bound to responder methods with  HTTPQuery  and  HTTPHeader  metadata. (Note that a failed parse for query or header binding return a 400 Bad Request response.) The following responder method will bind the query string parameters  limit  and  offset  to  numberOfThings  and  offset :  @ httpGet  Future Response   getThing ( \n   @ HTTPQuery ( limit )   int   numberOfThings , \n   @ HTTPQuery ( offset )   int   offset )   async   { \n     var   things   =   await   getThingsBetween ( offset ,   offset   +   numberOfThings ); \n     return   new   Response . ok ( things );  }   For example, if the request was  /?limit=10 offset=0 , the values of  numberOfThings  and  offset  are 10 and 1. In this above method, both  limit  and  offset  are  required . If one or both are values are missing from the query string in a request, the responder method is not called and a 400 Bad Request response is sent.  Query parameters can be made optional by moving them to the optional part of the method signature. Thus, the following method still requires  limit , but if  offset  is omitted, its value defaults to 0:  @ httpGet  Future Response   getThing ( \n   @ HTTPQuery ( limit )   int   numberOfThings , \n   { @ HTTPQuery ( offset )   int   offset:   0 })   async   { \n     var   things   =   await   getThingsBetween ( offset ,   offset   +   numberOfThings ); \n     return   new   Response . ok ( things );  }   The argument to  HTTPQuery  is case-sensitive.  Headers are bound in the same way using  HTTPHeader  metadata. Unlike  HTTPQuery ,  HTTPHeader s are compared case-insensitively. Here's an example of a responder method that takes an optional  X-Timestamp  header:  @ httpGet  Future Response   getThings ( \n   { @ HTTPHeader ( x-timestamp )   DateTime   timestamp })   async   { \n     ...  }   The properties of an  HTTPController s may also have  HTTPQuery  and  HTTPHeader  metadata. This binds values from the request to the  HTTPController  instance itself, making them accessible from  all  responder methods.  class   ThingController   extends   HTTPController   { \n   @ requiredHTTPParameter \n   @ HTTPHeader ( x-timestamp ) \n   DateTime   timestamp ; \n\n   @ HTTPQuery ( limit ) \n   int   limit ; \n\n   @ httpGet \n   Future Response   getThings ()   async   { \n       // can use both limit and timestamp \n   } \n\n   @ httpGet \n   Future Response   getThing ( @ HTTPPath ( id )   int   id )   async   { \n       // can use both limit and timestamp \n   }  }   In the above, both  timestamp  and  limit  are bound prior to  getThing  and  getThings  being invoked. By default, a bound property is optional but can have additional  requiredHTTPParameter  metadata. If required, any request without the required property fails with a 400 Bad Request status code and none of the responder methods are invoked.  Aqueduct treats  POST  and  PUT  requests with  application/x-www-form-urlencoded  content type as query strings, so the body of the request can be bound to  HTTPQuery  parameters.  Query strings can have repeating keys, i.e.  /?x=1 x=2 . You may also bind a query parameter to a  List :  @ httpGet  Future Response   getThing ( @ HTTPQuery ( x )   List String   xs )   async   { \n   // xs = [ 1 ,  2 ]  }   Query strings may also have no value, i.e.  /?flag . You may bind a query parameter to a boolean that will be true if the bound key is present in the query string:  @ httpGet  Future Response   getThing ( @ HTTPQuery ( flag )   bool   flag )   async   { \n   // xs = true  }", 
            "title": "Responder Methods and Parameter Binding"
        }, 
        {
            "location": "/http/http_controller/#binding-http-request-bodies", 
            "text": "You may also bind an HTTP request body to an object with  @HTTPBody  metadata:  @ httpPost  Future Response   createUser ( @ HTTPBody ()   User   user )   async   { \n   var   query   =   new   Query User () \n     .. values   =   user ; \n   var   insertedUser   =   await   query . insert (); \n   return   new   Response . ok ( insertedUser );  }   Body binding is available for both properties and responder method parameters, just like query and header bindings.  An type must implement  HTTPSerializable  to be bound to a request body. This interface requires that the method  readFromMap  be implemented:  class   Person   implements   HTTPSerializable   { \n   String   name ; \n   String   email ; \n\n   @ override \n   void   readFromMap ( Map String ,   dynamic   requestBody )   { \n     name   =   requestBody [ name ]; \n     email   =   requestBody [ email ]; \n   } \n\n   @ override \n   Map String ,   dynamic   asMap ()   { \n     return   { \n       name :   name , \n       email :   email \n     }; \n   }  }  class   PersonController   extends   HTTPController   { \n   @ httpPost \n   Future Response   createPerson ( @ HTTPBody ()   Person   p )   { \n     // p.name and p.email are read from body when body is { name :  ... ,  email :  ... } \n   }  }   You may also bind a  List HTTPSerializable :  class   PersonController   extends   HTTPController   { \n   @ httpPost \n   Future Response   createPerson ( @ HTTPBody ()   List Person   people )   { \n     // When body is [{ name :  ... ,  email :  ... }] \n   }  }   The request body is decoded based on its content type prior to binding it to an  HTTPSerializable . Thus, for data like  application/json , the bound body object is read from a  Map String, dynamic .", 
            "title": "Binding HTTP Request Bodies"
        }, 
        {
            "location": "/http/http_controller/#request-and-response-bodies", 
            "text": "An  HTTPController  can limit the content type of HTTP request bodies it accepts. By default, an  HTTPController  will accept both  application/json  and  application/x-www-form-urlencoded  request bodies for its  POST  and  PUT  methods. This can be modified by setting the  acceptedContentTypes  property in the constructor.  class   UserController   extends   HTTPController   { \n   UserController ()   { \n     acceptedContentTypes   =   [ ContentType . JSON ,   ContentType . XML ]; \n   }  }   If a request is made with a content type other than the accepted content types, the controller automatically responds with a 415 Unsupported Media Type response.  The body of an HTTP request is decoded if the content type is accepted and there exists a responder method to handle the request. This means two things. First, the body is not decoded if the request is going to be discarded because no responder method was found.  Second, methods on  HTTPRequestBody  have two flavors: those that return the contents as a  Future  or those that return the already decoded body. Responder methods can access the already decoded body without awaiting on the  Future -flavored variants of  HTTPRequestBody :  @ httpPost  Future Response   createThing ()   async   { \n   // do this: \n   var   bodyMap   =   request . body . asMap (); \n\n   // no need to do this: \n   var   bodyMap   =   await   request . body . decodeAsMap (); \n\n   return   ...;  }   An  HTTPController  can also have a default content type for its  response  bodies. By default, this is  application/json  - any response body returned as JSON. This default can be changed by changing  responseContentType  in the constructor:  class   UserController   extends   HTTPController   { \n   UserController ()   { \n     responseContentType   =   ContentType . XML ; \n   }  }   The  responseContentType  is the  default  response content type. An individual  Response  may set its own  contentType , which takes precedence over the  responseContentType . For example, the following controller returns JSON by default, but if the request specifically asks for XML, that's what it will return:  class   UserController   extends   HTTPController   { \n   UserController ()   { \n     responseContentType   =   ContentType . JSON ; \n   } \n\n   @ httpGet \n   Future Response   getUserByID ( @ HTTPPath ( id )   int   id )   async   { \n     var   response   =   new   Response . ok (...); \n\n     if   ( request . headers . value ( HttpHeaders . ACCEPT ). startsWith ( application/xml ))   { \n       response . contentType   =   ContentType . XML ; \n     } \n\n     return   response ; \n   }  }", 
            "title": "Request and Response Bodies"
        }, 
        {
            "location": "/http/http_controller/#more-specialized-httpcontrollers", 
            "text": "Because many  HTTPController  subclasses will execute  queries , there are helpful  HTTPController  subclasses for reducing boilerplate code.  A  QueryController T  builds a  Query T  based on the incoming request. If the request has a body, this  Query T 's  values  property is read from that body. If the request has a path variable, the  Query T  assigns a matcher to the primary key value of its  where . For example, in a normal  HTTPController  that responds to a PUT request, you might write the following:  @ httpPut  Future Response   updateUser ( @ HTTPPath ( id )   int   id )   async   { \n   var   query   =   new   Query User () \n     .. where . id   =   whereEqualTo ( id ) \n     .. values   =   ( new   User ().. readFromMap ( request . body . asMap ()); \n\n   return   new   Response . ok ( await   query . updateOne ());  }   A  QueryController T  builds this query before a responder method is invoked, storing it in the inherited  query  property. A  ManagedObject T  subclass is the type argument to  QueryController T .  class   UserController   extends   QueryController User   { \n   Future Response   updateUser ( @ HTTPPath ( id )   int   id )   async   { \n     // query already exists and is identical to the snippet above \n     var   result   =   await   query . updateOne (); \n     return   new   Response . ok ( result ); \n   }  }   A  ManagedObjectController T  is significantly more powerful; you don't even need to subclass it. It does all the things a CRUD endpoint does without any code. Here's an example usage:  router \n   . route ( /users/[:id] ) \n   . generate (()   =   new   ManagedObjectController User ());   This controller has the following behavior:     Request  Action      POST /users  Inserts a user into the database with values from the request body    GET /users  Fetches all users in the database    GET /users/:id  Fetches a single user by id    DELETE /users/:id  Deletes a single user by id    PUT /users/:id  Updated a single user by id, using values from the request body     The objects returned from getting the collection - e.g,  GET /users  - can be modified with query parameters. For example, the following request will return the users sorted by their name in ascending order:  GET /users?sortBy=name,asc   The results can be paged (see  Paging in Advanced Queries ) with query parameters  offset ,  count ,  pageBy ,  pageAfter  and  pagePrior .  A  ManagedObjectController T  can also be subclassed. A subclass allows for callbacks to be overridden to adjust the query before execution, or the results before sending the respond. Each operation - fetch, update, delete, etc. - has a pair of methods to do this. For example, the following subclass alters the query and results before any update via  PUT :  class   UserController   extends   ManagedObjectController User   { \n   Future Query User   willUpdateObjectWithQuery ( \n       Query User   query )   async   { \n     query . values . lastUpdatedAt   =   new   DateTime . now (). toUtc (); \n     return   query ; \n   } \n\n   Future Response   didUpdateObject ( User   object )   async   { \n     object . removePropertyFromBackingMap ( private ); \n     return   new   Response . ok ( object ); \n   }  }   See also  validations , which are powerful when combined with  ManagedObjectController T .", 
            "title": "More Specialized HTTPControllers"
        }, 
        {
            "location": "/http/http_controller/#accessing-the-request", 
            "text": "Any value from the request itself can be accessed through the  request  property of a controller.  This also means that an  HTTPController  instance cannot be reused to handle multiple requests; if it awaited on an operation, a new request could be assigned to the  request  property. Therefore, all  HTTPController s must be added to a request processing pipeline with  generate . If you add a controller with  pipe , an exception will be thrown immediately at startup.", 
            "title": "Accessing the Request"
        }, 
        {
            "location": "/http/configure/", 
            "text": "Configuring an Aqueduct Application\n\n\nThis guide covers configuring an Aqueduct application.\n\n\nConfiguration Files\n\n\nAqueduct applications use YAML configuration files to provide environment-specific values like database connection information. Use separate configuration files for testing and different deployment environments.\n\n\nThe path of a configuration file is available at runtime via \nApplicationConfiguration.configurationFilePath\n and is read in a \nRequestSink\n constructor (and sometimes \nRequestSink.initializeApplication\n).\n\n\nclass\n \nTodoAppSink\n \nextends\n \nRequestSink\n \n{\n\n  \nTodoAppSink\n(\nApplicationConfiguration\n \noptions\n)\n \n:\n \nsuper\n(\noptions\n)\n \n{\n\n    \nvar\n \nconfigFilePath\n \n=\n \noptions\n.\nconfigurationFilePath\n;\n\n    \nvar\n \nconfig\n \n=\n \nnew\n \nTodoConfiguration\n(\nconfigFilePath\n);\n\n\n  \n}\n\n\n}\n\n\n\n\n\n\nThe default value is \nconfig.yaml\n.\n\n\nThe best practice for reading a configuration file is to subclass \nConfigurationItem\n. A \nConfigurationItem\n declares a property for each key in a configuration file. For example, see the following \nConfigurationItem\n subclass:\n\n\nclass\n \nTodoConfiguration\n \nextends\n \nConfigurationItem\n \n{\n\n  \nTodoConfiguration\n(\nString\n \nfileName\n)\n \n:\n \nsuper\n.\nfromFile\n(\nfileName\n);\n\n\n  \nDatabaseConnectionConfiguration\n \ndatabase\n;\n\n  \nString\n \napiBaseURL\n;\n\n\n  \n@\noptionalConfiguration\n\n  \nint\n \nidentifier\n;\n\n\n}\n\n\n\n\n\n\nThis would read a YAML file like this:\n\n\ndatabase\n:\n\n  \nusername\n:\n \nfred\n\n  \npassword\n:\n \nfredspassword\n\n  \nhost\n:\n \ndb\n.\nmyapp\n.\ncom\n\n  \nport\n:\n \n5432\n\n  \ndatabaseName\n:\n \nfredsdb\n\n\napiBaseURL\n:\n \n/\napi\n\n\nidentifier\n:\n \n2\n\n\n\n\n\n\nIf required properties are omitted from the YAML file being read, application startup will fail and throw an informative error.\n\n\nYou may use \nConfigurationItem\ns to read values from environment variables. In \nconfig.yaml\n, use a \n$\n-prefixed environment variable name instead of a value:\n\n\ndatabase: $DATABASE_CONNECTION_URL\napiBaseURL: /api\n\n\n\n\n\nIf the environment variable \nDATABASE_CONNECTION_URL\n's value were \n\"postgres://user:password@localhost:5432/test\"\n, the value of \nTodoConfigurationItem.database\n will be that string at runtime. (Note that \nDatabaseConnectionConfiguration\n may either a YAML object for each connection attribute, or a database connection string.)\n\n\nThe \nsafe_config package\n has instructions for more additional usages.\n\n\nConfiguration Conventions and Deployment Options\n\n\nAqueduct uses two configuration files for a project: \nconfig.yaml\n and \nconfig.src.yaml\n. The latter is the \nconfiguration source file\n. The configuration source file declares key-value pairs that will be used when running the application tests. Deployed instances use \nconfig.yaml\n.\n\n\nThis pattern is used for two reasons:\n\n\n\n\nIt is the template for the \nconfig.yaml\n that will be read on deployed applications, providing documentation for your application's configuration.\n\n\nIt has the configuration values used during testing to inject mock dependencies.\n\n\n\n\nFor example, a production API instance might have the following \nconfig.yaml\n file with connection info for a production database:\n\n\ndatabase\n:\n \npostgres\n://\napp_user\n:\n$\n%\n4\njlkn\n#\nan\n*\n@\nmOZkea2\n@\nsomedns\n.\nname\n.\ncom\n:\n5432\n/\nproduction_db\n\n\n\n\n\n\nWhereas \nconfig.src.yaml\n would have connection info for a local, test database:\n\n\ndatabase\n:\n \npostgres\n://\ntest\n:\ntest\n@\nlocalhost\n:\n5432\n/\ntemporary_db\n\n\n\n\n\n\nThe source configuration file should be checked into version control. Whether or not \nconfig.yaml\n is checked in depends on how you are deploying your code. If you are using environment variables to control application configuration, you should check \nconfig.yaml\n into source control and provide \n$\n-prefixed environment variable values. If you are using managing configuration files on each deployed instance, do not check \nconfig.yaml\n into source control because it'll be a different file for each instance.\n\n\nIt can sometimes makes sense to have a \nlocal.yaml\n with values for running the application locally, e.g. when doing client testing. Use \n--config-path\n with \naqueduct serve\n to use a non-default name.\n\n\nPreventing Resource Leaks\n\n\nWhen an Aqueduct application starts, the application and its \nRequestSink\ns will likely create services that they use to respond to requests. In order for application tests to complete successfully, these services must be \"closed\" when the application stops. For built-in services, like \nPostgreSQLPersistentStore\n, this happens automatically when \nApplication.stop()\n is invoked.\n\n\nA \nServiceRegistry\n automatically stops registered services. Registration looks like this:\n\n\nvar\n \nconnection\n \n=\n \nnew\n \nConnectionOfSomeKind\n();\n\n\nawait\n \nconnection\n.\nopen\n();\n\n\nServiceRegistry\n.\ndefaultInstance\n\n  \n.\nregister\nConnectionOfSomeKind\n(\nconnection\n,\n \n(\nc\n)\n \n=\n \nc\n.\nclose\n());\n\n\n\n\n\n\nThis method takes the service to be closed and a closure that closes it. The service is passed as an argument to the closure. If the closure returns a \nFuture\n, \nServiceRegistry.close\n will not complete until the \nFuture\n completes. \nServiceRegistry.defaultInstance\n is closed in \nApplication.stop()\n, any registries created by the programmer must be closed manually.\n\n\nThe return type of \nServiceRegistry.register\n is the object being registered. This makes registration syntax a bit more palatable:\n\n\nvar\n \nconnection\n \n=\n \nServiceRegistry\n.\ndefaultInstance\n\n  \n.\nregister\nConnectionOfSomeKind\n(\n\n    \nnew\n \nConnectionOfSomeKind\n(),\n \n(\nc\n)\n \n=\n \nc\n.\nclose\n());\n\n\n\nawait\n \nconnection\n.\nopen\n();\n  \n\n\n\n\n\nConfiguring CORS Headers\n\n\nAll request controllers have built-in behavior for handling CORS requests from a browser. When a preflight request is received from a browser (an OPTIONS request with Access-Control-Request-Method header and Origin headers), the response is created by evaluating the policy of the \nRequestController\n that will respond to the real request.\n\n\nIn practice, this means that the policy of the last controller in a channel is used. For example, the policy of \nFooController\n is generates the preflight response:\n\n\nrouter\n\n  \n.\nroute\n(\n/foo\n)\n\n  \n.\npipe\n(\nnew\n \nAuthorizer\n(...))\n\n  \n.\ngenerate\n(()\n \n=\n \nnew\n \nFooController\n());\n\n\n\n\n\n\nEvery \nRequestController\n has a \npolicy\n property (a \nCORSPolicy\n instance). The \npolicy\n has properties for configuring CORS options for that particular endpoint. By having a \npolicy\n, every \nRequestController\n automatically implements logic to respond to preflight requests without any additional code.\n\n\nPolicies can be set at the controller level or at the application level. The static property \nCORSPolicy.defaultPolicy\n can be modified at initialization time to set the CORS options for every controller.\n\n\nclass\n \nMyRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nMyRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n    \nCORSPolicy\n.\ndefaultPolicy\n.\nallowedOrigins\n \n=\n \n[\nhttp://mywebsite.com/\n];\n\n  \n}\n\n\n}\n\n\n\n\n\n\nThe default policy is very permissive: POST, PUT, DELETE and GET are allowed methods. All origins are valid (*).\n\n\nEach individual controller can override or replace the default policy by modifying its own \npolicy\n in its constructor.\n\n\nclass\n \nMyHTTPController\n \nextends\n \nHTTPController\n \n{\n\n  \nMyHTTPController\n()\n \n{\n\n    \npolicy\n.\nallowedMethods\n \n=\n \n[\nPOST\n];\n\n  \n}\n\n\n}\n\n\n\n\n\n\nConfiguring HTTPS\n\n\nBy default, an Aqueduct application does not use HTTPS. In many cases, an Aqueduct application sits behind an SSL-enabled load balancer or some other proxy. The traffic from the load balancer is sent to the Aqueduct application unencrypted over HTTP.\n\n\nHowever, Aqueduct may be configured to manage HTTPS connections itself. By passing the value private key and SSL certificate paths as options to \n--ssl-key-path\n \nand\n \n--ssl-certificate-path\n in \naqueduct serve\n, an Aqueduct application will configure itself to only allow HTTPS connections.\n\n\naqueduct serve --ssl-key-path server.key.pem --ssl-certificate-path server.cert.pem\n\n\n\n\n\nBoth the key and certificate file must be unencrypted PEM files, and both must be provided to this command. These files are typically issued by a \"Certificate Authority\", such as \nletsencrypt.org\n.\n\n\nWhen an application is started with these options, the \ncertificateFilePath\n and \nkeyFilePath\n are set on the \nApplicationConfiguration\n your application is being run with. (If you are not using \naqueduct serve\n, you can set these values directly when instantiating \nApplicationConfiguration\n.)\n\n\nFor more granular control over setting up an HTTPS server, you may override \nsecurityContext\n in \nRequestSink\n. By default, this property will create a \nSecurityContext\n from the \ncertificateFilePath\n and \nkeyFilePath\n in the sink's \nconfiguration\n. A \nSecurityContext\n allows for password-encrypted credential files, configuring client certificates and other less used HTTPS schemes.\n\n\nclass\n \nMyRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \n@\noverride\n\n  \nSecurityContext\n \nget\n \nsecurityContext\n \n{\n\n    \nreturn\n \nnew\n \nSecurityContext\n()\n\n      \n..\nusePrivateKey\n(\nserver.key\n,\n \npassword:\n \n1234\n)\n\n      \n..\nuseCertificateChain\n(\nserver.crt\n,\n \npassword:\n \n1234\n);\n\n  \n}\n\n\n}", 
            "title": "Configuration"
        }, 
        {
            "location": "/http/configure/#configuring-an-aqueduct-application", 
            "text": "This guide covers configuring an Aqueduct application.", 
            "title": "Configuring an Aqueduct Application"
        }, 
        {
            "location": "/http/configure/#configuration-files", 
            "text": "Aqueduct applications use YAML configuration files to provide environment-specific values like database connection information. Use separate configuration files for testing and different deployment environments.  The path of a configuration file is available at runtime via  ApplicationConfiguration.configurationFilePath  and is read in a  RequestSink  constructor (and sometimes  RequestSink.initializeApplication ).  class   TodoAppSink   extends   RequestSink   { \n   TodoAppSink ( ApplicationConfiguration   options )   :   super ( options )   { \n     var   configFilePath   =   options . configurationFilePath ; \n     var   config   =   new   TodoConfiguration ( configFilePath ); \n\n   }  }   The default value is  config.yaml .  The best practice for reading a configuration file is to subclass  ConfigurationItem . A  ConfigurationItem  declares a property for each key in a configuration file. For example, see the following  ConfigurationItem  subclass:  class   TodoConfiguration   extends   ConfigurationItem   { \n   TodoConfiguration ( String   fileName )   :   super . fromFile ( fileName ); \n\n   DatabaseConnectionConfiguration   database ; \n   String   apiBaseURL ; \n\n   @ optionalConfiguration \n   int   identifier ;  }   This would read a YAML file like this:  database : \n   username :   fred \n   password :   fredspassword \n   host :   db . myapp . com \n   port :   5432 \n   databaseName :   fredsdb  apiBaseURL :   / api  identifier :   2   If required properties are omitted from the YAML file being read, application startup will fail and throw an informative error.  You may use  ConfigurationItem s to read values from environment variables. In  config.yaml , use a  $ -prefixed environment variable name instead of a value:  database: $DATABASE_CONNECTION_URL\napiBaseURL: /api  If the environment variable  DATABASE_CONNECTION_URL 's value were  \"postgres://user:password@localhost:5432/test\" , the value of  TodoConfigurationItem.database  will be that string at runtime. (Note that  DatabaseConnectionConfiguration  may either a YAML object for each connection attribute, or a database connection string.)  The  safe_config package  has instructions for more additional usages.", 
            "title": "Configuration Files"
        }, 
        {
            "location": "/http/configure/#configuration-conventions-and-deployment-options", 
            "text": "Aqueduct uses two configuration files for a project:  config.yaml  and  config.src.yaml . The latter is the  configuration source file . The configuration source file declares key-value pairs that will be used when running the application tests. Deployed instances use  config.yaml .  This pattern is used for two reasons:   It is the template for the  config.yaml  that will be read on deployed applications, providing documentation for your application's configuration.  It has the configuration values used during testing to inject mock dependencies.   For example, a production API instance might have the following  config.yaml  file with connection info for a production database:  database :   postgres :// app_user : $ % 4 jlkn # an * @ mOZkea2 @ somedns . name . com : 5432 / production_db   Whereas  config.src.yaml  would have connection info for a local, test database:  database :   postgres :// test : test @ localhost : 5432 / temporary_db   The source configuration file should be checked into version control. Whether or not  config.yaml  is checked in depends on how you are deploying your code. If you are using environment variables to control application configuration, you should check  config.yaml  into source control and provide  $ -prefixed environment variable values. If you are using managing configuration files on each deployed instance, do not check  config.yaml  into source control because it'll be a different file for each instance.  It can sometimes makes sense to have a  local.yaml  with values for running the application locally, e.g. when doing client testing. Use  --config-path  with  aqueduct serve  to use a non-default name.", 
            "title": "Configuration Conventions and Deployment Options"
        }, 
        {
            "location": "/http/configure/#preventing-resource-leaks", 
            "text": "When an Aqueduct application starts, the application and its  RequestSink s will likely create services that they use to respond to requests. In order for application tests to complete successfully, these services must be \"closed\" when the application stops. For built-in services, like  PostgreSQLPersistentStore , this happens automatically when  Application.stop()  is invoked.  A  ServiceRegistry  automatically stops registered services. Registration looks like this:  var   connection   =   new   ConnectionOfSomeKind ();  await   connection . open ();  ServiceRegistry . defaultInstance \n   . register ConnectionOfSomeKind ( connection ,   ( c )   =   c . close ());   This method takes the service to be closed and a closure that closes it. The service is passed as an argument to the closure. If the closure returns a  Future ,  ServiceRegistry.close  will not complete until the  Future  completes.  ServiceRegistry.defaultInstance  is closed in  Application.stop() , any registries created by the programmer must be closed manually.  The return type of  ServiceRegistry.register  is the object being registered. This makes registration syntax a bit more palatable:  var   connection   =   ServiceRegistry . defaultInstance \n   . register ConnectionOfSomeKind ( \n     new   ConnectionOfSomeKind (),   ( c )   =   c . close ());  await   connection . open ();", 
            "title": "Preventing Resource Leaks"
        }, 
        {
            "location": "/http/configure/#configuring-cors-headers", 
            "text": "All request controllers have built-in behavior for handling CORS requests from a browser. When a preflight request is received from a browser (an OPTIONS request with Access-Control-Request-Method header and Origin headers), the response is created by evaluating the policy of the  RequestController  that will respond to the real request.  In practice, this means that the policy of the last controller in a channel is used. For example, the policy of  FooController  is generates the preflight response:  router \n   . route ( /foo ) \n   . pipe ( new   Authorizer (...)) \n   . generate (()   =   new   FooController ());   Every  RequestController  has a  policy  property (a  CORSPolicy  instance). The  policy  has properties for configuring CORS options for that particular endpoint. By having a  policy , every  RequestController  automatically implements logic to respond to preflight requests without any additional code.  Policies can be set at the controller level or at the application level. The static property  CORSPolicy.defaultPolicy  can be modified at initialization time to set the CORS options for every controller.  class   MyRequestSink   extends   RequestSink   { \n   MyRequestSink ( ApplicationConfiguration   config )   :   super ( config )   { \n     CORSPolicy . defaultPolicy . allowedOrigins   =   [ http://mywebsite.com/ ]; \n   }  }   The default policy is very permissive: POST, PUT, DELETE and GET are allowed methods. All origins are valid (*).  Each individual controller can override or replace the default policy by modifying its own  policy  in its constructor.  class   MyHTTPController   extends   HTTPController   { \n   MyHTTPController ()   { \n     policy . allowedMethods   =   [ POST ]; \n   }  }", 
            "title": "Configuring CORS Headers"
        }, 
        {
            "location": "/http/configure/#configuring-https", 
            "text": "By default, an Aqueduct application does not use HTTPS. In many cases, an Aqueduct application sits behind an SSL-enabled load balancer or some other proxy. The traffic from the load balancer is sent to the Aqueduct application unencrypted over HTTP.  However, Aqueduct may be configured to manage HTTPS connections itself. By passing the value private key and SSL certificate paths as options to  --ssl-key-path   and   --ssl-certificate-path  in  aqueduct serve , an Aqueduct application will configure itself to only allow HTTPS connections.  aqueduct serve --ssl-key-path server.key.pem --ssl-certificate-path server.cert.pem  Both the key and certificate file must be unencrypted PEM files, and both must be provided to this command. These files are typically issued by a \"Certificate Authority\", such as  letsencrypt.org .  When an application is started with these options, the  certificateFilePath  and  keyFilePath  are set on the  ApplicationConfiguration  your application is being run with. (If you are not using  aqueduct serve , you can set these values directly when instantiating  ApplicationConfiguration .)  For more granular control over setting up an HTTPS server, you may override  securityContext  in  RequestSink . By default, this property will create a  SecurityContext  from the  certificateFilePath  and  keyFilePath  in the sink's  configuration . A  SecurityContext  allows for password-encrypted credential files, configuring client certificates and other less used HTTPS schemes.  class   MyRequestSink   extends   RequestSink   { \n   @ override \n   SecurityContext   get   securityContext   { \n     return   new   SecurityContext () \n       .. usePrivateKey ( server.key ,   password:   1234 ) \n       .. useCertificateChain ( server.crt ,   password:   1234 ); \n   }  }", 
            "title": "Configuring HTTPS"
        }, 
        {
            "location": "/http/serving_files/", 
            "text": "Serving Files and Caching\n\n\nAqueduct can serve files by returning the contents of a file as an HTTP response body.\n\n\nHTTPFileController\n\n\nInstances of \nHTTPFileController\n serve a directory from the filesystem through an HTTP interface. Any route that channels requests to an \nHTTPFileController\n \nmust\n contain a \n*\n match-all token.\n\n\n@\noverride\n\n\nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n  \nrouter\n.\nroute\n(\n/files/*\n).\npipe\n(\nnew\n \nHTTPFileController\n(\npublic/\n));\n\n\n}\n\n\n\n\n\n\nThe argument to \nHTTPFileController\n is the directory on the filesystem in which request paths will be resolved against. In the above example, an HTTP request with the path \n/files/image.jpg\n would return the contents of the file \npublic/image.jpg\n.\n\n\nNote that \npublic/\n does not have a leading slash - therefore, the directory \npublic\n must be relative to the directory that the Aqueduct application was served from. In practice, this means you might have a directory structure like:\n\n\nproject/\n  pubspec.yaml  \n  lib/\n    project_sink.dart\n    ...\n  test/\n    ...\n  public/\n    image.jpg\n\n\n\n\n\nAdding a leading slash to the directory served by \nHTTPFileController\n will resolve it relative to the filesystem root.\n\n\nIf the requested path was a directory, the filename \nindex.html\n will be appended to the path when searching for a file to return.\n\n\nIf a file does not exist, an \nHTTPFileController\n returns a 404 Not Found response.\n\n\nContent-Type of Files\n\n\nAn \nHTTPFileController\n will set the content-type of the HTTP response based on the served files path extension. By default, it recognizes many common extensions like \n.html\n, \n.css\n, \n.jpg\n, \n.js\n. You may add content-types for extensions to an instance:\n\n\nvar\n \ncontroller\n \n=\n \nnew\n \nHTTPFileController\n(\npublic/\n)\n\n  \n..\nsetContentTypeForExtension\n(\nxml\n,\n \nnew\n \nContentType\n(\napplication\n,\n \nxml\n));\n\n\n\n\n\n\nIf there is no entry for an extension of a file being served, the content-type defaults to \napplication/octet-stream\n. An \nHTTPFileController\n will never invoke any encoders from \nHTTPCodecRepository\n, but it will GZIP data if the repository allows compression for the content-type of the file (see \nHTTPCodecRepository.add\n and \nHTTPCodecRepository.setAllowsCompression\n).\n\n\nCaching\n\n\nAn \nHTTPFileController\n always sets the the Last-Modified header of the response to the last modified date according to the filesystem. If a request sends an If-Modified-Since header and the file has not been modified since that date, a 304 Not Modified response is sent with the appropriate headers.\n\n\nYou may provide Cache-Control headers depending on the path of the file being served. Here's an example that adds \nCache-Control: public, max-age=31536000\n\n\nvar\n \npolicy\n \n=\n \nnew\n \nHTTPCachePolicy\n(\nexpirationFromNow:\n \nnew\n \nDuration\n(\ndays:\n \n365\n));\n\n\nvar\n \ncontroller\n \n=\n \nnew\n \nHTTPFileController\n(\npublic/\n)\n\n  \n..\naddCachePolicy\n(\npolicy\n,\n \n(\npath\n)\n \n=\n \npath\n.\nendsWith\n(\n.css\n));\n\n\n\n\n\n\nFile Serving and Caching Outside of HTTPFileController\n\n\nA file can be served by any controller by setting the body object of a \nResponse\n with its contents:\n\n\nvar\n \nfile\n \n=\n \nnew\n \nFile\n(\nindex.html\n);\n\n\n\n// By loading contents into memory first...\n\n\nvar\n \nresponse\n \n=\n \nnew\n \nResponse\n.\nok\n(\nfile\n.\nreadAsStringSync\n())\n\n  \n..\ncontentType\n \n=\n \nnew\n \nContentType\n(\napplication\n,\n \nhtml\n);\n\n\n\n// Or by streaming the contents from disk\n\n\nvar\n \nresponse\n \n=\n \nnew\n \nResponse\n.\nok\n(\nfile\n.\nopenRead\n())\n\n  \n..\nencodeBody\n \n=\n \nfalse\n\n  \n..\ncontentType\n \n=\n \nnew\n \nContentType\n(\napplication\n,\n \nhtml\n);\n\n\n\n\n\n\nIt is important to understand the how Aqueduct \nuses content-types to manipulate response bodies\n to serve file contents.\n\n\nYou may set the \nHTTPCachePolicy\n of any \nResponse\n. Note that \nHTTPCachePolicy\n only modifies the Cache-Control header of a response - headers like Last-Modified and ETag are not added.\n\n\nvar\n \nresponse\n \n=\n \nnew\n \nResponse\n.\nok\n(\ncontents\n)\n\n  \n..\ncachePolicy\n \n=\n \nnew\n \nHTTPCachePolicy\n();", 
            "title": "Serving Files and Caching"
        }, 
        {
            "location": "/http/serving_files/#serving-files-and-caching", 
            "text": "Aqueduct can serve files by returning the contents of a file as an HTTP response body.", 
            "title": "Serving Files and Caching"
        }, 
        {
            "location": "/http/serving_files/#httpfilecontroller", 
            "text": "Instances of  HTTPFileController  serve a directory from the filesystem through an HTTP interface. Any route that channels requests to an  HTTPFileController   must  contain a  *  match-all token.  @ override  void   setupRouter ( Router   router )   { \n   router . route ( /files/* ). pipe ( new   HTTPFileController ( public/ ));  }   The argument to  HTTPFileController  is the directory on the filesystem in which request paths will be resolved against. In the above example, an HTTP request with the path  /files/image.jpg  would return the contents of the file  public/image.jpg .  Note that  public/  does not have a leading slash - therefore, the directory  public  must be relative to the directory that the Aqueduct application was served from. In practice, this means you might have a directory structure like:  project/\n  pubspec.yaml  \n  lib/\n    project_sink.dart\n    ...\n  test/\n    ...\n  public/\n    image.jpg  Adding a leading slash to the directory served by  HTTPFileController  will resolve it relative to the filesystem root.  If the requested path was a directory, the filename  index.html  will be appended to the path when searching for a file to return.  If a file does not exist, an  HTTPFileController  returns a 404 Not Found response.", 
            "title": "HTTPFileController"
        }, 
        {
            "location": "/http/serving_files/#content-type-of-files", 
            "text": "An  HTTPFileController  will set the content-type of the HTTP response based on the served files path extension. By default, it recognizes many common extensions like  .html ,  .css ,  .jpg ,  .js . You may add content-types for extensions to an instance:  var   controller   =   new   HTTPFileController ( public/ ) \n   .. setContentTypeForExtension ( xml ,   new   ContentType ( application ,   xml ));   If there is no entry for an extension of a file being served, the content-type defaults to  application/octet-stream . An  HTTPFileController  will never invoke any encoders from  HTTPCodecRepository , but it will GZIP data if the repository allows compression for the content-type of the file (see  HTTPCodecRepository.add  and  HTTPCodecRepository.setAllowsCompression ).", 
            "title": "Content-Type of Files"
        }, 
        {
            "location": "/http/serving_files/#caching", 
            "text": "An  HTTPFileController  always sets the the Last-Modified header of the response to the last modified date according to the filesystem. If a request sends an If-Modified-Since header and the file has not been modified since that date, a 304 Not Modified response is sent with the appropriate headers.  You may provide Cache-Control headers depending on the path of the file being served. Here's an example that adds  Cache-Control: public, max-age=31536000  var   policy   =   new   HTTPCachePolicy ( expirationFromNow:   new   Duration ( days:   365 ));  var   controller   =   new   HTTPFileController ( public/ ) \n   .. addCachePolicy ( policy ,   ( path )   =   path . endsWith ( .css ));", 
            "title": "Caching"
        }, 
        {
            "location": "/http/serving_files/#file-serving-and-caching-outside-of-httpfilecontroller", 
            "text": "A file can be served by any controller by setting the body object of a  Response  with its contents:  var   file   =   new   File ( index.html );  // By loading contents into memory first...  var   response   =   new   Response . ok ( file . readAsStringSync ()) \n   .. contentType   =   new   ContentType ( application ,   html );  // Or by streaming the contents from disk  var   response   =   new   Response . ok ( file . openRead ()) \n   .. encodeBody   =   false \n   .. contentType   =   new   ContentType ( application ,   html );   It is important to understand the how Aqueduct  uses content-types to manipulate response bodies  to serve file contents.  You may set the  HTTPCachePolicy  of any  Response . Note that  HTTPCachePolicy  only modifies the Cache-Control header of a response - headers like Last-Modified and ETag are not added.  var   response   =   new   Response . ok ( contents ) \n   .. cachePolicy   =   new   HTTPCachePolicy ();", 
            "title": "File Serving and Caching Outside of HTTPFileController"
        }, 
        {
            "location": "/http/websockets/", 
            "text": "Using Websockets in Aqueduct\n\n\nA standard HTTP request will yield an HTTP response from a web server. In order for the server to send data to a client, the client must have sent a request for that data. A \nwebsocket\n is a special type of HTTP request that stays open, and both the server and client can send data to one another whenever they please.\n\n\nFor example, a chat application might use websockets to send messages to everyone in a chatroom. In this scenario, the chat client application opens a websocket connection to the server application. When the user types a message, their chat client sends that message on its websocket. The payload might be JSON data that looks like this:\n\n\n{\n\n  \naction\n:\n \nsend_message\n,\n\n  \nroom\n:\n \ngeneral\n,\n\n  \ntext\n:\n \nHi everyone\n\n\n}\n\n\n\n\n\n\nThe server will receive this data, then turn around and send a modified version to \nevery\n websocket connection it has. That data might look like this:\n\n\n{\n\n  \naction\n:\n \nreceive_message\n,\n\n  \nroom\n:\n \ngeneral\n,\n\n  \nfrom\n:\n \nBob\n,\n\n  \ntext\n:\n \nHi everyone\n\n\n}\n\n\n\n\n\n\nEvery connected user will receive this data and draw \nBob: Hi everyone\n to the screen.\n\n\nNote that there's nothing about websockets that says you have to use JSON data - you can use any data format you like.\n\n\nUpgrading an HTTP Request to a WebSocket\n\n\nIn Aqueduct, websockets are handled by Dart's standard library \nWebSocket\n type. Here's an example:\n\n\nrouter\n\n  \n.\nroute\n(\n/connect\n)\n\n  \n.\nlisten\n((\nrequest\n)\n \nasync\n \n{\n\n    \nvar\n \nsocket\n \n=\n \nawait\n \nWebSocketTransformer\n.\nupgrade\n(\nrequest\n.\ninnerRequest\n);\n\n    \nsocket\n.\nlisten\n(\nlistener\n);\n\n\n    \nreturn\n \nnull\n;\n\n  \n});\n\n\n\n\n\n\nIt's important that a request that is upgraded to a websocket is removed from the request channel by returning null from the request controller. (See the section on \nAqueduct and dart:io\n \nin this guide\n for more details.)\n\n\nA client application can connect to the URL \nws://localhost:8081/connect\n. A Dart application would make this connection like so:\n\n\nvar\n \nsocket\n \n=\n \nawait\n \nWebSocket\n.\nconnect\n(\nws://localhost:8081/connect\n);\n\n\nsocket\n.\nlisten\n(...);\n\n\n\n\n\n\nBi-directional Communication\n\n\nIn the simple example above, the server only listens for data from the client. For data to be sent to the client, a reference must be kept to the \nWebSocket\n so that data can be added to it. How an Aqueduct application manages its websocket connections depends greatly on the behavior of the application, the number of isolates the application is running on and the infrastructure of the system as a whole.\n\n\nA simple application might keep track of websocket connections in a \nMap\n, where the key is a user identifier acquired from the authorization of the request:\n\n\nrouter\n\n  \n.\nroute\n(\n/connect\n)\n\n  \n.\npipe\n(\nnew\n \nAuthorizer\n(\nauthServer\n));\n\n  \n.\nlisten\n((\nrequest\n)\n \nasync\n \n{\n\n    \nvar\n \nuserID\n \n=\n \nrequest\n.\nauthorization\n.\nresourceOwnerIdentifier\n;\n\n    \nvar\n \nsocket\n \n=\n \nawait\n \nWebSocketTransformer\n.\nupgrade\n(\nrequest\n.\ninnerRequest\n);\n\n    \nsocket\n.\nlisten\n((\nevent\n)\n \n=\n \nhandleEvent\n(\nevent\n,\n \nfromUserID:\n \nuserID\n));\n\n\n    \nconnections\n[\nuserID\n]\n \n=\n \nsocket\n;\n\n\n    \nreturn\n \nnull\n;\n\n  \n});\n\n\n\n\n\n\nIf we continue with the 'chat application' example, the code for \nhandleEvent\n may be something like:\n\n\nvoid\n \nhandleRequest\n(\ndynamic\n \nevent\n,\n \n{\nint\n \nfromUserID\n})\n \n{\n\n  \nvar\n \nincoming\n \n=\n \nJSON\n.\ndecode\n(\nUTF8\n.\ndecode\n(\nevent\n));\n\n  \nvar\n \noutgoing\n \n=\n \nUTF8\n.\nencode\n(\nJSON\n.\nencode\n({\n\n    \ntext\n:\n \nincoming\n[\ntext\n],\n\n    \n...\n\n  \n}));\n\n\n  \nconnections\n.\nkeys\n\n    \n.\nwhere\n((\nuserID\n)\n \n=\n \nuserID\n \n!=\n \nfromUserID\n)\n\n    \n.\nforEach\n((\nuserID\n)\n \n{\n\n      \nvar\n \nconnection\n \n=\n \nconnections\n[\nuserID\n];\n\n      \nconnection\n.\nadd\n(\noutgoing\n);\n\n    \n});\n        \n\n}\n\n\n\n\n\n\nNote that this simple implementation doesn't account for multiple connections from the same user or multi-isolate applications.\n\n\nConsiderations for Multi-Isolate and Multi-Instance Applications\n\n\nBy default, an Aqueduct application runs on multiple isolates. Since each isolate has its own heap, a websocket created on one isolate is not directly accessible by another isolate. In the example above, each isolate would have its own map of connections - therefore, a message is only sent to connections that were opened on the same isolate that the chat message originated from.\n\n\nA simple solution is to only run the application on a single isolate, ensuring that all websockets are on a single isolate and accessible to one another:\n\n\naqueduct\n \nserve\n \n-\nn\n \n1\n\n\n\n\n\n\nFor many applications, this is a fine solution. For others, it may not be.\n\n\nRecall that one of the benefits of Aqueduct's multi-isolate architecture is that code tested on a single instance will scale to multiple instances behind a load balancer. If an Aqueduct application runs correctly on a single, multi-isolate instance, it will will correctly on multiple instances. This (somewhat) enforced structure prevents us from naively keeping track of websocket connections on a single isolate, which would cause issues when we scale out to a multi-instance system.\n\n\nIf you find yourself in a situation where your application is so popular you need multiple servers to efficiently serve requests, you'll have a good idea on how to architect an appropriate solution (or you'll have the money to hire someone that does). In many situations, the REST API and websocket server are separate instances anyhow - they have different lifecycles and deployment behavior. It may make sense to run a websocket server on a single isolate, since you are likely IO-bound instead of CPU bound.\n\n\nIf you still prefer to have a multi-isolate server with websockets, the \nApplicationMessageHub\n will come in handy. When broadcasting messages to connected websockets across the application, you first send the data to each websocket connected to the isolate that is originating the message. Then, the message is added to the \nApplicationMessageHub\n:\n\n\nvoid\n \nonChatMessage\n(\nString\n \nmessage\n)\n \n{\n\n  \nconnectedSockets\n.\nforEach\n((\nsocket\n)\n \n{\n\n    \nsocket\n.\nadd\n(\nmessage\n);\n\n  \n});\n\n\n  \nrequestSink\n.\nmessageHub\n.\nadd\n({\nevent\n:\n \nwebsocket_broadcast\n,\n \nmessage\n:\n \nmessage\n});\n\n\n}\n\n\n\n\n\n\nAnything added to the \nmessageHub\n will be delivered to the listener for every other message hub - i.e., every other isolate will receive this data. The other isolates then send the message to each of their connected websockets:\n\n\nclass\n \nChatSink\n \nextends\n \nRequestSink\n \n{\n\n  \nChatSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n    \nmessageHub\n.\nlisten\n((\nevent\n)\n \n{\n\n      \nif\n \n(\nevent\n \nis\n \nMap\n \n \nevent\n[\nevent\n]\n \n==\n \nwebsocket_broadcast\n)\n \n{\n\n        \nconnectedSockets\n.\nforEach\n((\nsocket\n)\n \n{\n\n          \nsocket\n.\nadd\n(\nevent\n[\nmessage\n]);\n\n        \n});\n\n      \n}\n\n    \n});\n\n  \n}\n\n\n}", 
            "title": "Websockets"
        }, 
        {
            "location": "/http/websockets/#using-websockets-in-aqueduct", 
            "text": "A standard HTTP request will yield an HTTP response from a web server. In order for the server to send data to a client, the client must have sent a request for that data. A  websocket  is a special type of HTTP request that stays open, and both the server and client can send data to one another whenever they please.  For example, a chat application might use websockets to send messages to everyone in a chatroom. In this scenario, the chat client application opens a websocket connection to the server application. When the user types a message, their chat client sends that message on its websocket. The payload might be JSON data that looks like this:  { \n   action :   send_message , \n   room :   general , \n   text :   Hi everyone  }   The server will receive this data, then turn around and send a modified version to  every  websocket connection it has. That data might look like this:  { \n   action :   receive_message , \n   room :   general , \n   from :   Bob , \n   text :   Hi everyone  }   Every connected user will receive this data and draw  Bob: Hi everyone  to the screen.  Note that there's nothing about websockets that says you have to use JSON data - you can use any data format you like.", 
            "title": "Using Websockets in Aqueduct"
        }, 
        {
            "location": "/http/websockets/#upgrading-an-http-request-to-a-websocket", 
            "text": "In Aqueduct, websockets are handled by Dart's standard library  WebSocket  type. Here's an example:  router \n   . route ( /connect ) \n   . listen (( request )   async   { \n     var   socket   =   await   WebSocketTransformer . upgrade ( request . innerRequest ); \n     socket . listen ( listener ); \n\n     return   null ; \n   });   It's important that a request that is upgraded to a websocket is removed from the request channel by returning null from the request controller. (See the section on  Aqueduct and dart:io   in this guide  for more details.)  A client application can connect to the URL  ws://localhost:8081/connect . A Dart application would make this connection like so:  var   socket   =   await   WebSocket . connect ( ws://localhost:8081/connect );  socket . listen (...);", 
            "title": "Upgrading an HTTP Request to a WebSocket"
        }, 
        {
            "location": "/http/websockets/#bi-directional-communication", 
            "text": "In the simple example above, the server only listens for data from the client. For data to be sent to the client, a reference must be kept to the  WebSocket  so that data can be added to it. How an Aqueduct application manages its websocket connections depends greatly on the behavior of the application, the number of isolates the application is running on and the infrastructure of the system as a whole.  A simple application might keep track of websocket connections in a  Map , where the key is a user identifier acquired from the authorization of the request:  router \n   . route ( /connect ) \n   . pipe ( new   Authorizer ( authServer )); \n   . listen (( request )   async   { \n     var   userID   =   request . authorization . resourceOwnerIdentifier ; \n     var   socket   =   await   WebSocketTransformer . upgrade ( request . innerRequest ); \n     socket . listen (( event )   =   handleEvent ( event ,   fromUserID:   userID )); \n\n     connections [ userID ]   =   socket ; \n\n     return   null ; \n   });   If we continue with the 'chat application' example, the code for  handleEvent  may be something like:  void   handleRequest ( dynamic   event ,   { int   fromUserID })   { \n   var   incoming   =   JSON . decode ( UTF8 . decode ( event )); \n   var   outgoing   =   UTF8 . encode ( JSON . encode ({ \n     text :   incoming [ text ], \n     ... \n   })); \n\n   connections . keys \n     . where (( userID )   =   userID   !=   fromUserID ) \n     . forEach (( userID )   { \n       var   connection   =   connections [ userID ]; \n       connection . add ( outgoing ); \n     });          }   Note that this simple implementation doesn't account for multiple connections from the same user or multi-isolate applications.", 
            "title": "Bi-directional Communication"
        }, 
        {
            "location": "/http/websockets/#considerations-for-multi-isolate-and-multi-instance-applications", 
            "text": "By default, an Aqueduct application runs on multiple isolates. Since each isolate has its own heap, a websocket created on one isolate is not directly accessible by another isolate. In the example above, each isolate would have its own map of connections - therefore, a message is only sent to connections that were opened on the same isolate that the chat message originated from.  A simple solution is to only run the application on a single isolate, ensuring that all websockets are on a single isolate and accessible to one another:  aqueduct   serve   - n   1   For many applications, this is a fine solution. For others, it may not be.  Recall that one of the benefits of Aqueduct's multi-isolate architecture is that code tested on a single instance will scale to multiple instances behind a load balancer. If an Aqueduct application runs correctly on a single, multi-isolate instance, it will will correctly on multiple instances. This (somewhat) enforced structure prevents us from naively keeping track of websocket connections on a single isolate, which would cause issues when we scale out to a multi-instance system.  If you find yourself in a situation where your application is so popular you need multiple servers to efficiently serve requests, you'll have a good idea on how to architect an appropriate solution (or you'll have the money to hire someone that does). In many situations, the REST API and websocket server are separate instances anyhow - they have different lifecycles and deployment behavior. It may make sense to run a websocket server on a single isolate, since you are likely IO-bound instead of CPU bound.  If you still prefer to have a multi-isolate server with websockets, the  ApplicationMessageHub  will come in handy. When broadcasting messages to connected websockets across the application, you first send the data to each websocket connected to the isolate that is originating the message. Then, the message is added to the  ApplicationMessageHub :  void   onChatMessage ( String   message )   { \n   connectedSockets . forEach (( socket )   { \n     socket . add ( message ); \n   }); \n\n   requestSink . messageHub . add ({ event :   websocket_broadcast ,   message :   message });  }   Anything added to the  messageHub  will be delivered to the listener for every other message hub - i.e., every other isolate will receive this data. The other isolates then send the message to each of their connected websockets:  class   ChatSink   extends   RequestSink   { \n   ChatSink ( ApplicationConfiguration   config )   :   super ( config )   { \n     messageHub . listen (( event )   { \n       if   ( event   is   Map     event [ event ]   ==   websocket_broadcast )   { \n         connectedSockets . forEach (( socket )   { \n           socket . add ( event [ message ]); \n         }); \n       } \n     }); \n   }  }", 
            "title": "Considerations for Multi-Isolate and Multi-Instance Applications"
        }, 
        {
            "location": "/http/threading/", 
            "text": "Multi-threading in Aqueduct\n\n\nOne of the primary differentiators between Aqueduct and other server frameworks is its multi-threaded behavior. When an Aqueduct application starts, it replicates the application logic across a number of threads.\n\n\nIn Dart, threads are called \nisolates\n. The difference in naming isn't just to be cute - an isolate is a little bit different than a thread in that memory is not shared between isolates. Each isolate has its own heap (this is where memory for objects is allocated from) that other isolates can't access.\n\n\nAn isolated thread is very valuable. Many multi-threaded applications share memory across threads, which require difficult design patterns and can easily introduce hard to track bugs that even the most experienced programmer will have a hard time avoiding.\n\n\nHow Aqueduct Uses Isolates\n\n\nAn application is initialized by invoking a series of initialization methods in a \nRequestSink\n. Once these methods are finished executing, the application starts sending HTTP requests through the request channel created by the \nRequestSink\n.\n\n\nBecause a \nRequestSink\n is a type - and can be instantiated - Aqueduct simply creates a number of isolates and instantiates \nRequestSink\n for each. The initialization code is therefore identical for each isolate, which means that the ongoing behavior of each isolate is also identical.\n\n\nMore importantly, you - the programmer - have to do absolutely nothing to take advantage of Aqueduct's multi-threaded behavior. You simply have to pick the number of isolates you want to run the application on (see \nthis section\n).\n\n\nWhile you don't have to do anything in an Aqueduct application to take advantage of multiple processors, there are things you shouldn't do or should do in another way.\n\n\nFirst, you must be careful of keeping any state in your application. Practically, this means that once your application has finished initializing, it shouldn't store a value in an object that lives past the lifetime of the request it was stored for. Any data that needs to be persisted must be stored in a database or other data store. This is just good practice for a REST API anyhow, so nothing is really lost here.\n\n\nHowever, there are times where you do need to track state. For example, if you are managing websocket connections, you do need to store some state after initialization - a reference to the websocket. \nThis guide\n covers this topic in detail; the simple explanation is to use the \nApplicationMessageHub\n.\n\n\nAnother thing that is important to consider is initialization. Most initialization occurs in \nRequestSink\n's constructor and its \nsetupRouter\n method. This behavior is guaranteed to occur for each isolate and there is often little to worry about.\n\n\nHowever, when implementing \nRequestSink.initializeApplication\n, code runs on the main isolate. Any changes to static variables or singletons will not be replicated to the isolates running the application logic. The use case for this method is rather minimal, but it is very important that types like \nHTTPCodecRepository\n aren't configured in this method.\n\n\nHow Many Isolates Should I Use\n\n\nTo give you a starting point, the default number of isolates for an application is 3 when started with \naqueduct serve\n. While less than 3 isolates will most certainly degrade performance, more than 3 doesn't necessarily improve performance. (And many more will certainly degrade performance.)\n\n\nThere are a number of variables that factor into the isolate count decision. First, recall a computer essentially does two things: it executes instructions and transmits data. Both of these tasks take time, especially when that data is transmitted to another computer thousands of miles away. (It is a modern marvel that these tasks occur as fast as they do - one that we often take for granted.)\n\n\nWhile a few milliseconds difference in speed to handle a single request isn't all that meaningful, when you start receiving thousands of requests at a time, these milliseconds add up. When your application is struggling to keep up with executing instructions, it it said to be \nCPU-bound\n. (When your application is struggling to transmit data, it is said to be \nIO-bound\n.)\n\n\nA CPU-bound application has two choices: execute less instructions or have more processors (or cores) to execute instructions with. Most computers these days have multiple processors. When only using a single isolate, an Aqueduct application can only use one of those processors at a time. As the number of isolates increases, so too do the number of processors that can be used at a time.\n\n\nThus, more isolates means more processors means more instructions and your application bound less by the CPU. However, there is a limit - creating 100 isolates when there are only two processors doesn't yield any greater performance, because 50 isolates will fight over a single processor. In fact, the amount of work to schedule these instructions and the amount of memory this many isolates will consume will hurt performance.\n\n\nFor example, when running benchmarks with \nwrk\n on my four-core machine, I get significantly better results as I add isolates 1 through 4, but then I get marginal gains and finally less or equal performance as I increase the number of isolates past 4. The blue bar represents the number of requests per second for each isolate count when making a simple HTTP call to Aqueduct.\n\n\n\n\n(Note that benchmarks are a good way of measuring relative performance of an application and identifying bottlenecks, but real world usability performance across a network is the only measurement that matters. The absolute value of these benchmarks should not be taken seriously, as they entirely remove the network portion of the transmission.)\n\n\nBut this isn't the whole story. Server applications are rarely CPU-bound, but instead IO-bound. The red bar represents the number of requests per second when making a database query and returning the results of that query as JSON in the response body. When using Observatory, the measurements indicate that the overwhelming majority of the time is spent transmitting data to and from the database. This is an example of being bound by IO (and the speed of the query).\n\n\nRecall that each isolate has its own database connection. A database connection is a serial queue - it can only handle one query at a time. Increasing the number of database connections means handling more queries at a time. This is no more apparent than when looking at the following graph, which measures the latency of requests with and without a database call.\n\n\n\n\nWhen there is only one database connection (one isolate), the latency is significantly higher per request - the application is entirely bound by the speed of the database serial queue. Adding a second isolate, and therefore a second database connection, drops the latency by more than half.\n\n\nThere are diminishing returns as more database connections are added. That's because the same thing about the number of threads per processor also applies to the number of database connections. Adding more database connections doesn't magically make the database server run any faster.\n\n\nAs a general rule, start with N-1 isolates, where N is the number of processors on the machine. Use \nwrk\n and Observatory to profile your application and tune accordingly. In a multi-instance environment, remember that the total number of database connections is MxN, where M is the number of machines and N is the number of isolates.\n\n\nIf you find yourself in a jam where IO would be better served by less isolates, but CPU would be better served by more isolates, you may consider using a database connection pool isolate.", 
            "title": "Multi-threading"
        }, 
        {
            "location": "/http/threading/#multi-threading-in-aqueduct", 
            "text": "One of the primary differentiators between Aqueduct and other server frameworks is its multi-threaded behavior. When an Aqueduct application starts, it replicates the application logic across a number of threads.  In Dart, threads are called  isolates . The difference in naming isn't just to be cute - an isolate is a little bit different than a thread in that memory is not shared between isolates. Each isolate has its own heap (this is where memory for objects is allocated from) that other isolates can't access.  An isolated thread is very valuable. Many multi-threaded applications share memory across threads, which require difficult design patterns and can easily introduce hard to track bugs that even the most experienced programmer will have a hard time avoiding.", 
            "title": "Multi-threading in Aqueduct"
        }, 
        {
            "location": "/http/threading/#how-aqueduct-uses-isolates", 
            "text": "An application is initialized by invoking a series of initialization methods in a  RequestSink . Once these methods are finished executing, the application starts sending HTTP requests through the request channel created by the  RequestSink .  Because a  RequestSink  is a type - and can be instantiated - Aqueduct simply creates a number of isolates and instantiates  RequestSink  for each. The initialization code is therefore identical for each isolate, which means that the ongoing behavior of each isolate is also identical.  More importantly, you - the programmer - have to do absolutely nothing to take advantage of Aqueduct's multi-threaded behavior. You simply have to pick the number of isolates you want to run the application on (see  this section ).  While you don't have to do anything in an Aqueduct application to take advantage of multiple processors, there are things you shouldn't do or should do in another way.  First, you must be careful of keeping any state in your application. Practically, this means that once your application has finished initializing, it shouldn't store a value in an object that lives past the lifetime of the request it was stored for. Any data that needs to be persisted must be stored in a database or other data store. This is just good practice for a REST API anyhow, so nothing is really lost here.  However, there are times where you do need to track state. For example, if you are managing websocket connections, you do need to store some state after initialization - a reference to the websocket.  This guide  covers this topic in detail; the simple explanation is to use the  ApplicationMessageHub .  Another thing that is important to consider is initialization. Most initialization occurs in  RequestSink 's constructor and its  setupRouter  method. This behavior is guaranteed to occur for each isolate and there is often little to worry about.  However, when implementing  RequestSink.initializeApplication , code runs on the main isolate. Any changes to static variables or singletons will not be replicated to the isolates running the application logic. The use case for this method is rather minimal, but it is very important that types like  HTTPCodecRepository  aren't configured in this method.", 
            "title": "How Aqueduct Uses Isolates"
        }, 
        {
            "location": "/http/threading/#how-many-isolates-should-i-use", 
            "text": "To give you a starting point, the default number of isolates for an application is 3 when started with  aqueduct serve . While less than 3 isolates will most certainly degrade performance, more than 3 doesn't necessarily improve performance. (And many more will certainly degrade performance.)  There are a number of variables that factor into the isolate count decision. First, recall a computer essentially does two things: it executes instructions and transmits data. Both of these tasks take time, especially when that data is transmitted to another computer thousands of miles away. (It is a modern marvel that these tasks occur as fast as they do - one that we often take for granted.)  While a few milliseconds difference in speed to handle a single request isn't all that meaningful, when you start receiving thousands of requests at a time, these milliseconds add up. When your application is struggling to keep up with executing instructions, it it said to be  CPU-bound . (When your application is struggling to transmit data, it is said to be  IO-bound .)  A CPU-bound application has two choices: execute less instructions or have more processors (or cores) to execute instructions with. Most computers these days have multiple processors. When only using a single isolate, an Aqueduct application can only use one of those processors at a time. As the number of isolates increases, so too do the number of processors that can be used at a time.  Thus, more isolates means more processors means more instructions and your application bound less by the CPU. However, there is a limit - creating 100 isolates when there are only two processors doesn't yield any greater performance, because 50 isolates will fight over a single processor. In fact, the amount of work to schedule these instructions and the amount of memory this many isolates will consume will hurt performance.  For example, when running benchmarks with  wrk  on my four-core machine, I get significantly better results as I add isolates 1 through 4, but then I get marginal gains and finally less or equal performance as I increase the number of isolates past 4. The blue bar represents the number of requests per second for each isolate count when making a simple HTTP call to Aqueduct.   (Note that benchmarks are a good way of measuring relative performance of an application and identifying bottlenecks, but real world usability performance across a network is the only measurement that matters. The absolute value of these benchmarks should not be taken seriously, as they entirely remove the network portion of the transmission.)  But this isn't the whole story. Server applications are rarely CPU-bound, but instead IO-bound. The red bar represents the number of requests per second when making a database query and returning the results of that query as JSON in the response body. When using Observatory, the measurements indicate that the overwhelming majority of the time is spent transmitting data to and from the database. This is an example of being bound by IO (and the speed of the query).  Recall that each isolate has its own database connection. A database connection is a serial queue - it can only handle one query at a time. Increasing the number of database connections means handling more queries at a time. This is no more apparent than when looking at the following graph, which measures the latency of requests with and without a database call.   When there is only one database connection (one isolate), the latency is significantly higher per request - the application is entirely bound by the speed of the database serial queue. Adding a second isolate, and therefore a second database connection, drops the latency by more than half.  There are diminishing returns as more database connections are added. That's because the same thing about the number of threads per processor also applies to the number of database connections. Adding more database connections doesn't magically make the database server run any faster.  As a general rule, start with N-1 isolates, where N is the number of processors on the machine. Use  wrk  and Observatory to profile your application and tune accordingly. In a multi-instance environment, remember that the total number of database connections is MxN, where M is the number of machines and N is the number of isolates.  If you find yourself in a jam where IO would be better served by less isolates, but CPU would be better served by more isolates, you may consider using a database connection pool isolate.", 
            "title": "How Many Isolates Should I Use"
        }, 
        {
            "location": "/db/overview/", 
            "text": "Tasks\n\n\nAqueduct has an ORM to store data in a database and map database data to Dart objects.\n\n\n\n\nDefining a data model by declaring \nManagedObject\nT\n subclasses\n\n\nInserting, updating, reading and deleting data with \nQuery\nT\n.\n\n\nCreating \nManagedObject\nT\ns from HTTP request body data like JSON\n\n\nEncoding \nManagedObject\nT\ns into an HTTP response body\n\n\nGenerating and upgrading database schemas with the \naqueduct db\n tool.\n\n\n\n\nGuides\n\n\n\n\nConnecting to a Database\n\n\nModeling Data\n\n\nStorage, Serialization and Deserialization\n\n\nExecuting Queries\n\n\nJoins, Filtering and Paging\n\n\nAdding Validations and Callbacks to ManagedObject\n\n\nAqueduct Database Tool", 
            "title": "Overview"
        }, 
        {
            "location": "/db/overview/#tasks", 
            "text": "Aqueduct has an ORM to store data in a database and map database data to Dart objects.   Defining a data model by declaring  ManagedObject T  subclasses  Inserting, updating, reading and deleting data with  Query T .  Creating  ManagedObject T s from HTTP request body data like JSON  Encoding  ManagedObject T s into an HTTP response body  Generating and upgrading database schemas with the  aqueduct db  tool.", 
            "title": "Tasks"
        }, 
        {
            "location": "/db/overview/#guides", 
            "text": "Connecting to a Database  Modeling Data  Storage, Serialization and Deserialization  Executing Queries  Joins, Filtering and Paging  Adding Validations and Callbacks to ManagedObject  Aqueduct Database Tool", 
            "title": "Guides"
        }, 
        {
            "location": "/db/connecting/", 
            "text": "Connecting to a Database from Aqueduct\n\n\nThe interface to a database from Aqueduct is an instance of \nManagedContext\n, which contains the following two objects:\n\n\n\n\na \nManagedDataModel\n that describes your application's data model\n\n\na \nPersistentStore\n that creates database connections and transmits data across that connection.\n\n\n\n\nA \nManagedContext\n uses these two objects to coordinate moving data to and from your application and a database when executing \nQuery\nT\ns. A \nManagedContext\n - and its store and data model - are created in a \nRequestSink\n constructor.\n\n\nclass\n \nMyRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nMyRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n    \nvar\n \ndataModel\n \n=\n \nnew\n \nManagedDataModel\n.\nfromCurrentMirrorSystem\n();\n\n    \nvar\n \npsc\n \n=\n \nnew\n \nPostgreSQLPersistentStore\n.\nfromConnectionInfo\n(\n\n        \nusername\n,\n \npassword\n,\n \nhost\n,\n \n5432\n,\n \ndatabaseName\n);\n\n\n    \nManagedContext\n.\ndefaultContext\n \n=\n \nnew\n \nManagedContext\n(\ndataModel\n,\n \npsc\n);\n\n  \n}\n\n\n}\n\n\n\n\n\n\nA \nManagedDataModel\n should be instantiated with its \nfromCurrentMirrorSystem\n convenience constructor. You may optionally pass a list of \nManagedObject\nT\n subclasses to its default constructor.\n\n\nvar\n \ndataModel\n \n=\n \nnew\n \nManagedDataModel\n([\nUser\n,\n \nPost\n,\n \nFriendship\n]);\n\n\n\n\n\n\nThere is one \ndefaultContext\n in an application. When executing a \nQuery\nT\n, the default context is chosen if no other context is provided to its constructor. A default context must be instantiated before using any Aqueduct ORM objects or behavior.\n\n\nConnection information for a database is often configured through a configuration file.\n\n\nclass\n \nMyConfigurationItem\n \nextends\n \nConfigurationItem\n \n{\n\n  \nMyConfigurationItem\n(\nString\n \nconfigPath\n)\n \n:\n \nsuper\n.\nfromFile\n(\nconfigPath\n);\n\n\n  \nDatabaseConnectionConfiguration\n \ndatabase\n;\n\n\n}\n\n\n\nclass\n \nMyRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nMyRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n    \nvar\n \nappConfig\n \n=\n \nnew\n \nMyConfigurationItem\n(\nconfig\n.\nconfigurationFilePath\n);\n\n\n    \nvar\n \ndataModel\n \n=\n \nnew\n \nManagedDataModel\n.\nfromCurrentMirrorSystem\n();\n\n    \nvar\n \npsc\n \n=\n \nnew\n \nPostgreSQLPersistentStore\n.\nfromConnectionInfo\n(\n\n        \nappConfig\n.\ndatabase\n.\nusername\n,\n\n        \nappConfig\n.\ndatabase\n.\npassword\n,\n\n        \nappConfig\n.\ndatabase\n.\nhost\n,\n\n        \nappConfig\n.\ndatabase\n.\nport\n,\n\n        \nappConfig\n.\ndatabase\n.\ndatabaseName\n);\n        \n\n    \nManagedContext\n.\ndefaultContext\n \n=\n \nnew\n \nManagedContext\n(\ndataModel\n,\n \npsc\n);\n\n  \n}\n\n\n}\n\n\n\n\n\n\nA YAML configuration file loaded by this application must look like this:\n\n\ndatabase\n:\n\n  \nusername\n:\n \nbob\n\n  \npassword\n:\n \nbobspassword\n\n  \nhost\n:\n \nlocalhost\n\n  \nport\n:\n \n5432\n\n  \ndatabaseName\n:\n \nmy_app\n\n\n\n\n\n\nA \nPersistentStore\n is an interface. Concrete implementations - like \nPostgreSQLPersistentStore\n - implement that interface to transmit data to a database in the format it expects. A \nPostgreSQLPersistentStore\n will automatically connect and maintain a persistent connection to a database. If the connection is lost for some reason, it will automatically reconnect the next time a query is executed. If a connection cannot be established, a \nQueryException\n is thrown that yields a 503 status code response.", 
            "title": "Connecting to a Database"
        }, 
        {
            "location": "/db/connecting/#connecting-to-a-database-from-aqueduct", 
            "text": "The interface to a database from Aqueduct is an instance of  ManagedContext , which contains the following two objects:   a  ManagedDataModel  that describes your application's data model  a  PersistentStore  that creates database connections and transmits data across that connection.   A  ManagedContext  uses these two objects to coordinate moving data to and from your application and a database when executing  Query T s. A  ManagedContext  - and its store and data model - are created in a  RequestSink  constructor.  class   MyRequestSink   extends   RequestSink   { \n   MyRequestSink ( ApplicationConfiguration   config )   :   super ( config )   { \n     var   dataModel   =   new   ManagedDataModel . fromCurrentMirrorSystem (); \n     var   psc   =   new   PostgreSQLPersistentStore . fromConnectionInfo ( \n         username ,   password ,   host ,   5432 ,   databaseName ); \n\n     ManagedContext . defaultContext   =   new   ManagedContext ( dataModel ,   psc ); \n   }  }   A  ManagedDataModel  should be instantiated with its  fromCurrentMirrorSystem  convenience constructor. You may optionally pass a list of  ManagedObject T  subclasses to its default constructor.  var   dataModel   =   new   ManagedDataModel ([ User ,   Post ,   Friendship ]);   There is one  defaultContext  in an application. When executing a  Query T , the default context is chosen if no other context is provided to its constructor. A default context must be instantiated before using any Aqueduct ORM objects or behavior.  Connection information for a database is often configured through a configuration file.  class   MyConfigurationItem   extends   ConfigurationItem   { \n   MyConfigurationItem ( String   configPath )   :   super . fromFile ( configPath ); \n\n   DatabaseConnectionConfiguration   database ;  }  class   MyRequestSink   extends   RequestSink   { \n   MyRequestSink ( ApplicationConfiguration   config )   :   super ( config )   { \n     var   appConfig   =   new   MyConfigurationItem ( config . configurationFilePath ); \n\n     var   dataModel   =   new   ManagedDataModel . fromCurrentMirrorSystem (); \n     var   psc   =   new   PostgreSQLPersistentStore . fromConnectionInfo ( \n         appConfig . database . username , \n         appConfig . database . password , \n         appConfig . database . host , \n         appConfig . database . port , \n         appConfig . database . databaseName );         \n\n     ManagedContext . defaultContext   =   new   ManagedContext ( dataModel ,   psc ); \n   }  }   A YAML configuration file loaded by this application must look like this:  database : \n   username :   bob \n   password :   bobspassword \n   host :   localhost \n   port :   5432 \n   databaseName :   my_app   A  PersistentStore  is an interface. Concrete implementations - like  PostgreSQLPersistentStore  - implement that interface to transmit data to a database in the format it expects. A  PostgreSQLPersistentStore  will automatically connect and maintain a persistent connection to a database. If the connection is lost for some reason, it will automatically reconnect the next time a query is executed. If a connection cannot be established, a  QueryException  is thrown that yields a 503 status code response.", 
            "title": "Connecting to a Database from Aqueduct"
        }, 
        {
            "location": "/db/modeling_data/", 
            "text": "Modeling Data\n\n\nIn Aqueduct, database tables are modeled by subclassing \nManagedObject\nT\n. These are declared like so:\n\n\nclass\n \nUser\n \nextends\n \nManagedObject\n_User\n \nimplements\n \n_User\n \n{\n  \n\n}\n\n\n\nclass\n \n_User\n \n{\n\n  \n@\nmanagedPrimaryKey\n\n  \nint\n \nid\n;\n\n\n  \nString\n \nname\n;\n\n\n}\n\n\n\n\n\n\nThis declares a \nUser\n type for use in application code. The \n_User\n type describes a table named \n_User\n in a database. The table has two columns, a primary key integer named \nid\n and a text column named \nname\n.\n\n\nAn instance of \nUser\n represents a row in the \n_User\n table. When you fetch rows from the \n_User\n table, you will get instances of \nUser\n. This type - referred to as the \ninstance type\n of a managed object - must subclass \nManagedObject\nT\n.\n\n\nThe type argument of \nManagedObject\nT\n declares the table that backs instances of this type. The table definition type - \n_User\n - is called the \npersistent type\n of a managed object. Properties in the persistent type must have a corresponding column in the database. Properties in the instance type are not stored in a database and are called \ntransient\n.\n\n\nAn instance type should implement its persistent type, e.g. \nimplements _User\n. This allows \nUser\n to have the properties \nid\n and \nname\n.\n\n\nA persistent type, by convention, is prefixed with an underscore. This is for two reasons. First, the underscore makes it can't be used in other files - because it shouldn't be. Second, some databases have predefined tables and you may want to have similarly named tables in your application. For example, there is a \nuser\n table in PostgreSQL. The prefix makes it so you don't have a name collision with a predefined table. (Later in the guide, we'll go over how to name tables differently than the persistent type name, but this is rarely useful.)\n\n\nA \nManagedObject\nT\n manages the storage and validation of properties that are stored in a database - i.e. the properties declared in the persistent type.\n\n\nThe distinction between persistent type and instance type allows for many of the powerful features of Aqueduct, which are covered by other guides. For now, the key takeaway is that the persistent type must map directly to a database table - every property must correspond to a database column, and vice versa. Aqueduct has tools to generate database tables based on the declaration of persistent types in an application (see \nAqueduct Database Tool\n).\n\n\nMore on Persistent Types\n\n\nPersistent types define the mapping between your managed objects and a database table. As each property in a persistent type represents a database column, the type of the property must be storable in a database. The following types are available as scalar properties on a persistent type:\n\n\n\n\nint\n\n\ndouble\n\n\nString\n\n\nDateTime\n\n\nbool\n\n\nAny \nenum\n\n\n\n\nProperties that are one of these types are more referred to as the \nattributes\n of an entity. Properties that are references to other model objects - which we will see later - are called \nrelationships\n. Collectively, attributes and relationships are called \nproperties\n.\n\n\nIn addition to a type and name, each property can also have \nManagedColumnAttributes\n that adds some details to the associated column. \nManagedColumnAttributes\n are added as metadata to a property. For example, the following change to the \n_User\n persistent type adds a \nString\n \nemail\n property which must be unique across all users:\n\n\nclass\n \n_User\n \n{\n\n  \n@\nmanagedPrimaryKey\n\n  \nint\n \nid\n;\n\n\n  \nString\n \nname\n;\n\n\n  \n@\nManagedColumnAttributes\n(\nunique:\n \ntrue\n)\n\n  \nString\n \nemail\n;\n\n\n}\n\n\n\n\n\n\nThere are eight configurable items available in the \nManagedColumnAttributes\n class.\n\n\n\n\nprimaryKey\n - Indicates that property is the primary key of the table represented by this persistent type. Must be one per persistent type.\n\n\ndatabaseType\n - Uses a more specific type for the database column than can be derived from the Dart type of the property. For example, you may wish to specify that an integer property is stored in a database column that holds an 8-byte integer, instead of the default 4-byte integer.\n\n\nnullable\n - Toggles whether or not this property can contain the null value.\n\n\ndefaultValue\n - A default value for this property when inserted into a database without an explicit value.\n\n\nunique\n - Toggles whether or not this property must be unique across all instances of this type.\n\n\nindexed\n - Toggles whether or not this property's database column should be indexed for faster searching.\n\n\nomitByDefault\n - Toggles whether or not this property should be fetched from the database by default. Useful for properties like hashed passwords, where you don't want to return that information when fetching an account unless you explicitly want to check the password.\n\n\nautoincrement\n - Toggles whether or not the underlying database should generate a new value from a serial generator each time a new instance is inserted into the database.\n\n\n\n\nBy not specifying \nManagedColumnAttributes\n, the default values for each of these possible configurations is used and the database type is inferred from the type of the property. This also means that \nall\n properties declared in a persistent type represent a column in a database table - even without \nManagedColumnAttributes\n metadata.\n\n\nEvery persistent type must have at least one property with \nManagedColumnAttributes\n where \nprimaryKey\n is true. There is a convenience instance of \nManagedColumnAttributes\n for this purpose, \n@managedPrimaryKey\n, which is equivalent to the following:\n\n\n@\nManagedColumnAttributes\n(\nprimaryKey:\n \ntrue\n,\n \ndatabaseType:\n \nPropertyType\n.\nbigInteger\n,\n \nautoincrement:\n \ntrue\n)\n\n\n\n\n\n\nAlso in the persistent type - and only the persistent type - you may override the name of the table by implementing a static method named \ntableName\n that returns the name of the table in a persistent type:\n\n\nclass\n \n_User\n \n{\n\n  \n@\nmanagedPrimaryKey\n\n  \nint\n \nid\n;\n\n\n  \nString\n \nname\n;\n\n\n  \nstatic\n \nString\n \ntableName\n()\n \n=\n \nUserTable\n;\n\n\n}\n\n\n\n\n\n\nNote that the specific database driver determines whether or not the table name is case-sensitive or not. The included database driver for PostgreSQL automatically lowercases table names and is case-insensitive.\n\n\nEnum Type Persistent Properties\n\n\nWhen a persistent property is an \nenum\n type, the enumeration is stored as a string in the database. Consider the following definition where a user can be an admin or a normal user:\n\n\nenum\n \nUserType\n \n{\n\n  \nadmin\n,\n \nuser\n\n\n}\n\n\n\nclass\n \nUser\n \nextends\n \nManagedObject\n_User\n \nimplements\n \n_User\n \n{}\n\n\nclass\n \n_User\n \n{\n\n  \n@\nmanagedPrimaryKey\n\n  \nint\n \nid\n;\n\n\n  \nString\n \nname\n;\n\n  \nUserType\n \ntype\n;\n\n\n}\n\n\n\n\n\n\nYour code works be assigning valid enumeration cases to the \nUser.type\n property:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nvalues\n.\nname\n \n=\n \nBob\n\n  \n..\nvalues\n.\ntype\n \n=\n \nUserType\n.\nadmin\n;\n\n\nvar\n \nbob\n \n=\n \nawait\n \nquery\n.\ninsert\n();\n\n\n\nquery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nwhere\n.\ntype\n \n=\n \nwhereEqualTo\n(\nUserType\n.\nadmin\n);\n\n\nvar\n \nallAdmins\n \n=\n \nawait\n \nquery\n.\nfetch\n();\n\n\n\n\n\n\nIn the underlying database, the \ntype\n column is stored as a string. Its value is either \"admin\" or \"user\" - which is derived from the two enumeration case names. A enumerated type property has an implicit \nValidate.oneOf\n validator that asserts the value is one of the valid enumeration cases.\n\n\nManagedObject\n\n\nWhere persistent types simply declare a mapping to a database table, \nManagedObject\nT\ns do the actual work of lugging data between HTTP clients, Aqueduct applications and databases.\n\n\nManaged objects can be inserted into and fetched from a database. They can be used to configure an update to a database row. They can read their values from a \nMap\n and write them into a \nMap\n - this \nMap\n can safely be encoded to or decoded from JSON or another transmission format. This allows \nManagedObject\nT\ns to be exactly represented in an HTTP request or response. Managed objects also lay the foundation for building queries. Here's an example of a common lifecycle of a \nManagedObject\nT\n subclass, \nUser\n:\n\n\n@\nhttpPost\n \ncreateThing\n(\n@\nHTTPBody\n()\n \nUser\n \nuser\n)\n \nasync\n \n{\n\n  \n// Construct Query for inserting the user, using values from the request body.\n\n  \nvar\n \ninsertQuery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n    \n..\nvalues\n \n=\n \nuser\n;\n\n\n  \n// Execute insert, get User back from database\n\n  \nvar\n \ninsertedUser\n \n=\n \nawait\n \ninsertQuery\n.\ninsert\n();\n\n\n  \n// Return response with inserted User serialized as JSON HTTP response body.\n\n  \nreturn\n \nnew\n \nResponse\n.\nok\n(\ninsertedUser\n);\n\n\n}\n\n\n\n\n\n\nWhen getting managed objects from a database, each instance will represent one row. For example, consider the following table, and the previous example of \n_User\n and \nUser\n types:\n\n\n\n\n\n\n\n\nid\n\n\nname\n\n\n\n\n\n\n\n\n\n\n1\n\n\nBob\n\n\n\n\n\n\n2\n\n\nFred\n\n\n\n\n\n\n\n\nIf this entire table were fetched, you'd get a \nList\nUser\n as though you had written the following code:\n\n\nvar\n \nusers\n \n=\n \n[\n\n  \nnew\n \nUser\n()\n\n    \n..\nid\n \n=\n \n1\n\n    \n..\nname\n \n=\n \nBob\n,\n\n\n  \nnew\n \nUser\n()\n\n    \n..\nid\n \n=\n \n2\n\n    \n..\nname\n \n=\n \nFred\n\n\n];\n\n\n\n\n\n\nManaged objects may also declare additional properties and methods beyond those in its persistent type. Because these properties and methods are not part of the persistent type, they are \ntransient\n - that is, their values are not stored in the database. Any method or property defined in a subclass of \nManagedObject\nT\n is ignored when sending data to a database. This is different than properties in a persistent type, where every property explicitly maps to a database column. Here's an example:\n\n\nclass\n \nVideo\n \nextends\n \nManagedObject\n_Video\n \nimplements\n \n_Video\n \n{\n\n  \nbool\n \nget\n \nisRecent\n \n=\n \nreturn\n \nnew\n \nDateTime\n.\nnow\n().\ndifference\n(\nuploadDate\n).\ninDays\n \n \n7\n;\n\n\n}\n\n\n\nclass\n \n_Video\n \n{\n\n  \n@\nmanagedPrimaryKey\n \nint\n \nid\n;\n\n  \nDateTime\n \nuploadDate\n;\n\n\n  \n/* more properties */\n\n  \n...\n\n\n}\n\n\n\n\n\n\nEach video has a persistent property that indicates when the video was uploaded. As a convenience, you'd like to be able to determine if a video is \"recent\" - that is, it has been uploaded in the last week. Adding an \nisRecent\n property to the persistent type doesn't make any sense, because that information can be derived from the existing upload date property. This is a good place to use a transient property.\n\n\nBy default, transient properties are not included when a \nManagedObject\nT\n is written into or read from a \nMap\n. When a \nVideo\n is returned as JSON in an HTTP response, \nisRecent\n won't be in the HTTP body. However, this is just the default behavior and can easily be changed, though - see \nStorage, Serialization and Deserialization\n for more details.\n\n\nYou may also override a \nManagedObject\nT\ns \nasMap()\n method to get to similar behavior:\n\n\nclass\n \nVideo\n \nextends\n \nManagedObject\n_Video\n \nimplements\n \n_Video\n \n{\n\n  \nMap\nString\n,\n \ndynamic\n \nasMap\n()\n \n{\n\n    \nvar\n \nm\n \n=\n \nsuper\n.\nasMap\n();\n\n    \nm\n[\nisRecent\n]\n \n=\n \nnew\n \nDateTime\n.\nnow\n().\ndifference\n(\nuploadDate\n).\ninDays\n \n \n7\n;\n\n    \nreturn\n \nm\n;\n\n  \n}\n\n\n}\n\n\n\n\n\n\nModeling Managed Object Relationships\n\n\nIn addition to attributes, managed objects may also have properties that are other managed objects or collections of managed objects. These types of properties are called \nrelationships\n. For example, in a social network application, a user may have many posts that they have created. A user, then, should have a property that is a list of posts. This is called a 'has-many' relationship, because a user can have many posts.\n\n\nA user might also have a job, so the user type should also have a property that references their job. This is called a 'has-one' relationship, because a user can only ever have one job at a time.\n\n\nRelationships are also properties declared in a persistent type. In the above examples, a user would look like this:\n\n\nclass\n \nUser\n \nextends\n \nManagedObject\n_User\n \nimplements\n \n_User\n \n{}\n\n\nclass\n \n_User\n \n{\n  \n  \n@\nprimaryKey\n \nint\n \nid\n;\n\n  \nString\n \nname\n;\n\n\n  \nJob\n \njob\n;\n\n  \nManagedSet\nPost\n \nposts\n;\n\n\n}\n\n\n\n\n\n\nIf the type of a property is a \nManagedObject\nT\n subclass - like \nJob\n - the relationship is has-one.\n\n\nThe type \nManagedSet\n is what indicates that the relationship is has-many. A \nManagedSet\n is a glorified \nList\n - it can do everything a \nList\n can do - but has some additional behavior to help manage relationships and build queries. The type argument must be a \nManagedObject\nT\n subclass.\n\n\nOne thing to note here is that all things 'database related' are declared inside the persistent type. The persistent type declares the database table, attribute properties declare the columns the table has, and relationship properties declare relationships to other database tables.\n\n\nThe relationship properties in \n_User\n do not represent columns in a database - they represent \nentire rows\n in a database table. Relationships in the database are maintained by foreign key constraints. Therefore, the types \nJob\n and \nPost\n must have a column that stores the primary key of a  \n_User\n. Let's look at \nJob\n first:\n\n\nclass\n \nJob\n \nextends\n \nManagedObject\n_Job\n \nimplements\n \n_Job\n \n{}\n\n\nclass\n \n_Job\n \n{\n\n  \n@\nmanagedPrimaryKey\n\n  \nint\n \nid\n;\n\n  \nString\n \ntitle\n;\n\n\n  \n@\nManagedRelationship\n(\n#\njob\n)\n\n  \nUser\n \nuser\n;\n\n\n}\n\n\n\n\n\n\nJob.user\n is a relationship property because it is a \nManagedObject\nT\n subclass. It is the \ninverse\n property of \nUser.job\n. All relationship properties must have an inverse. In other words, if a user has a job, then a job has a user. The inverse is set up by adding \nManagedRelationship\n data to one of the relationship properties. The argument to \nManagedRelationship\n is the name of the property on the other type.\n\n\nOnly one side of the relationship may have \nManagedRelationship\n metadata. The side with this metadata is said to \nbelong to\n the other side. Thus, a \nJob\n belongs to a \nUser\n and a \nUser\n has-one \nJob\n. The property with \nManagedRelationship\n metadata is represented by a foreign key column in the database. The table \n_Job\n, then, has three columns: \nid\n, \ntitle\n and \nuser_id\n. The name \nuser_id\n is generated by joining the name of the relationship property with the name of the primary key on the other object.\n\n\nSetting the inverse of a has-many relationship is done in the same way, so \nPost\n would be declared like so:\n\n\nclass\n \nPost\n \nextends\n \nManagedObject\n_Post\n \nimplements\n \n_Post\n \n{}\n\n\nclass\n \n_Post\n \n{\n\n  \n@\nmanagedPrimaryKey\n\n  \nint\n \nid\n;\n\n  \nString\n \ntext\n;\n\n\n  \n@\nManagedRelationship\n(\n#\nposts\n)\n\n  \nUser\n \nuser\n;\n\n\n}\n\n\n\n\n\n\nThe types of relationship properties must always be the instance type, not the persistent type. In other words, \nUser.job\n is of type \nJob\n, not \n_Job\n.\n\n\nWhen an application starts up, relationships are checked for integrity. This check ensures that relationships are two-sided and only one property has the \nManagedRelationship\n metadata. If they do not, an exception will be thrown.\n\n\nManagedRelationship\n properties are always indexed; this may change in the future to be configurable, but it will always be the default. Additionally, the column backing \nManagedRelationship\n properties are unique if the other side is a 'has-one' relationship. Because the \nManagedRelationship\n property is actually a foreign key column, it may also define some extra configuration parameters: a delete rule and whether or not it is required.\n\n\nBy making \nPost.user\n required, we will require that every \nPost\n must have a user in order to be inserted into the database. This means that a \nPost\n cannot exist without a user (i.e., the foreign key may not be null),\n\n\nclass\n \n_Post\n \n{\n\n  \n...\n\n  \n@\nManagedRelationship\n(\n#\nposts\n,\n \nrequired:\n \ntrue\n)\n\n  \nUser\n \nuser\n;\n\n\n}\n\n\n\n\n\n\nBy changing the \nJob.user\n delete rule to \nRelationshipDeleteRule.cascade\n, deleting a \nUser\n will also delete its \nJob\n:\n\n\nclass\n \n_Job\n \n{\n\n  \n...\n\n  \n@\nManagedRelationship\n(\n#\njob\n,\n \nonDelete:\n \nManagedRelationshipDeleteRule\n.\ncascade\n)\n\n  \nUser\n \nuser\n;\n\n\n}\n\n\n\n\n\n\nBy default, the delete rule is \nManagedRelationshipDeleteRule.nullify\n (it is the least destructive action) and required is \nfalse\n. If you try and set up a relationship where the \nManagedRelationship\n is both \nManagedRelationshipDeleteRule.nullify\n and \nisRequired\n, you will get an exception during startup: if the foreign key column can't be null and deleting the related object would nullify the foreign key column... well, that wouldn't work.\n\n\nWhen fetching managed objects from a database, there are rules on which relationship properties are fetched. By default, any 'has-one' or 'has-many' relationships are \nnot\n fetched from the database:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n();\n\n\nvar\n \nuser\n \n=\n \nawait\n \nquery\n.\nfetchOne\n();\n\n\n\nvar\n \nuserMap\n \n=\n \nuser\n.\nasMap\n();\n\n\nuserMap\n \n==\n \n{\n\n  \nid\n \n:\n \n1\n,\n\n  \nname\n \n:\n \nBob\n\n\n};\n \n// does not contain \njob\n or \nposts\n\n\n\n\n\n\nIn order to fetch these types of relationships, you must explicitly configure a \nQuery\nT\n to include them, which executes a SQL join. This is covered in the \nExecuting Queries\n.\n\n\nThe \nManagedRelationship\n property, however, will be fetched by default. But, the entire object is not fetched - only its primary key value:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nJob\n();\n\n\nvar\n \njob\n \n=\n \nawait\n \nquery\n.\nfetchOne\n();\n\n\n\nvar\n \njobMap\n \n=\n \njob\n.\nasMap\n();\n\n\njobMap\n \n==\n \n{\n\n  \nid\n \n:\n \n1\n,\n\n  \ntitle\n \n:\n \nProgrammer\n,\n\n  \nuser\n \n:\n \n{\n\n    \nid\n \n:\n \n1\n\n  \n}\n\n\n};\n\n\n\n\n\n\nIt is possible to configure a \nQuery\nT\n that will fetch the full object in this case, too.", 
            "title": "Modeling Data"
        }, 
        {
            "location": "/db/modeling_data/#modeling-data", 
            "text": "In Aqueduct, database tables are modeled by subclassing  ManagedObject T . These are declared like so:  class   User   extends   ManagedObject _User   implements   _User   {    }  class   _User   { \n   @ managedPrimaryKey \n   int   id ; \n\n   String   name ;  }   This declares a  User  type for use in application code. The  _User  type describes a table named  _User  in a database. The table has two columns, a primary key integer named  id  and a text column named  name .  An instance of  User  represents a row in the  _User  table. When you fetch rows from the  _User  table, you will get instances of  User . This type - referred to as the  instance type  of a managed object - must subclass  ManagedObject T .  The type argument of  ManagedObject T  declares the table that backs instances of this type. The table definition type -  _User  - is called the  persistent type  of a managed object. Properties in the persistent type must have a corresponding column in the database. Properties in the instance type are not stored in a database and are called  transient .  An instance type should implement its persistent type, e.g.  implements _User . This allows  User  to have the properties  id  and  name .  A persistent type, by convention, is prefixed with an underscore. This is for two reasons. First, the underscore makes it can't be used in other files - because it shouldn't be. Second, some databases have predefined tables and you may want to have similarly named tables in your application. For example, there is a  user  table in PostgreSQL. The prefix makes it so you don't have a name collision with a predefined table. (Later in the guide, we'll go over how to name tables differently than the persistent type name, but this is rarely useful.)  A  ManagedObject T  manages the storage and validation of properties that are stored in a database - i.e. the properties declared in the persistent type.  The distinction between persistent type and instance type allows for many of the powerful features of Aqueduct, which are covered by other guides. For now, the key takeaway is that the persistent type must map directly to a database table - every property must correspond to a database column, and vice versa. Aqueduct has tools to generate database tables based on the declaration of persistent types in an application (see  Aqueduct Database Tool ).", 
            "title": "Modeling Data"
        }, 
        {
            "location": "/db/modeling_data/#more-on-persistent-types", 
            "text": "Persistent types define the mapping between your managed objects and a database table. As each property in a persistent type represents a database column, the type of the property must be storable in a database. The following types are available as scalar properties on a persistent type:   int  double  String  DateTime  bool  Any  enum   Properties that are one of these types are more referred to as the  attributes  of an entity. Properties that are references to other model objects - which we will see later - are called  relationships . Collectively, attributes and relationships are called  properties .  In addition to a type and name, each property can also have  ManagedColumnAttributes  that adds some details to the associated column.  ManagedColumnAttributes  are added as metadata to a property. For example, the following change to the  _User  persistent type adds a  String   email  property which must be unique across all users:  class   _User   { \n   @ managedPrimaryKey \n   int   id ; \n\n   String   name ; \n\n   @ ManagedColumnAttributes ( unique:   true ) \n   String   email ;  }   There are eight configurable items available in the  ManagedColumnAttributes  class.   primaryKey  - Indicates that property is the primary key of the table represented by this persistent type. Must be one per persistent type.  databaseType  - Uses a more specific type for the database column than can be derived from the Dart type of the property. For example, you may wish to specify that an integer property is stored in a database column that holds an 8-byte integer, instead of the default 4-byte integer.  nullable  - Toggles whether or not this property can contain the null value.  defaultValue  - A default value for this property when inserted into a database without an explicit value.  unique  - Toggles whether or not this property must be unique across all instances of this type.  indexed  - Toggles whether or not this property's database column should be indexed for faster searching.  omitByDefault  - Toggles whether or not this property should be fetched from the database by default. Useful for properties like hashed passwords, where you don't want to return that information when fetching an account unless you explicitly want to check the password.  autoincrement  - Toggles whether or not the underlying database should generate a new value from a serial generator each time a new instance is inserted into the database.   By not specifying  ManagedColumnAttributes , the default values for each of these possible configurations is used and the database type is inferred from the type of the property. This also means that  all  properties declared in a persistent type represent a column in a database table - even without  ManagedColumnAttributes  metadata.  Every persistent type must have at least one property with  ManagedColumnAttributes  where  primaryKey  is true. There is a convenience instance of  ManagedColumnAttributes  for this purpose,  @managedPrimaryKey , which is equivalent to the following:  @ ManagedColumnAttributes ( primaryKey:   true ,   databaseType:   PropertyType . bigInteger ,   autoincrement:   true )   Also in the persistent type - and only the persistent type - you may override the name of the table by implementing a static method named  tableName  that returns the name of the table in a persistent type:  class   _User   { \n   @ managedPrimaryKey \n   int   id ; \n\n   String   name ; \n\n   static   String   tableName ()   =   UserTable ;  }   Note that the specific database driver determines whether or not the table name is case-sensitive or not. The included database driver for PostgreSQL automatically lowercases table names and is case-insensitive.", 
            "title": "More on Persistent Types"
        }, 
        {
            "location": "/db/modeling_data/#enum-type-persistent-properties", 
            "text": "When a persistent property is an  enum  type, the enumeration is stored as a string in the database. Consider the following definition where a user can be an admin or a normal user:  enum   UserType   { \n   admin ,   user  }  class   User   extends   ManagedObject _User   implements   _User   {}  class   _User   { \n   @ managedPrimaryKey \n   int   id ; \n\n   String   name ; \n   UserType   type ;  }   Your code works be assigning valid enumeration cases to the  User.type  property:  var   query   =   new   Query User () \n   .. values . name   =   Bob \n   .. values . type   =   UserType . admin ;  var   bob   =   await   query . insert ();  query   =   new   Query User () \n   .. where . type   =   whereEqualTo ( UserType . admin );  var   allAdmins   =   await   query . fetch ();   In the underlying database, the  type  column is stored as a string. Its value is either \"admin\" or \"user\" - which is derived from the two enumeration case names. A enumerated type property has an implicit  Validate.oneOf  validator that asserts the value is one of the valid enumeration cases.", 
            "title": "Enum Type Persistent Properties"
        }, 
        {
            "location": "/db/modeling_data/#managedobject", 
            "text": "Where persistent types simply declare a mapping to a database table,  ManagedObject T s do the actual work of lugging data between HTTP clients, Aqueduct applications and databases.  Managed objects can be inserted into and fetched from a database. They can be used to configure an update to a database row. They can read their values from a  Map  and write them into a  Map  - this  Map  can safely be encoded to or decoded from JSON or another transmission format. This allows  ManagedObject T s to be exactly represented in an HTTP request or response. Managed objects also lay the foundation for building queries. Here's an example of a common lifecycle of a  ManagedObject T  subclass,  User :  @ httpPost   createThing ( @ HTTPBody ()   User   user )   async   { \n   // Construct Query for inserting the user, using values from the request body. \n   var   insertQuery   =   new   Query User () \n     .. values   =   user ; \n\n   // Execute insert, get User back from database \n   var   insertedUser   =   await   insertQuery . insert (); \n\n   // Return response with inserted User serialized as JSON HTTP response body. \n   return   new   Response . ok ( insertedUser );  }   When getting managed objects from a database, each instance will represent one row. For example, consider the following table, and the previous example of  _User  and  User  types:     id  name      1  Bob    2  Fred     If this entire table were fetched, you'd get a  List User  as though you had written the following code:  var   users   =   [ \n   new   User () \n     .. id   =   1 \n     .. name   =   Bob , \n\n   new   User () \n     .. id   =   2 \n     .. name   =   Fred  ];   Managed objects may also declare additional properties and methods beyond those in its persistent type. Because these properties and methods are not part of the persistent type, they are  transient  - that is, their values are not stored in the database. Any method or property defined in a subclass of  ManagedObject T  is ignored when sending data to a database. This is different than properties in a persistent type, where every property explicitly maps to a database column. Here's an example:  class   Video   extends   ManagedObject _Video   implements   _Video   { \n   bool   get   isRecent   =   return   new   DateTime . now (). difference ( uploadDate ). inDays     7 ;  }  class   _Video   { \n   @ managedPrimaryKey   int   id ; \n   DateTime   uploadDate ; \n\n   /* more properties */ \n   ...  }   Each video has a persistent property that indicates when the video was uploaded. As a convenience, you'd like to be able to determine if a video is \"recent\" - that is, it has been uploaded in the last week. Adding an  isRecent  property to the persistent type doesn't make any sense, because that information can be derived from the existing upload date property. This is a good place to use a transient property.  By default, transient properties are not included when a  ManagedObject T  is written into or read from a  Map . When a  Video  is returned as JSON in an HTTP response,  isRecent  won't be in the HTTP body. However, this is just the default behavior and can easily be changed, though - see  Storage, Serialization and Deserialization  for more details.  You may also override a  ManagedObject T s  asMap()  method to get to similar behavior:  class   Video   extends   ManagedObject _Video   implements   _Video   { \n   Map String ,   dynamic   asMap ()   { \n     var   m   =   super . asMap (); \n     m [ isRecent ]   =   new   DateTime . now (). difference ( uploadDate ). inDays     7 ; \n     return   m ; \n   }  }", 
            "title": "ManagedObject"
        }, 
        {
            "location": "/db/modeling_data/#modeling-managed-object-relationships", 
            "text": "In addition to attributes, managed objects may also have properties that are other managed objects or collections of managed objects. These types of properties are called  relationships . For example, in a social network application, a user may have many posts that they have created. A user, then, should have a property that is a list of posts. This is called a 'has-many' relationship, because a user can have many posts.  A user might also have a job, so the user type should also have a property that references their job. This is called a 'has-one' relationship, because a user can only ever have one job at a time.  Relationships are also properties declared in a persistent type. In the above examples, a user would look like this:  class   User   extends   ManagedObject _User   implements   _User   {}  class   _User   {   \n   @ primaryKey   int   id ; \n   String   name ; \n\n   Job   job ; \n   ManagedSet Post   posts ;  }   If the type of a property is a  ManagedObject T  subclass - like  Job  - the relationship is has-one.  The type  ManagedSet  is what indicates that the relationship is has-many. A  ManagedSet  is a glorified  List  - it can do everything a  List  can do - but has some additional behavior to help manage relationships and build queries. The type argument must be a  ManagedObject T  subclass.  One thing to note here is that all things 'database related' are declared inside the persistent type. The persistent type declares the database table, attribute properties declare the columns the table has, and relationship properties declare relationships to other database tables.  The relationship properties in  _User  do not represent columns in a database - they represent  entire rows  in a database table. Relationships in the database are maintained by foreign key constraints. Therefore, the types  Job  and  Post  must have a column that stores the primary key of a   _User . Let's look at  Job  first:  class   Job   extends   ManagedObject _Job   implements   _Job   {}  class   _Job   { \n   @ managedPrimaryKey \n   int   id ; \n   String   title ; \n\n   @ ManagedRelationship ( # job ) \n   User   user ;  }   Job.user  is a relationship property because it is a  ManagedObject T  subclass. It is the  inverse  property of  User.job . All relationship properties must have an inverse. In other words, if a user has a job, then a job has a user. The inverse is set up by adding  ManagedRelationship  data to one of the relationship properties. The argument to  ManagedRelationship  is the name of the property on the other type.  Only one side of the relationship may have  ManagedRelationship  metadata. The side with this metadata is said to  belong to  the other side. Thus, a  Job  belongs to a  User  and a  User  has-one  Job . The property with  ManagedRelationship  metadata is represented by a foreign key column in the database. The table  _Job , then, has three columns:  id ,  title  and  user_id . The name  user_id  is generated by joining the name of the relationship property with the name of the primary key on the other object.  Setting the inverse of a has-many relationship is done in the same way, so  Post  would be declared like so:  class   Post   extends   ManagedObject _Post   implements   _Post   {}  class   _Post   { \n   @ managedPrimaryKey \n   int   id ; \n   String   text ; \n\n   @ ManagedRelationship ( # posts ) \n   User   user ;  }   The types of relationship properties must always be the instance type, not the persistent type. In other words,  User.job  is of type  Job , not  _Job .  When an application starts up, relationships are checked for integrity. This check ensures that relationships are two-sided and only one property has the  ManagedRelationship  metadata. If they do not, an exception will be thrown.  ManagedRelationship  properties are always indexed; this may change in the future to be configurable, but it will always be the default. Additionally, the column backing  ManagedRelationship  properties are unique if the other side is a 'has-one' relationship. Because the  ManagedRelationship  property is actually a foreign key column, it may also define some extra configuration parameters: a delete rule and whether or not it is required.  By making  Post.user  required, we will require that every  Post  must have a user in order to be inserted into the database. This means that a  Post  cannot exist without a user (i.e., the foreign key may not be null),  class   _Post   { \n   ... \n   @ ManagedRelationship ( # posts ,   required:   true ) \n   User   user ;  }   By changing the  Job.user  delete rule to  RelationshipDeleteRule.cascade , deleting a  User  will also delete its  Job :  class   _Job   { \n   ... \n   @ ManagedRelationship ( # job ,   onDelete:   ManagedRelationshipDeleteRule . cascade ) \n   User   user ;  }   By default, the delete rule is  ManagedRelationshipDeleteRule.nullify  (it is the least destructive action) and required is  false . If you try and set up a relationship where the  ManagedRelationship  is both  ManagedRelationshipDeleteRule.nullify  and  isRequired , you will get an exception during startup: if the foreign key column can't be null and deleting the related object would nullify the foreign key column... well, that wouldn't work.  When fetching managed objects from a database, there are rules on which relationship properties are fetched. By default, any 'has-one' or 'has-many' relationships are  not  fetched from the database:  var   query   =   new   Query User ();  var   user   =   await   query . fetchOne ();  var   userMap   =   user . asMap ();  userMap   ==   { \n   id   :   1 , \n   name   :   Bob  };   // does not contain  job  or  posts   In order to fetch these types of relationships, you must explicitly configure a  Query T  to include them, which executes a SQL join. This is covered in the  Executing Queries .  The  ManagedRelationship  property, however, will be fetched by default. But, the entire object is not fetched - only its primary key value:  var   query   =   new   Query Job ();  var   job   =   await   query . fetchOne ();  var   jobMap   =   job . asMap ();  jobMap   ==   { \n   id   :   1 , \n   title   :   Programmer , \n   user   :   { \n     id   :   1 \n   }  };   It is possible to configure a  Query T  that will fetch the full object in this case, too.", 
            "title": "Modeling Managed Object Relationships"
        }, 
        {
            "location": "/db/serialization/", 
            "text": "Storage, Serialization and Deserialization\n\n\nIn the previous chapter, you have seen that \nManagedObject\nT\ns subclasses are responsible for representing database rows and can be encoded to or decoded from formats like JSON or XML. This chapter explains the behavior of those transformations.\n\n\nManagedObject\nT\n implements \nHTTPSerializable\n so that they can read from a \nMap\n or converted to a \nMap\n. A \nManagedObject\nT\n can be passed as the body object of a \nResponse\n and bound to \nHTTPBody\n variables in \nHTTPController\n:\n\n\nclass\n \nUserController\n \nextends\n \nHTTPController\n \n{\n\n  \n@\nhttpPost\n\n  \nFuture\nResponse\n \ncreateUser\n(\n@\nHTTPBody\n()\n \nUser\n \nuser\n)\n \nasync\n \n{\n\n    \nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n      \n..\nvalues\n \n=\n \nuser\n;\n\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\nawait\n \nquery\n.\ninsert\n());\n\n  \n}\n\n\n}\n\n\n\n\n\n\nNote that \nManagedObject\nT\ns don't have anything to do with JSON, XML or some other format here. Other parts of Aqueduct manage moving data back and forth between JSON and \nMap\ns - \nManagedObject\nT\n doesn't care about the transmission format as long as its a \nMap\n or \nList\nMap\n.\n\n\nNull Behavior\n\n\nIt's important to understand how \nnull\n works when reading from or writing to a \nMap\n with a \nManagedObject\nT\n. Consider the following managed object:\n\n\nclass\n \nUser\n \nextends\n \nManagedObject\n_User\n \nimplements\n \n_User\n \n{}\n\n\nclass\n \n_User\n \n{\n\n  \n@\nmanagedPrimaryKey\n\n  \nint\n \nid\n;\n\n\n  \nString\n \nname\n;\n\n\n}\n\n\n\n\n\n\nUser\n has two properties, \nid\n and \nname\n. If we read a \nUser\n from a \nMap\n that does not contain an \nid\n key, its \nid\n will be null. If we convert \nUser\n to a \nMap\n, the key \nid\n will not be present:\n\n\nvar\n \nuserMap\n \n=\n \n{\n\n  \nname\n \n:\n \nBob\n\n\n};\n\n\n\nvar\n \nuser\n \n=\n \nnew\n \nUser\n()..\nreadFromMap\n(\nuserMap\n);\n\n\n\nuser\n.\nid\n \n==\n \nnull\n;\n \n// yup\n\n\nuser\n.\nname\n \n==\n \nBob\n;\n \n// yup\n\n\n\nvar\n \noutUserMap\n \n=\n \nuser\n.\nasMap\n();\n\n\noutUserMap\n \n==\n \n{\n\n  \nname\n \n:\n \nBob\n\n\n};\n\n\n\n\n\n\nHowever, if we read \nUser\n from a \nMap\n where the \nid\n key is the \nvalue\n null, when we transform it back to a \nMap\n the \nid\n is present and its value is null:\n\n\nvar\n \nuserMap\n \n=\n \n{\n\n  \nid\n \n:\n \nnull\n\n  \nname\n \n:\n \nBob\n\n\n};\n\n\n\nvar\n \nuser\n \n=\n \nnew\n \nUser\n()..\nreadFromMap\n(\nuserMap\n);\n\n\n\nuser\n.\nid\n \n==\n \nnull\n;\n \n// yup\n\n\nuser\n.\nname\n \n==\n \nBob\n;\n \n// yup\n\n\n\nvar\n \noutUserMap\n \n=\n \nuser\n.\nasMap\n();\n\n\noutUserMap\n \n==\n \n{\n\n  \nid\n \n:\n \nnull\n\n  \nname\n \n:\n \nBob\n\n\n};\n\n\n\n\n\n\nA \nManagedObject\nT\n like \nUser\n makes the distinction between a value that is \nnull\n and a value that it \ndoesn't have enough information for\n. A property of a \nManagedObject\nT\n can get set in three ways: it is read from a map, its setter is invoked or it is read from the database. In all three of these situations, not every property is available. This is no more obvious than when  creating a brand new instance:\n\n\nvar\n \nuser\n \n=\n \nnew\n \nUser\n();\n\n\nuser\n.\nid\n \n==\n \nnull\n;\n \n// yup\n\n\nuser\n.\nname\n \n==\n \nnull\n;\n \n// yup\n\n\n\nuser\n.\nasMap\n()\n \n==\n \n{};\n \n// yup\n\n\n\n\n\n\nA \nManagedObject\nT\n will not include keys in its \nasMap()\n if it doesn't have a value for them. The value may exist somewhere else - like in the database - but if it doesn't have it, it won't include it. This distinction is useful information for clients of Aqueduct applications.\n\n\nSo what about values that are actually \nnull\n? A property with the value \nnull\n will be included in \nasMap()\n if its been read from the database, read using \nreadFromMap()\n or explicitly assigned with a setter. The following three user objects will all have \n{\"name\": null}\n:\n\n\nvar\n \nuser1\n \n=\n \nnew\n \nUser\n()\n\n  \n..\nid\n \n=\n \n1\n\n  \n..\nname\n \n=\n \nnull\n;\n\n\n\nvar\n \nuser2\n \n=\n \nnew\n \nUser\n()..\nreadFromMap\n({\n\n  \nid\n:\n \n2\n\n  \nname\n:\n \nnull\n\n\n});\n\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nwhere\n.\nid\n \n=\n \nwhereEqualTo\n(\n3\n)\n\n  \n..\nwhere\n.\nname\n \n=\n \nwhereNull\n;\n\n\nvar\n \nuser3\n \n=\n \nawait\n \nquery\n.\nfetchOne\n();\n\n\n\n\n\n\nNote that an unset value that is returned from a getter will be \nnull\n. If using an object's values to perform some calculation, it's your job to know if the value has been fetched or not. (While \nManagedObject\nT\n.hasValueForProperty()\n checks this at runtime, that isn't a good practice.)\n\n\nOne last thing to note: if you wish to remove a value from a \nManagedObject\nT\ns storage (and likewise, its \nasMap()\n), you must use \nManagedObject\nT\n.removePropertyFromBackingMap()\n.\n\n\nIt is helpful to think of a \nManagedObject\nT\n as a proxy to a database row that may or may not exist yet, and may have less data than actually exists in the database row.\n\n\nTransient Properties and Serialization/Deserialization\n\n\nBy default, transient properties and getters - those declared in the subclass of \nManagedObject\nT\n - are \nnot\n included in the \nasMap()\n. (Setters are obviously not included, as you can't get a value from them.) To include a transient property or getter in \nasMap()\n, you may mark it with \n@managedTransientOutputAttribute\n metadata. Properties marked with this metadata will be included in \nasMap()\n if and only if they are not null. A good reason to use this feature is when you want to provide a value to the consumer of the API that is derived from persistent properties:\n\n\nclass\n \nUser\n \nextends\n \nManagedObject\n_User\n \nimplements\n \n_User\n \n{\n\n  \n@\nmanagedTransientOutputAttribute\n\n  \nString\n \nget\n \nfullName\n \n=\n \n$\nfirstName\n \n$\nlastName\n;\n\n\n}\n\n\n\nclass\n \n_User\n \n{\n\n  \nString\n \nfirstName\n;\n\n  \nString\n \nlastName\n;\n\n\n  \n...\n\n\n}\n\n\n\nvar\n \nuser\n \n=\n \nnew\n \nUser\n()\n\n  \n..\nfirstName\n \n=\n \nBob\n\n  \n..\nlastName\n \n=\n \nBoberson\n;\n\n\n\nvar\n \nmap\n \n=\n \nuser\n.\nasMap\n();\n\n\nmap\n \n==\n \n{\n\n  \nfirstName\n \n:\n \nBob\n,\n\n  \nlastName\n \n:\n \nBoberson\n,\n\n  \nfullName\n \n:\n \nBob Boberson\n\n\n};\n\n\n\n\n\n\nTransient properties may also be used as inputs when reading with \nreadFromMap()\n by marking a property with \n@managedTransientInputAttribute\n. For example, consider how to handle user passwords. A password is not stored in plain-text in the database, but they are sent in requests. Thus, a password could read from a request body, but it needs to be salted, hashed and stored in two columns in the database. An instance type could then define a password property, which automatically set the salt and hash of the password in the underlying persistent type:\n\n\nclass\n \nUser\n \nextends\n \nManagedObject\n_User\n \nimplements\n \n_User\n \n{\n\n  \n@\nmanagedTransientInputAttribute\n\n  \nvoid\n \nset\n \npassword\n(\nString\n \npw\n)\n \n{\n\n    \nsalt\n \n=\n \ngenerateSalt\n();\n\n    \nhashedPassword\n \n=\n \nhash\n(\npw\n,\n \nsalt\n);\n\n  \n}\n\n\n}\n\n\nclass\n \n_User\n \n{\n\n  \nString\n \nsalt\n;\n\n  \nString\n \nhashedPassword\n;\n\n  \n...\n\n\n}\n\n\n\nvar\n \nmap\n \n=\n \n{\n\n  \npassword\n \n:\n \nmypassword\n\n\n};\n\n\nvar\n \nuser\n \n=\n \nnew\n \nUser\n()..\nreadFromMap\n(\nmap\n);\n\n\nvar\n \nsalt\n \n=\n \nuser\n.\nsalt\n;\n \n// \nsomerandomstring\n\n\nvar\n \nhashedPassword\n \n=\n \nuser\n.\nhashedPassword\n;\n \n// \nsomehashedstring\n\n\n\nvar\n \npassword\n \n=\n \nuser\n.\npassword\n;\n \n// Analyzer error - user.password doesn\nt exist!\n\n\n\n\n\n\nOn a related note, persistent properties are always included in \nasMap()\n by default, but can be omitted by adding \nManagedColumnAttributes\n metadata with the \nomitByDefault\n option:\n\n\nclass\n \n_User\n \n{\n\n  \n@\nManagedColumnAttributes\n(\nomitByDefault:\n \ntrue\n)\n\n  \nString\n \nsalt\n;\n\n\n  \n@\nManagedColumnAttributes\n(\nomitByDefault:\n \ntrue\n)\n\n  \nString\n \nhashedPassword\n;\n\n  \n...\n\n\n}\n\n\n\n\n\n\nA transient input attribute must be a setter or a property, just like an transient output attribute must be a getter or a property. For properties that are both inputs and outputs, you may use the metadata \n@managedTransientAttribute\n.\n\n\nclass\n \nUser\n \nextends\n \nManagedObject\n_User\n \nimplements\n \n_User\n \n{\n\n  \n@\nmanagedTransientAttribute\n\n  \nString\n \nnickname\n;\n \n// shows up in asMap() and can be read from readFromMap()\n\n\n}\n\n\n\n\n\n\nAlso, a separate getter and setter may exist for the same name to allow both input and output:\n\n\nclass\n \nUser\n \nextends\n \nManagedObject\n_User\n \nimplements\n \n_User\n \n{\n\n  \n@\nmanagedTransientInputAttribute\n\n  \nvoid\n \nset\n \ntransientValue\n(\nString\n \ns\n)\n \n{\n\n    \n...\n\n  \n}\n\n\n  \n@\nmanagedTransientOutputAttribute\n\n  \nString\n \nget\n \ntransientValue\n \n=\n \n...;\n\n\n}\n\n\n\n\n\n\nSerialization and Deserialization of Relationship Properties\n\n\nRelationship properties - references to other \nManagedObject\nT\n subclasses - can also be included in \nasMap()\n and read from \nreadFromMap()\n. Relationship properties are populated when using \nQuery.join\n - aka, a SQL JOIN.\n\n\nIf a relationship property has been set or read from the database, its \nasMap()\n will contain the nested \nMap\n produced by the related objects \nasMap()\n. For example, recall the \nUser\n with a \njob\n:\n\n\nvar\n \njob\n \n=\n \nnew\n \nJob\n()\n\n  \n..\ntitle\n \n=\n \nProgrammer\n;\n\n\nvar\n \nuser\n \n=\n \nnew\n \nUser\n()\n\n  \n..\nname\n \n=\n \nBob\n\n  \n..\njob\n \n=\n \njob\n;\n\n\n\nvar\n \nuserMap\n \n=\n \nuser\n.\nasMap\n();\n\n\nuserMap\n \n==\n \n{\n\n  \nid\n:\n \n1\n,\n\n  \nname\n:\n \nBob\n,\n\n  \njob\n:\n \n{\n\n    \nid\n:\n \n1\n\n    \ntitle\n:\n \nProgrammer\n\n  \n}\n\n\n};\n \n// yup\n\n\n\n\n\n\nNotice that the names of the keys - including relationship properties and properties of the related object - all match the names of their declared properties.\n\n\nIt's important to note that \"belongs to\" relationships - those with \nManagedRelationship\n metadata - are always returned in \nasMap()\n when fetching an object from the database. However, the full object is not returned - only its primary key. Therefore, you will get the following result:\n\n\nvar\n \njobQuery\n \n=\n \nnew\n \nQuery\nJob\n();\n\n\nvar\n \njob\n \n=\n \nawait\n \njobQuery\n.\nfetchOne\n();\n\n\n\njob\n.\nasMap\n()\n \n==\n \n{\n\n  \nid\n:\n \n1\n,\n\n  \ntitle\n:\n \nProgrammer\n,\n\n  \nuser\n:\n \n{\n\n    \nid\n:\n \n1\n\n  \n}\n\n\n};\n \n// yup\n\n\n\n\n\n\nThis behavior might be different than some ORMs, which may collapse the \nuser\n into a scalar \nuser_id\n:\n\n\njob\n.\nasMap\n()\n \n==\n \n{\n\n  \nid\n:\n \n1\n,\n\n  \ntitle\n:\n \nProgrammer\n,\n\n  \nuser_id\n:\n \n1\n\n\n};\n \n// nope\n\n\n\n\n\n\nAqueduct treats relationships consistently and chooses not to expose any of the underlying database details to the API consumer. An iOS app, for example, shouldn't care - a relationship could be maintained by foreign key references or by witchcraft. The interesting piece to the API consumer is that job's have a user, and user's have a job.\n\n\n\"Has-many\" relationships, which are represented as \nManagedSet\nT\ns, are written as \nList\nMap\ns in \nasMap()\n.\n\n\nvar\n \nuser\n \n=\n \nnew\n \nUser\n()\n\n  \n..\nid\n \n=\n \n1\n;\n\n  \n..\nposts\n \n=\n \nnew\n \nManagedSet\n.\nfrom\n([\n\n      \nnew\n \nPost\n()..\nid\n \n=\n \n2\n,\n\n      \nnew\n \nPost\n()..\nid\n \n=\n \n3\n\n  \n]);\n\n\n\nvar\n \nuserMap\n \n=\n \nuser\n.\nasMap\n();\n\n\nuserMap\n \n==\n \n{\n\n  \nid\n \n:\n \n1\n,\n\n  \nposts\n \n:\n \n[\n\n    \n{\n\n      \nid\n \n:\n \n2\n\n    \n},\n\n    \n{\n\n      \nid\n \n:\n \n3\n\n    \n}\n\n  \n]\n\n\n};\n\n\n\n\n\n\nIt is important to note the potential for cyclic object graphs. Since all relationship properties are two-sided, the two properties in that relationship are references to one another. That is, you could do something like this:\n\n\nidentical\n(\nuser\n.\nprofile\n.\nuser\n,\n \nuser\n);\n\n\nidentical\n(\nuser\n.\nposts\n.\nfirst\n.\nuser\n,\n \nuser\n);\n\n\n\n\n\n\nWhen fetching objects from a database, this won't happen - Aqueduct will create multiple instances of the same row when necessary to avoid this. Therefore, the previous code snippet would not be true, but the following two statements that check the values inside those objects would be:\n\n\nuser.profile.user.id == user.id;\n\nuser.posts.first.user.id == user.id\n\n\n\n\n\nWhile managed objects from a database will not have cyclic references, managed objects you instantiate yourself can if you mistakenly do so. When you invoke \nasMap()\n on a cyclic graph, you'll get a stack overflow error. It's best to avoid creating cyclic graphs altogether. For example:\n\n\n// do:\n\n\nvar\n \nuser\n \n=\n \nnew\n \nUser\n();\n\n\nposts\n.\nforEach\n((\np\n)\n \n{\n\n  \np\n.\nuser\n \n=\n \nnew\n \nUser\n()..\nid\n \n=\n \nuser\n.\nid\n;\n\n\n});\n\n\n\n// do not:\n\n\nvar\n \nuser\n \n=\n \nnew\n \nUser\n();\n\n\nposts\n.\nforEach\n((\np\n)\n \n{\n\n  \np\n.\nuser\n \n=\n \nuser\n;\n\n\n});\n\n\n\n\n\n\nWhen reading the values of a \nManagedObject\nT\n with \nreadFromMap()\n, relationship properties must also be represented as nested \nMap\ns or \nList\nMap\n. Thus:\n\n\nvar\n \nuserMap\n \n=\n \n{\n\n  \nid\n:\n \n1\n,\n\n  \nname\n:\n \nBob\n,\n\n  \nposts\n:\n \n[\n\n    \n{\nid\n:\n \n1\n,\n \ntext\n:\n \nhello\n}\n\n  \n]\n\n\n};\n\n\n\nvar\n \nuser\n \n=\n \nnew\n \nUser\n()..\nreadFromMap\n(\nuserMap\n);\n\n\nuser\n.\nposts\n \n==\n \nnew\n \nManagedSet\nPost\n[\n\n  \nnew\n \nPost\n()\n\n    \n..\nid\n \n=\n \n1\n\n    \n..\ntext\n \n=\n \nhello\n\n\n];\n \n// yup, other Post doesn\nt implement == to check property equality", 
            "title": "Storage, Serialization and Deserialization"
        }, 
        {
            "location": "/db/serialization/#storage-serialization-and-deserialization", 
            "text": "In the previous chapter, you have seen that  ManagedObject T s subclasses are responsible for representing database rows and can be encoded to or decoded from formats like JSON or XML. This chapter explains the behavior of those transformations.  ManagedObject T  implements  HTTPSerializable  so that they can read from a  Map  or converted to a  Map . A  ManagedObject T  can be passed as the body object of a  Response  and bound to  HTTPBody  variables in  HTTPController :  class   UserController   extends   HTTPController   { \n   @ httpPost \n   Future Response   createUser ( @ HTTPBody ()   User   user )   async   { \n     var   query   =   new   Query User () \n       .. values   =   user ; \n\n     return   new   Response . ok ( await   query . insert ()); \n   }  }   Note that  ManagedObject T s don't have anything to do with JSON, XML or some other format here. Other parts of Aqueduct manage moving data back and forth between JSON and  Map s -  ManagedObject T  doesn't care about the transmission format as long as its a  Map  or  List Map .", 
            "title": "Storage, Serialization and Deserialization"
        }, 
        {
            "location": "/db/serialization/#null-behavior", 
            "text": "It's important to understand how  null  works when reading from or writing to a  Map  with a  ManagedObject T . Consider the following managed object:  class   User   extends   ManagedObject _User   implements   _User   {}  class   _User   { \n   @ managedPrimaryKey \n   int   id ; \n\n   String   name ;  }   User  has two properties,  id  and  name . If we read a  User  from a  Map  that does not contain an  id  key, its  id  will be null. If we convert  User  to a  Map , the key  id  will not be present:  var   userMap   =   { \n   name   :   Bob  };  var   user   =   new   User ().. readFromMap ( userMap );  user . id   ==   null ;   // yup  user . name   ==   Bob ;   // yup  var   outUserMap   =   user . asMap ();  outUserMap   ==   { \n   name   :   Bob  };   However, if we read  User  from a  Map  where the  id  key is the  value  null, when we transform it back to a  Map  the  id  is present and its value is null:  var   userMap   =   { \n   id   :   null \n   name   :   Bob  };  var   user   =   new   User ().. readFromMap ( userMap );  user . id   ==   null ;   // yup  user . name   ==   Bob ;   // yup  var   outUserMap   =   user . asMap ();  outUserMap   ==   { \n   id   :   null \n   name   :   Bob  };   A  ManagedObject T  like  User  makes the distinction between a value that is  null  and a value that it  doesn't have enough information for . A property of a  ManagedObject T  can get set in three ways: it is read from a map, its setter is invoked or it is read from the database. In all three of these situations, not every property is available. This is no more obvious than when  creating a brand new instance:  var   user   =   new   User ();  user . id   ==   null ;   // yup  user . name   ==   null ;   // yup  user . asMap ()   ==   {};   // yup   A  ManagedObject T  will not include keys in its  asMap()  if it doesn't have a value for them. The value may exist somewhere else - like in the database - but if it doesn't have it, it won't include it. This distinction is useful information for clients of Aqueduct applications.  So what about values that are actually  null ? A property with the value  null  will be included in  asMap()  if its been read from the database, read using  readFromMap()  or explicitly assigned with a setter. The following three user objects will all have  {\"name\": null} :  var   user1   =   new   User () \n   .. id   =   1 \n   .. name   =   null ;  var   user2   =   new   User ().. readFromMap ({ \n   id :   2 \n   name :   null  });  var   query   =   new   Query User () \n   .. where . id   =   whereEqualTo ( 3 ) \n   .. where . name   =   whereNull ;  var   user3   =   await   query . fetchOne ();   Note that an unset value that is returned from a getter will be  null . If using an object's values to perform some calculation, it's your job to know if the value has been fetched or not. (While  ManagedObject T .hasValueForProperty()  checks this at runtime, that isn't a good practice.)  One last thing to note: if you wish to remove a value from a  ManagedObject T s storage (and likewise, its  asMap() ), you must use  ManagedObject T .removePropertyFromBackingMap() .  It is helpful to think of a  ManagedObject T  as a proxy to a database row that may or may not exist yet, and may have less data than actually exists in the database row.", 
            "title": "Null Behavior"
        }, 
        {
            "location": "/db/serialization/#transient-properties-and-serializationdeserialization", 
            "text": "By default, transient properties and getters - those declared in the subclass of  ManagedObject T  - are  not  included in the  asMap() . (Setters are obviously not included, as you can't get a value from them.) To include a transient property or getter in  asMap() , you may mark it with  @managedTransientOutputAttribute  metadata. Properties marked with this metadata will be included in  asMap()  if and only if they are not null. A good reason to use this feature is when you want to provide a value to the consumer of the API that is derived from persistent properties:  class   User   extends   ManagedObject _User   implements   _User   { \n   @ managedTransientOutputAttribute \n   String   get   fullName   =   $ firstName   $ lastName ;  }  class   _User   { \n   String   firstName ; \n   String   lastName ; \n\n   ...  }  var   user   =   new   User () \n   .. firstName   =   Bob \n   .. lastName   =   Boberson ;  var   map   =   user . asMap ();  map   ==   { \n   firstName   :   Bob , \n   lastName   :   Boberson , \n   fullName   :   Bob Boberson  };   Transient properties may also be used as inputs when reading with  readFromMap()  by marking a property with  @managedTransientInputAttribute . For example, consider how to handle user passwords. A password is not stored in plain-text in the database, but they are sent in requests. Thus, a password could read from a request body, but it needs to be salted, hashed and stored in two columns in the database. An instance type could then define a password property, which automatically set the salt and hash of the password in the underlying persistent type:  class   User   extends   ManagedObject _User   implements   _User   { \n   @ managedTransientInputAttribute \n   void   set   password ( String   pw )   { \n     salt   =   generateSalt (); \n     hashedPassword   =   hash ( pw ,   salt ); \n   }  }  class   _User   { \n   String   salt ; \n   String   hashedPassword ; \n   ...  }  var   map   =   { \n   password   :   mypassword  };  var   user   =   new   User ().. readFromMap ( map );  var   salt   =   user . salt ;   //  somerandomstring  var   hashedPassword   =   user . hashedPassword ;   //  somehashedstring  var   password   =   user . password ;   // Analyzer error - user.password doesn t exist!   On a related note, persistent properties are always included in  asMap()  by default, but can be omitted by adding  ManagedColumnAttributes  metadata with the  omitByDefault  option:  class   _User   { \n   @ ManagedColumnAttributes ( omitByDefault:   true ) \n   String   salt ; \n\n   @ ManagedColumnAttributes ( omitByDefault:   true ) \n   String   hashedPassword ; \n   ...  }   A transient input attribute must be a setter or a property, just like an transient output attribute must be a getter or a property. For properties that are both inputs and outputs, you may use the metadata  @managedTransientAttribute .  class   User   extends   ManagedObject _User   implements   _User   { \n   @ managedTransientAttribute \n   String   nickname ;   // shows up in asMap() and can be read from readFromMap()  }   Also, a separate getter and setter may exist for the same name to allow both input and output:  class   User   extends   ManagedObject _User   implements   _User   { \n   @ managedTransientInputAttribute \n   void   set   transientValue ( String   s )   { \n     ... \n   } \n\n   @ managedTransientOutputAttribute \n   String   get   transientValue   =   ...;  }", 
            "title": "Transient Properties and Serialization/Deserialization"
        }, 
        {
            "location": "/db/serialization/#serialization-and-deserialization-of-relationship-properties", 
            "text": "Relationship properties - references to other  ManagedObject T  subclasses - can also be included in  asMap()  and read from  readFromMap() . Relationship properties are populated when using  Query.join  - aka, a SQL JOIN.  If a relationship property has been set or read from the database, its  asMap()  will contain the nested  Map  produced by the related objects  asMap() . For example, recall the  User  with a  job :  var   job   =   new   Job () \n   .. title   =   Programmer ;  var   user   =   new   User () \n   .. name   =   Bob \n   .. job   =   job ;  var   userMap   =   user . asMap ();  userMap   ==   { \n   id :   1 , \n   name :   Bob , \n   job :   { \n     id :   1 \n     title :   Programmer \n   }  };   // yup   Notice that the names of the keys - including relationship properties and properties of the related object - all match the names of their declared properties.  It's important to note that \"belongs to\" relationships - those with  ManagedRelationship  metadata - are always returned in  asMap()  when fetching an object from the database. However, the full object is not returned - only its primary key. Therefore, you will get the following result:  var   jobQuery   =   new   Query Job ();  var   job   =   await   jobQuery . fetchOne ();  job . asMap ()   ==   { \n   id :   1 , \n   title :   Programmer , \n   user :   { \n     id :   1 \n   }  };   // yup   This behavior might be different than some ORMs, which may collapse the  user  into a scalar  user_id :  job . asMap ()   ==   { \n   id :   1 , \n   title :   Programmer , \n   user_id :   1  };   // nope   Aqueduct treats relationships consistently and chooses not to expose any of the underlying database details to the API consumer. An iOS app, for example, shouldn't care - a relationship could be maintained by foreign key references or by witchcraft. The interesting piece to the API consumer is that job's have a user, and user's have a job.  \"Has-many\" relationships, which are represented as  ManagedSet T s, are written as  List Map s in  asMap() .  var   user   =   new   User () \n   .. id   =   1 ; \n   .. posts   =   new   ManagedSet . from ([ \n       new   Post ().. id   =   2 , \n       new   Post ().. id   =   3 \n   ]);  var   userMap   =   user . asMap ();  userMap   ==   { \n   id   :   1 , \n   posts   :   [ \n     { \n       id   :   2 \n     }, \n     { \n       id   :   3 \n     } \n   ]  };   It is important to note the potential for cyclic object graphs. Since all relationship properties are two-sided, the two properties in that relationship are references to one another. That is, you could do something like this:  identical ( user . profile . user ,   user );  identical ( user . posts . first . user ,   user );   When fetching objects from a database, this won't happen - Aqueduct will create multiple instances of the same row when necessary to avoid this. Therefore, the previous code snippet would not be true, but the following two statements that check the values inside those objects would be:  user.profile.user.id == user.id;\n\nuser.posts.first.user.id == user.id  While managed objects from a database will not have cyclic references, managed objects you instantiate yourself can if you mistakenly do so. When you invoke  asMap()  on a cyclic graph, you'll get a stack overflow error. It's best to avoid creating cyclic graphs altogether. For example:  // do:  var   user   =   new   User ();  posts . forEach (( p )   { \n   p . user   =   new   User ().. id   =   user . id ;  });  // do not:  var   user   =   new   User ();  posts . forEach (( p )   { \n   p . user   =   user ;  });   When reading the values of a  ManagedObject T  with  readFromMap() , relationship properties must also be represented as nested  Map s or  List Map . Thus:  var   userMap   =   { \n   id :   1 , \n   name :   Bob , \n   posts :   [ \n     { id :   1 ,   text :   hello } \n   ]  };  var   user   =   new   User ().. readFromMap ( userMap );  user . posts   ==   new   ManagedSet Post [ \n   new   Post () \n     .. id   =   1 \n     .. text   =   hello  ];   // yup, other Post doesn t implement == to check property equality", 
            "title": "Serialization and Deserialization of Relationship Properties"
        }, 
        {
            "location": "/db/executing_queries/", 
            "text": "Inserting, Updating, Deleting and Fetching Objects\n\n\nTo send commands to a database - whether to fetch, insert, delete or update objects - you will create, configure and execute instances of \nQuery\nT\n. The type argument must be a subclass of \nManagedObject\nT\n. This tells the \nQuery\nT\n which table it will operate on. Here's an example of a \nQuery\nT\n that fetches all instances of \nUser\n:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n();\n\n\nvar\n \nallUsers\n \n=\n \nawait\n \nquery\n.\nfetch\n();\n\n\n\n\n\n\nA \nQuery\nT\n has four basic execution methods: \nfetch\n, \nupdate\n, \ninsert\n, \ndelete\n.\n\n\n\n\nfetch\n will retrieve data from a database (it is equivalent to the SQL operation \nSELECT\n).\n\n\nupdate\n will modify existing data in a database (it is equivalent to the SQL operation \nUPDATE\n).\n\n\ninsert\n will add new data to a database (it is equivalent to the SQL operation \nINSERT\n).\n\n\ndelete\n will remove data from a database (it is equivalent to the SQL operation \nDELETE\n).\n\n\n\n\nA \nQuery\nT\n has many configurable properties. These properties will impact which objects get fetched, the data that gets sent to the database, the order that data is returned in, and so on.\n\n\nInserting Data with a Query\n\n\nLet's assume this \nUser\n type exists:\n\n\nclass\n \nUser\n \nextends\n \nManagedObject\n_User\n \nimplements\n \n_User\n \n{}\n\n\nclass\n \n_User\n \n{\n\n  \n@\nmanagedPrimaryKey\n\n  \nint\n \nid\n;\n\n\n  \n@\nManagedColumnAttributes\n(\nindexed:\n \ntrue\n)\n\n  \nString\n \nemail\n;\n\n\n  \nString\n \nname\n;\n\n\n}\n\n\n\n\n\n\nTo insert a new row into the \n_User\n table, a \nQuery\nT\n is constructed and executed:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nvalues\n.\nname\n \n=\n \nBob\n\n  \n..\nvalues\n.\nemail\n \n=\n \nbob@stablekernel.com\n;\n  \n\n\nvar\n \nuser\n \n=\n \nawait\n \nquery\n.\ninsert\n();\n  \n\nuser\n.\nasMap\n()\n \n==\n \n{\n\n  \nid\n:\n \n1\n,\n\n  \nname\n:\n \nBob\n,\n\n  \nemail\n:\n \nbob@stablekernel.com\n\n\n};\n\n\n\n\n\n\nEvery \nQuery\nT\n has a \nvalues\n property that is the type of managed object being inserted. Here, \nvalues\n is an instance of \nUser\n. When a \nQuery\nT\n is executed with \ninsert()\n, a new row is created in the database with every property that has been set in \nvalues\n. In this case, both \nname\n and \nemail\n have been set. The generated SQL looks like this:\n\n\nINSERT\n \nINTO\n \n_user\n \n(\nname\n,\n \nemail\n)\n \nVALUES\n \n(\nBob\n,\n \nbob@stablekernel.com\n)\n\n\n\n\n\n\nNote there is no value provided for the \nid\n property in this query. Recall that \nmanagedPrimaryKey\n metadata is a convenience for \nManagedColumnAttributes\n with autoincrementing behavior. Therefore, the database will assign a value for \nid\n during insertion. The object returned from \ninsert()\n will be an instance of \nUser\n that represents the inserted row and will include the auto-generated \nid\n.\n\n\nProperties that are not set in the \nvalues\n property will not be sent to the database.\n\n\nValues that are explicitly set to \nnull\n will be sent as \nNULL\n. For example, consider the following \nQuery\nT\n:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nvalues\n.\nname\n \n=\n \nnull\n;\n\n\nawait\n \nquery\n.\ninsert\n();\n\n\n\n\n\n\nThe generated SQL for this query does not send \nemail\n - because it isn't included - and sends \nNULL\n for \nname\n:\n\n\nINSERT\n \nINTO\n \n_user\n \n(\nname\n)\n \nVALUES\n \n(\nNULL\n);\n\n\n\n\n\n\nIf a property is not nullable (its \nManagedColumnAttributes\n has \nnullable: false\n) and its value is not set in a query prior to inserting it, the query will fail and throw an exception.\n\n\nYou may also set \nQuery.values\n with an instance of a managed object. This is valuable when reading an object from a JSON HTTP request body:\n\n\nvar\n \nuser\n \n=\n \nnew\n \nUser\n()\n\n  \n..\nreadFromMap\n(\nrequestBody\n);\n\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nvalues\n \n=\n \nuser\n;\n\n\n\n\n\n\nBy default, the returned object from an \ninsert()\n will have all of its properties set. See a later section on configuring which properties are returned from a \nQuery\nT\n.\n\n\nIf an insert query fails because of a conflict - a unique constraint is violated - the \nQuery\nT\n will throw a \nQueryException\n.  See a later section on how \nQueryException\ns are gracefully handled by \nRequestController\ns. In short, it is unlikely that you have to handle \nQueryException\n directly - \nRequestController\ns know how to turn them into the appropriate HTTP response.\n\n\nUpdating Data with a Query\n\n\nUpdating rows with a \nQuery\nT\n is similar to inserting data: you set the \nQuery.values\n for properties you want to change. The type parameter for the \nQuery\nT\n indicates which database table will get updated when the query is executed.\n\n\nAn update query can - and likely should - be restricted to a single row or subset of rows. This is done by configuring the \nQuery.where\n property - which gets translated into the \nwhere clause\n of the SQL command. Here's an example:\n\n\n// A Query that will change any user\ns whose name is \nBob\n to \nFred\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nvalues\n.\nname\n \n=\n \nFred\n\n  \n..\nwhere\n.\nname\n \n=\n \nwhereEqualTo\n(\nBob\n);\n\n\n\nList\nUser\n \nbobsThatAreNowFreds\n \n=\n \nawait\n \nquery\n.\nupdate\n();\n\n\n\n\n\n\nLike \nvalues\n, \nwhere\n is also the same managed object type the query is being executed on. In the above example, then, both \nvalues\n and \nwhere\n and instances of \nUser\n. This query executes the following SQL:\n\n\nUPDATE\n \n_user\n \nSET\n \nname\n=\nFred\n \nWHERE\n \nname\n=\nBob\n;\n\n\n\n\n\n\nThe \nwhere\n property is a very powerful and flexible, and so there is an entire section dedicated to it later in this guide. For now, we'll stick to some of the things specific to \nupdate()\n.\n\n\nLike \ninsert()\n, only the values set in the \nvalues\n property of a query get updated when executing \nupdate()\n. Values that are omitted are not included. Values that need to be set to \nnull\n must explicitly be set to \nnull\n in the query:\n\n\n// A Query that will remove names from anyone currently named Bob.\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nvalues\n.\nname\n \n=\n \nnull\n\n  \n..\nwhere\n.\nname\n \n=\n \nwhereEqualTo\n(\nBob\n);\n\n\n\n\n\n\nAn update query returns every modified row as a result. If no rows are updated, the return value is an empty list.  \n\n\nThere is a variant to \nQuery\nT\n.update\n named \nupdateOne\n. The \nupdateOne\n method will build and execute a SQL query in the same way a normal \nupdate\n does, however, it will only return the single instance that was updated instead of a list. This is convenience method for the caller to get back a single instance instead of a list:\n\n\n// Update user with id = 1 to have the name \nFred\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nvalues\n.\nname\n \n=\n \nFred\n\n  \n..\nwhere\n.\nid\n \n=\n \nwhereEqualTo\n(\n1\n);\n\n\n\nvar\n \nupdatedUser\n \n=\n \nawait\n \nquery\n.\nupdateOne\n();\n\n\n\n\n\n\nThe \nupdateOne\n method will return \nnull\n if no rows were updated. It is important to note that if \nupdateOne\n is used and more than one row is updated, \nupdateOne\n will throw an exception and the changes to the data \nare not reversible\n. Because this is likely a mistake, this is considered an error, hence the exception is thrown. It is up to the programmer to recognize whether or not a particular \nupdateOne\n query would impact multiple rows.\n\n\nUpdate queries have a safety feature that prevents you from accidentally updating every row. If you try to execute a \nQuery\nT\n to do an update without configuring \nwhere\n, an exception is thrown prior to carrying out the request. If you actually want to update every row of a table, you must set the \nQuery.canModifyAllInstances\n to \ntrue\n prior to execution. (This property defaults to \nfalse\n.)\n\n\nDeleting Data with a Query\n\n\nA \nQuery\nT\n will delete rows from a database when using \ndelete()\n. Like update queries, you should specify a row or rows using \nwhere\n properties of the \nQuery\nT\n. The result of a delete operation will be a \nFuture\nint\n with the number of rows deleted.\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nwhere\n.\nid\n \n=\n \nwhereEqualTo\n(\n1\n);\n\n\n\nint\n \nusersDeleted\n \n=\n \nawait\n \nquery\n.\ndelete\n();\n\n\n\n\n\n\nAlso like update queries, delete queries have a safety feature that prevents you from accidentally deleting every row in a table with \ncanModifyAllInstances\n.\n\n\nAny properties set in the query's \nvalues\n are ignored when executing a delete.\n\n\nFetching Data with a Query\n\n\nOf the four basic operations of a \nQuery\nT\n, fetching data is the most configurable. A simple \nQuery\nT\n that would fetch every instance of some entity looks like this:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n();\n\n\n\nList\nUser\n \nallUsers\n \n=\n \nawait\n \nquery\n.\nfetch\n();\n\n\n\n\n\n\nA fetch \nQuery\nT\n uses its \nwhere\n property to filter the result set, just like delete and update queries. Any properties set in the query's \nvalues\n are ignored when executing a fetch, since there is no need for them. In addition to fetching a list of instances from a database, you may also fetch a single instance with \nfetchOne\n. If no instance is found, \nnull\n is returned.\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nwhere\n.\nid\n \n=\n \nwhereEqualTo\n(\n1\n);\n\n\n\nUser\n \noneUser\n \n=\n \nawait\n \nquery\n.\nfetchOne\n();\n\n\n\n\n\n\nFetch queries can be limited to a number of instances with the \nfetchLimit\n property. You may also set the \noffset\n of a \nQuery\nT\n to skip the first \noffset\n number of rows. Between \nfetchLimit\n and \noffset\n, you can implement naive paging. However, this type of paging suffers from a number of problems and so there is another paging mechanism covered in later sections.\n\n\nMany of the other fantastic things you can do with fetch queries - like joins, sorting and complex predicates - all deserve their own section and are covered later.\n\n\nSpecifying Result Properties\n\n\nWhen executing queries that return managed objects (i.e., \ninsert()\n, \nupdate()\n and \nfetch()\n), the default properties for each object are fetched. The default properties of a managed object are properties that correspond to a database column - attributes declared in the persistent type. A managed object's default properties can be modified when declaring its persistent type:\n\n\nclass\n \n_User\n \n{\n\n  \n@\nManagedColumnAttributes\n(\nomitByDefault:\n \ntrue\n)\n\n  \nString\n \nhashedPassword\n;\n\n\n}\n\n\n\n\n\n\nAny property with \nomitByDefault\n set to true will not be fetched by default.\n\n\nA property that is \nomitByDefault\n can still be fetched. Likewise, a property that is in the defaults can still be omitted. Each \nQuery\nT\n has a \nreturningProperties\n method to adjust which properties do get returned from the query. Its usage looks like this:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nreturningProperties\n((\nuser\n)\n \n=\n \n[\nuser\n.\nid\n,\n \nuser\n.\nname\n]);\n\n\n\n\n\n\nThe method \nreturningProperties\n takes a closure with one argument - an instance of the type being queried. The closure must return a \nList\n of properties to be fetched. Here, both \nuser.id\n and \nuser.name\n are returned and this \nQuery\nT\n will fetch a user's \nid\n and \nname\n properties only. (The SQL would be something like \nSELECT id, name FROM _User\n.) Note that the properties returned from this closure \nare not\n added to the list of default properties - the list is an exact set of properties to be returned.\n\n\nThe way \nreturningProperties\n is constructed is a little interesting. The benefit of this approach is best explained by comparing it to another approach:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nreturningProperties\n \n=\n \n[\nid\n,\n \nname\n];\n \n// This code is not valid!\n\n\n\n\n\n\nIn the above approach - which is not valid code - the names of the properties are \nString\ns. The drawback here is that there is no way for the analyzer to tell us if \nid\n and \nname\n are actually properties of a \nUser\n or if we misspelled one of the properties. We'd only find out at runtime. Additionally, we get the benefit of code completion and refactoring tools when using the closure approach. Many other features of \nQuery\nT\n like joins, paging and sorting use a similar construct to identify which properties are being used in the query.\n\n\nYou may not add a 'has-many' or 'has-one' relationship to \nreturningProperties\n, as this mechanism is achieved by \nQuery.join\n. If you do add a 'has-one' or 'has-many' relationship property name to the list of \nreturningProperties\n, an exception will be thrown when the query is executed.\n\n\nNote that if you omit the primary key of a managed object from \nreturningProperties\n, it will automatically be added. The primary key is necessary to transform the rows into instances of their \nManagedObject\nT\n subclass.\n\n\nSorting\n\n\nResults of a fetch can be sorted using the \nsortBy\n method of a \nQuery\nT\n. Here's an example:\n\n\nvar\n \nq\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nsortBy\n((\nu\n)\n \n=\n \nu\n.\ndateCreated\n,\n \nQuerySortOrder\n.\nascending\n);\n\n\n\n\n\n\nsortBy\n takes two arguments: a closure that returns which property to sort by and the order of the sort.\n\n\nA \nQuery\nT\n results can be sorted by multiple properties. When multiple \nsortBy\ns are invoked on a \nQuery\nT\n, later \nsortBy\ns are used to break ties in previous \nsortBy\ns. For example, the following query will sort by last name, then by first name:\n\n\nvar\n \nq\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nsortBy\n((\nu\n)\n \n=\n \nu\n.\nlastName\n,\n \nQuerySortOrder\n.\nascending\n)\n\n  \n..\nsortBy\n((\nu\n)\n \n=\n \nu\n.\nfirstName\n,\n \nQuerySortOrder\n.\nascending\n);\n\n\n\n\n\n\nThus, the following three names would be ordered like so: 'Sally Smith', 'John Wu', 'Sally Wu'.\n\n\nExceptions and Errors\n\n\nAn exception encountered in preparing or executing a query will throw a \nQueryException\n. \nRequestController\ns, by default, will interpret the event of a \nQueryException\n to return a \nResponse\n to an HTTP client. For common scenarios Aqueduct will return a reasonable status code to the requesting HTTP client. Therefore, you do not have to catch query exceptions unless you wish to override the suggested status code.\n\n\nStatement Reuse\n\n\nAqueduct will parameterize and reuse queries when possible. This allows for significant speed and security improvements. Note that you do not have to do anything special to take advantage of this feature. However, currently at this time, you may not disable this feature.", 
            "title": "Executing Queries"
        }, 
        {
            "location": "/db/executing_queries/#inserting-updating-deleting-and-fetching-objects", 
            "text": "To send commands to a database - whether to fetch, insert, delete or update objects - you will create, configure and execute instances of  Query T . The type argument must be a subclass of  ManagedObject T . This tells the  Query T  which table it will operate on. Here's an example of a  Query T  that fetches all instances of  User :  var   query   =   new   Query User ();  var   allUsers   =   await   query . fetch ();   A  Query T  has four basic execution methods:  fetch ,  update ,  insert ,  delete .   fetch  will retrieve data from a database (it is equivalent to the SQL operation  SELECT ).  update  will modify existing data in a database (it is equivalent to the SQL operation  UPDATE ).  insert  will add new data to a database (it is equivalent to the SQL operation  INSERT ).  delete  will remove data from a database (it is equivalent to the SQL operation  DELETE ).   A  Query T  has many configurable properties. These properties will impact which objects get fetched, the data that gets sent to the database, the order that data is returned in, and so on.", 
            "title": "Inserting, Updating, Deleting and Fetching Objects"
        }, 
        {
            "location": "/db/executing_queries/#inserting-data-with-a-query", 
            "text": "Let's assume this  User  type exists:  class   User   extends   ManagedObject _User   implements   _User   {}  class   _User   { \n   @ managedPrimaryKey \n   int   id ; \n\n   @ ManagedColumnAttributes ( indexed:   true ) \n   String   email ; \n\n   String   name ;  }   To insert a new row into the  _User  table, a  Query T  is constructed and executed:  var   query   =   new   Query User () \n   .. values . name   =   Bob \n   .. values . email   =   bob@stablekernel.com ;    var   user   =   await   query . insert ();    user . asMap ()   ==   { \n   id :   1 , \n   name :   Bob , \n   email :   bob@stablekernel.com  };   Every  Query T  has a  values  property that is the type of managed object being inserted. Here,  values  is an instance of  User . When a  Query T  is executed with  insert() , a new row is created in the database with every property that has been set in  values . In this case, both  name  and  email  have been set. The generated SQL looks like this:  INSERT   INTO   _user   ( name ,   email )   VALUES   ( Bob ,   bob@stablekernel.com )   Note there is no value provided for the  id  property in this query. Recall that  managedPrimaryKey  metadata is a convenience for  ManagedColumnAttributes  with autoincrementing behavior. Therefore, the database will assign a value for  id  during insertion. The object returned from  insert()  will be an instance of  User  that represents the inserted row and will include the auto-generated  id .  Properties that are not set in the  values  property will not be sent to the database.  Values that are explicitly set to  null  will be sent as  NULL . For example, consider the following  Query T :  var   query   =   new   Query User () \n   .. values . name   =   null ;  await   query . insert ();   The generated SQL for this query does not send  email  - because it isn't included - and sends  NULL  for  name :  INSERT   INTO   _user   ( name )   VALUES   ( NULL );   If a property is not nullable (its  ManagedColumnAttributes  has  nullable: false ) and its value is not set in a query prior to inserting it, the query will fail and throw an exception.  You may also set  Query.values  with an instance of a managed object. This is valuable when reading an object from a JSON HTTP request body:  var   user   =   new   User () \n   .. readFromMap ( requestBody );  var   query   =   new   Query User () \n   .. values   =   user ;   By default, the returned object from an  insert()  will have all of its properties set. See a later section on configuring which properties are returned from a  Query T .  If an insert query fails because of a conflict - a unique constraint is violated - the  Query T  will throw a  QueryException .  See a later section on how  QueryException s are gracefully handled by  RequestController s. In short, it is unlikely that you have to handle  QueryException  directly -  RequestController s know how to turn them into the appropriate HTTP response.", 
            "title": "Inserting Data with a Query"
        }, 
        {
            "location": "/db/executing_queries/#updating-data-with-a-query", 
            "text": "Updating rows with a  Query T  is similar to inserting data: you set the  Query.values  for properties you want to change. The type parameter for the  Query T  indicates which database table will get updated when the query is executed.  An update query can - and likely should - be restricted to a single row or subset of rows. This is done by configuring the  Query.where  property - which gets translated into the  where clause  of the SQL command. Here's an example:  // A Query that will change any user s whose name is  Bob  to  Fred  var   query   =   new   Query User () \n   .. values . name   =   Fred \n   .. where . name   =   whereEqualTo ( Bob );  List User   bobsThatAreNowFreds   =   await   query . update ();   Like  values ,  where  is also the same managed object type the query is being executed on. In the above example, then, both  values  and  where  and instances of  User . This query executes the following SQL:  UPDATE   _user   SET   name = Fred   WHERE   name = Bob ;   The  where  property is a very powerful and flexible, and so there is an entire section dedicated to it later in this guide. For now, we'll stick to some of the things specific to  update() .  Like  insert() , only the values set in the  values  property of a query get updated when executing  update() . Values that are omitted are not included. Values that need to be set to  null  must explicitly be set to  null  in the query:  // A Query that will remove names from anyone currently named Bob.  var   query   =   new   Query User () \n   .. values . name   =   null \n   .. where . name   =   whereEqualTo ( Bob );   An update query returns every modified row as a result. If no rows are updated, the return value is an empty list.    There is a variant to  Query T .update  named  updateOne . The  updateOne  method will build and execute a SQL query in the same way a normal  update  does, however, it will only return the single instance that was updated instead of a list. This is convenience method for the caller to get back a single instance instead of a list:  // Update user with id = 1 to have the name  Fred  var   query   =   new   Query User () \n   .. values . name   =   Fred \n   .. where . id   =   whereEqualTo ( 1 );  var   updatedUser   =   await   query . updateOne ();   The  updateOne  method will return  null  if no rows were updated. It is important to note that if  updateOne  is used and more than one row is updated,  updateOne  will throw an exception and the changes to the data  are not reversible . Because this is likely a mistake, this is considered an error, hence the exception is thrown. It is up to the programmer to recognize whether or not a particular  updateOne  query would impact multiple rows.  Update queries have a safety feature that prevents you from accidentally updating every row. If you try to execute a  Query T  to do an update without configuring  where , an exception is thrown prior to carrying out the request. If you actually want to update every row of a table, you must set the  Query.canModifyAllInstances  to  true  prior to execution. (This property defaults to  false .)", 
            "title": "Updating Data with a Query"
        }, 
        {
            "location": "/db/executing_queries/#deleting-data-with-a-query", 
            "text": "A  Query T  will delete rows from a database when using  delete() . Like update queries, you should specify a row or rows using  where  properties of the  Query T . The result of a delete operation will be a  Future int  with the number of rows deleted.  var   query   =   new   Query User () \n   .. where . id   =   whereEqualTo ( 1 );  int   usersDeleted   =   await   query . delete ();   Also like update queries, delete queries have a safety feature that prevents you from accidentally deleting every row in a table with  canModifyAllInstances .  Any properties set in the query's  values  are ignored when executing a delete.", 
            "title": "Deleting Data with a Query"
        }, 
        {
            "location": "/db/executing_queries/#fetching-data-with-a-query", 
            "text": "Of the four basic operations of a  Query T , fetching data is the most configurable. A simple  Query T  that would fetch every instance of some entity looks like this:  var   query   =   new   Query User ();  List User   allUsers   =   await   query . fetch ();   A fetch  Query T  uses its  where  property to filter the result set, just like delete and update queries. Any properties set in the query's  values  are ignored when executing a fetch, since there is no need for them. In addition to fetching a list of instances from a database, you may also fetch a single instance with  fetchOne . If no instance is found,  null  is returned.  var   query   =   new   Query User () \n   .. where . id   =   whereEqualTo ( 1 );  User   oneUser   =   await   query . fetchOne ();   Fetch queries can be limited to a number of instances with the  fetchLimit  property. You may also set the  offset  of a  Query T  to skip the first  offset  number of rows. Between  fetchLimit  and  offset , you can implement naive paging. However, this type of paging suffers from a number of problems and so there is another paging mechanism covered in later sections.  Many of the other fantastic things you can do with fetch queries - like joins, sorting and complex predicates - all deserve their own section and are covered later.", 
            "title": "Fetching Data with a Query"
        }, 
        {
            "location": "/db/executing_queries/#specifying-result-properties", 
            "text": "When executing queries that return managed objects (i.e.,  insert() ,  update()  and  fetch() ), the default properties for each object are fetched. The default properties of a managed object are properties that correspond to a database column - attributes declared in the persistent type. A managed object's default properties can be modified when declaring its persistent type:  class   _User   { \n   @ ManagedColumnAttributes ( omitByDefault:   true ) \n   String   hashedPassword ;  }   Any property with  omitByDefault  set to true will not be fetched by default.  A property that is  omitByDefault  can still be fetched. Likewise, a property that is in the defaults can still be omitted. Each  Query T  has a  returningProperties  method to adjust which properties do get returned from the query. Its usage looks like this:  var   query   =   new   Query User () \n   .. returningProperties (( user )   =   [ user . id ,   user . name ]);   The method  returningProperties  takes a closure with one argument - an instance of the type being queried. The closure must return a  List  of properties to be fetched. Here, both  user.id  and  user.name  are returned and this  Query T  will fetch a user's  id  and  name  properties only. (The SQL would be something like  SELECT id, name FROM _User .) Note that the properties returned from this closure  are not  added to the list of default properties - the list is an exact set of properties to be returned.  The way  returningProperties  is constructed is a little interesting. The benefit of this approach is best explained by comparing it to another approach:  var   query   =   new   Query User () \n   .. returningProperties   =   [ id ,   name ];   // This code is not valid!   In the above approach - which is not valid code - the names of the properties are  String s. The drawback here is that there is no way for the analyzer to tell us if  id  and  name  are actually properties of a  User  or if we misspelled one of the properties. We'd only find out at runtime. Additionally, we get the benefit of code completion and refactoring tools when using the closure approach. Many other features of  Query T  like joins, paging and sorting use a similar construct to identify which properties are being used in the query.  You may not add a 'has-many' or 'has-one' relationship to  returningProperties , as this mechanism is achieved by  Query.join . If you do add a 'has-one' or 'has-many' relationship property name to the list of  returningProperties , an exception will be thrown when the query is executed.  Note that if you omit the primary key of a managed object from  returningProperties , it will automatically be added. The primary key is necessary to transform the rows into instances of their  ManagedObject T  subclass.", 
            "title": "Specifying Result Properties"
        }, 
        {
            "location": "/db/executing_queries/#sorting", 
            "text": "Results of a fetch can be sorted using the  sortBy  method of a  Query T . Here's an example:  var   q   =   new   Query User () \n   .. sortBy (( u )   =   u . dateCreated ,   QuerySortOrder . ascending );   sortBy  takes two arguments: a closure that returns which property to sort by and the order of the sort.  A  Query T  results can be sorted by multiple properties. When multiple  sortBy s are invoked on a  Query T , later  sortBy s are used to break ties in previous  sortBy s. For example, the following query will sort by last name, then by first name:  var   q   =   new   Query User () \n   .. sortBy (( u )   =   u . lastName ,   QuerySortOrder . ascending ) \n   .. sortBy (( u )   =   u . firstName ,   QuerySortOrder . ascending );   Thus, the following three names would be ordered like so: 'Sally Smith', 'John Wu', 'Sally Wu'.", 
            "title": "Sorting"
        }, 
        {
            "location": "/db/executing_queries/#exceptions-and-errors", 
            "text": "An exception encountered in preparing or executing a query will throw a  QueryException .  RequestController s, by default, will interpret the event of a  QueryException  to return a  Response  to an HTTP client. For common scenarios Aqueduct will return a reasonable status code to the requesting HTTP client. Therefore, you do not have to catch query exceptions unless you wish to override the suggested status code.", 
            "title": "Exceptions and Errors"
        }, 
        {
            "location": "/db/executing_queries/#statement-reuse", 
            "text": "Aqueduct will parameterize and reuse queries when possible. This allows for significant speed and security improvements. Note that you do not have to do anything special to take advantage of this feature. However, currently at this time, you may not disable this feature.", 
            "title": "Statement Reuse"
        }, 
        {
            "location": "/db/advanced_queries/", 
            "text": "Advanced Queries: Filtering, Joins, Paging and Reduce\n\n\nPaging Fetched Result Sets\n\n\nIn larger data sets, it may make sense to only return a portion of rows from a database. For example, in a social media application, a user could have thousands of pieces of content they had created over many years. The likely use case for fetching their content would be to grab only the most recent content, and only grab earlier content as necessary. Aqueduct has two mechanisms in \nQuery\nT\n for building queries that can fetch a subset of rows within a certain range.\n\n\nNaive paging can be accomplished using the \nfetchLimit\n and \noffset\n properties of a \nQuery\nT\n. For example, if a table contains 100 rows, and you would like to grab 10 at a time, each query would have a value of 10 for its \nfetchLimit\n. The first query would have an \noffset\n of 0, then 10, then 20, and so on. Especially when using \nsortBy\n, this type of paging can be effective. One of the drawbacks to this type of paging is that it can skip or duplicate rows if rows are being added or deleted between fetches.\n\n\n\n\nFor example, consider the seven objects above that are ordered by time. If we page by two objects at a time (\nfetchLimit=2\n) starting at the first item (\noffset=0\n), our first result set is the first two objects. The next page is the original offset plus the same limit - we grab the next two rows. But before the next page is fetched, a new object is inserted and its at an index that we already fetched. The next page would return \n3:00pm\n again. A similar problem occurs if a row is deleted when paging in this way.\n\n\nIt is really annoying for client applications to have to check for and merge duplicates. Another paging technique that doesn't suffer from this problem relies on the client sending a value from the last object in the previous page, instead of an offset. So in the above example, instead of asking for offset 2 in the second query, it'd send the value \n1:30pm\n. The query filters out rows with a value less than the one it was sent, orders the remaining rows and then fetches the newest from the top.\n\n\nQuery.pageBy\n uses this technique. Its usage is similar to \nsortBy\n:\n\n\nvar\n \nfirstQuery\n \n=\n \nnew\n \nQuery\nPost\n()\n\n  \n..\npageBy\n((\np\n)\n \n=\n \np\n.\ndateCreated\n,\n \nQuerySortOrder\n.\ndescending\n)\n\n  \n..\nfetchLimit\n \n=\n \n10\n;\n\n\n\nvar\n \nfirstQueryResults\n \n=\n \nawait\n \nfirstQuery\n.\nfetch\n();\n\n\n\nvar\n \noldestPostWeGot\n \n=\n \nfirstQueryResults\n.\nlast\n.\ndateCreated\n;\n\n\nvar\n \nnextQuery\n \n=\n \nnew\n \nQuery\nPost\n()\n\n  \n..\npageBy\n((\np\n)\n \n=\n \np\n.\ndateCreated\n,\n \nQuerySortOrder\n.\ndescending\n,\n \nboundingValue:\n \noldestPostWeGot\n)\n\n  \n..\nfetchLimit\n \n=\n \n10\n;\n\n\n\n\n\n\nThis query would fetch the newest 10 posts. Then, it fetches the next 10 after skipping past all of the ones newer than the oldest post it got in the first result set.\n\n\nWhen paging, the query must have a \nfetchLimit\n - otherwise you're just sorting and returning every row. The \npageBy\n method takes a closure to identify which property is being used to sort the rows. The closure is passed an instance of \nPost\n and it returns one of its properties. (This pattern of using a closure to identify a property like this is common to all of the advanced querying methods and is described elsewhere in this document.) The second argument to \npageBy\n defines the order the rows will be sorted in.\n\n\nWhen you first start paging, you don't have any results yet, so you can't send a value from the last result set. In this case, the \nboundingValue\n of \npageBy\n is null - meaning start from the beginning. Once the first set has been fetched, the \nboundingValue\n is the value of the paging property in the last object returned.\n\n\nThis is often accomplished by adding a query parameter to an endpoint that takes in a bounding value. (See \nManagedObjectController\nT\n as an example.)\n\n\nA \npageBy\n query will return an empty list of objects when no more values are left. If the number of objects remaining in the last page are less than the \nfetchLimit\n, only those objects will be returned. For example, if there four more objects left and the \nfetchLimit\n is 10, the number of objects returned will be four.\n\n\nYou should index properties that will be paged by:\n\n\n@\nManagedColumnAttributes\n(\nindexed:\n \ntrue\n)\n\n\nint\n \npageableProperty\n;\n\n\n\n\n\n\nFiltering Results of a Fetch Operation\n\n\nFetching every row of a table usually doesn't make sense. Instead, we want a specific object or a set of objects matching some condition.\n\n\nQuery.where\n is a safe and  elegant way to build a query. The \nwhere\n property allows you to assign \nmatchers\n to the properties of a \nManagedObject\nT\n. A matcher applies a condition - like equal to or less than - to the property it is assigned to. (This follows the same Hamcrest matcher style that the Dart test framework uses.)\n\n\nQuery.where\n is the same type as the object being fetched. For each property of \nwhere\n that is assigned a matcher, an expression will be added to the SQL where clause. Here's an example of a query that finds a \nUser\n with an \nid\n equal to 1:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nwhere\n.\nid\n \n=\n \nwhereEqualTo\n(\n1\n);\n\n\n\n\n\n\n(The generated SQL here would be 'SELECT _user.id, _user.name, ... FROM _user WHERE _user.id = 1'.)\n\n\nAll matchers begins with the word \nwhere\n. Other examples are \nwhereGreaterThan\n, \nwhereBetween\n, and \nwhereIn\n. Every matcher set on a \nwhere\n is combined using logical 'and'. In other words, the following query will find all users whose \nname\n is \"Bob\" \nand\n \nemail\n is not null:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nwhere\n.\nid\n \n=\n \nwhereEqualTo\n(\nBob\n)\n\n  \n..\nwhere\n.\nemail\n \n=\n \nwhereNotNull\n;\n\n\n\n\n\n\nThere are \nwhere\n methods for other operators and string comparisons, see the API reference for more.\n\n\nRelationship properties can be have matchers, too. For example, the following query will fetch all parents who have children that are less than 10 years old:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nParent\n()\n\n  \n..\nwhere\n.\nchildren\n.\nhaveAtLeastOneWhere\n.\nage\n \n=\n \nwhereLessThan\n(\n10\n);\n\n\n\n\n\n\nWhen building \nwhere\n with relationship properties, there are some important things to understand. First, the values for any relationship properties are not returned in the results. In the previous query, that means that a list of \nParent\ns would be returned - but their \nchildren\n property wouldn't be populated. (To actually include relationship values, the next section talks about \njoin\n.)\n\n\nMost matchers applied to a relationship property will incur a SQL join, which can be more expensive than a typical fetch. The only time a relationship matcher doesn't incur a SQL join is when matching the value of a foreign key column. That is, a belongs-to relationship property where we're only checking the primary key of the related object. There are two ways of doing this:\n\n\nvar\n \npreferredQuery\n \n=\n \nnew\n \nQuery\nChild\n()\n\n  \n..\nwhere\n.\nparent\n \n=\n \nwhereRelatedByValue\n(\n23\n);\n\n\n\nvar\n \nsameQuery\n \n=\n \nnew\n \nQuery\nChild\n()\n\n  \n..\nwhere\n.\nparent\n.\nid\n \n=\n \nwhereEqualTo\n(\n23\n);\n\n\n\n\n\n\nThe \nwhereRelatedByValue\n approach is preferred because it's clear to the reader what's happening. A query can be filtered by whether or not it has a value for its relationships. For example, the following queries return people with and without children:\n\n\nvar\n \npeopleWithoutChildren\n \n=\n \nnew\n \nQuery\nPerson\n()\n\n  \n..\nwhere\n.\nchildren\n \n=\n \nwhereNull\n;\n\n\n\nvar\n \npeopleWithChildren\n \n=\n \nnew\n \nQuery\nPerson\n()\n\n  \n..\nwhere\n.\nchildren\n \n=\n \nwhereNotNull\n;\n\n\n\n\n\n\nThe only matchers that can be applied directly to a relationship property are the three shown in these examples: \nwhereRelatedByValue\n, \nwhereNull\n and \nwhereNotNull\n. Properties of a relationship property, i.e. \nwhere.parent.age = whereGreaterThan(40)\n, don't have these restrictions.\n\n\nYou can access relationship properties of relationships, too. The following would fetch every child whose parent is a doctor.\n\n\nvar\n \nchildrenWithDoctorParents\n \n=\n \nnew\n \nQuery\nChild\n()\n\n  \n..\nwhere\n.\nparent\n.\njob\n.\ntitle\n \n=\n \nwhereEqualTo\n(\nDoctor\n);\n\n\n\n\n\n\nWhen assigning matchers to the properties of has-many relationships, you may use the \nhaveAtLeastOneWhere\n property. For example, the following returns all parents who have at least one child that is under 10 years old - but they could have other children that are not:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nParent\n()\n\n  \n..\nwhere\n.\nchildren\n.\nhaveAtLeastOneWhere\n.\nage\n \n=\n \nwhereLessThan\n(\n10\n);\n\n\n\n\n\n\nThe filter is applied to the returned \nParent\ns - if a parent doesn't have a child that is younger than 10, it will be removed from the result set. If just one of a parent's children is less than 10, it will be included. No children are fetched, either.\n\n\nIncluding Relationships in a Fetch (aka, Joins)\n\n\nA \nQuery\nT\n can also fetch relationship properties. This allows queries to fetch entire model graphs and reduces the number of round-trips to a database. (This type of fetch will execute a SQL LEFT OUTER JOIN.)\n\n\nBy default, relationship properties are not fetched in a query and therefore aren't included in an object's \nasMap()\n. For example, consider the following two \nManagedObject\nT\ns, where a \nUser\n has-many \nTask\ns:\n\n\nclass\n \nUser\n \nextends\n \nManagedObject\n_User\n \nimplements\n \n_User\n \n{}\n\n\nclass\n \n_User\n \n{\n\n  \n@\nmanagedPrimaryKey\n \nint\n \nid\n;\n\n\n  \nString\n \nname\n;\n\n  \nManagedSet\nTask\n \ntasks\n;\n  \n\n}\n\n\n\nclass\n \nTask\n \nextends\n \nManagedObject\n_Task\n \nimplements\n \n_Task\n \n{}\n\n\nclass\n \n_Task\n \n{\n\n  \n@\nmanagedPrimaryKey\n \nint\n \nid\n;\n\n\n  \n@\nManagedColumnAttributes\n(\n#\ntasks\n)\n\n  \nUser\n \nuser\n;\n\n\n  \nString\n \ncontents\n;\n\n\n}\n\n\n\n\n\n\nA \nQuery\nUser\n will fetch the \nname\n and \nid\n of each \nUser\n. A \nUser\n's \ntasks\n are not fetched, so the data returned looks like this:\n\n\nvar\n \nq\n \n=\n \nnew\n \nQuery\nUser\n();\n\n\nvar\n \nusers\n \n=\n \nawait\n \nq\n.\nfetch\n();\n\n\n\nusers\n.\nfirst\n.\nasMap\n()\n \n==\n \n{\n\n  \nid\n:\n \n1\n,\n\n  \nname\n:\n \nBob\n\n\n};\n \n// yup\n\n\n\n\n\n\nThe method \njoin()\n will tell a \nQuery\nT\n to also include a particular has-many relationship, here, a user's \ntasks\n:\n\n\nvar\n \nq\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\njoin\n(\nset\n:\n \n(\nu\n)\n \n=\n \nu\n.\ntasks\n);\n\n\nvar\n \nusers\n \n=\n \nawait\n \nq\n.\nfetch\n();\n\n\n\nusers\n.\nfirst\n.\nasMap\n()\n \n==\n \n{\n\n  \nid\n:\n \n1\n,\n\n  \nname\n:\n \nBob\n,\n\n  \ntasks\n:\n \n[\n\n      \n{\nid\n:\n \n1\n,\n \ncontents\n:\n \nTake out trash\n,\n \nuser\n \n:\n \n{\nid\n:\n \n1\n}},\n\n      \n...\n\n  \n]\n\n\n};\n \n// yup\n\n\n\n\n\n\nNotice that the \ntasks\n are in fact included in this query.  When joining a has-many relationship, the \nset:\n argument is given a closure that returns a \nManagedSet\nT\n property of the type being queried.\n\n\nThe method \njoin()\n actually returns a new \nQuery\nT\n, where \nT\n is the type of object in the relationship property. That is, the above code could also be written as such:\n\n\nvar\n \nq\n \n=\n \nnew\n \nQuery\nUser\n();\n\n\n\n// type annotation added for clarity\n\n\nQuery\nTask\n \ntaskSubQuery\n \n=\n \nq\n.\njoin\n(\nset\n:\n \n(\nu\n)\n \n=\n \nu\n.\ntasks\n);\n\n\n\n\n\n\nJust like any other \nQuery\nT\n, the set of returning properties can be modified through \nreturningProperties\n:\n\n\nvar\n \nq\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nreturningProperties\n((\nu\n)\n \n=\n \n[\nu\n.\nid\n,\n \nu\n.\nname\n]);\n\n\n\nq\n.\njoin\n(\nset\n:\n \n(\nu\n)\n \n=\n \nu\n.\ntasks\n)\n  \n  \n..\nreturningProperties\n((\nt\n)\n \n=\n \n[\nt\n.\nid\n,\n \nt\n.\ncontents\n]);\n\n\n\n\n\n\nWhen joining on a has-one or a belongs-to relationship, use \njoin(object:)\n instead of \njoin(set:)\n:\n\n\nvar\n \nq\n \n=\n \nnew\n \nQuery\nTask\n()\n\n  \n..\njoin\n(\nobject:\n \n(\nt\n)\n \n=\n \nt\n.\nuser\n);\n\n\nvar\n \nresults\n \n=\n \nawait\n \nq\n.\nfetch\n();\n\n\n\nresults\n.\nfirst\n.\nasMap\n()\n \n==\n \n{\n\n  \nid\n:\n \n1\n,\n\n  \ncontents\n:\n \nTake out trash\n,\n\n  \nuser\n:\n \n{\n\n    \nid\n:\n \n1\n,\n\n    \nname\n:\n \nBob\n\n  \n}\n\n\n};\n \n// yup\n\n\n\n\n\n\nNotice that the results of this query include all of the details for a \nTask.user\n - not just its \nid\n.\n\n\nA subquery created through \njoin\n can also be filtered through its \nwhere\n property. For example, the following query would return user's named 'Bob' and their overdue tasks only:\n\n\nvar\n \nq\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nwhere\n.\nname\n \n=\n \nwhereEquals\n(\nBob\n);\n\n\n\nq\n.\njoin\n(\nset\n:\n \n(\nu\n)\n \n=\n \nu\n.\ntasks\n)\n  \n  \n..\nwhere\n.\noverdue\n \n=\n \nwhereEqualTo\n(\ntrue\n);\n\n\n\n\n\n\nNote that the \nwhere\n property on the subquery is an instance of \nTask\n, whereas \nwhere\n on the \nUser\n query is \nUser\n. More on that in a bit.\n\n\nMore than one join can be applied to a query, and subqueries can be nested. So, this is all valid, assuming the relationship properties exist:\n\n\nvar\n \nq\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\njoin\n(\nobject:\n \n(\nu\n)\n \n=\n \nu\n.\naddress\n);\n\n\n\nq\n.\njoin\n(\nset\n:\n \n(\nu\n)\n \n=\n \nu\n.\ntasks\n)\n\n  \n..\njoin\n(\nobject:\n \n(\nu\n)\n \n=\n \nu\n.\nlocation\n);\n\n\n\n\n\n\nThis would fetch all users, their addresses, all of their tasks, and the location for each of their tasks. You'd get a nice sized tree of objects here.\n\n\nIt's important to understand how objects are filtered when using \nwhere\n and subqueries. Matchers applied to the top-level query will filter out those types of objects. A \nwhere\n on a subquery has no impact on the number of objects returned at the top-level.\n\n\nLet's say there were 10 total users, each with 10 total tasks. The following query returns all 10 user objects, but each user's \ntasks\n would only contains those that are overdue. So a user might have 0, 1, or 10 tasks returned - even though there are a total of 10 available.\n\n\nvar\n \nq\n \n=\n \nnew\n \nQuery\nUser\n();\n\n\n\nq\n.\njoin\n(\nset\n:\n \n(\nu\n)\n \n=\n \nu\n.\ntasks\n)\n\n  \n..\nwhere\n.\noverdue\n \n=\n \nwhereEqualTo\n(\ntrue\n);\n\n\n\n\n\n\nHowever, the following query would return less than 10 users, but for each user returned, they would have all 10 of their tasks:\n\n\nvar\n \nq\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nwhere\n.\nname\n \n=\n \nwhereEqualTo\n(\nBob\n)\n\n  \n..\njoin\n(\nset\n:\n \n(\nu\n)\n \n=\n \nu\n.\ntasks\n);\n\n\n\n\n\n\nNote that a query will always fetch the primary key of all objects, even if it is omitted in \nreturningProperties\n.\n\n\nReduce Functions (aka, Aggregate Functions)\n\n\nQueries can also be used to perform functions like \ncount\n, \nsum\n, \naverage\n, \nmin\n and \nmax\n. Here's an example:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n();\n\n\nvar\n \nnumberOfUsers\n \n=\n \nawait\n \nquery\n.\nreduce\n.\ncount\n();\n\n\n\n\n\n\nFor reduce functions that use the value of some property, a property selector is used to identify that property.\n\n\nvar\n \naverageSalary\n \n=\n \nawait\n \nquery\n.\nreduce\n.\nsum\n((\nu\n)\n \n=\n \nu\n.\nsalary\n);\n\n\n\n\n\n\nAny values configured in a \nQuery\nT\n also impact the \nreduce\n function. For example, applying a \nQuery.where\n and then executing a \nsum\n function will only sum the rows that meet the criteria of the where clause:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n()\n\n  \n..\nwhere\n.\nname\n \n=\n \nBob\n;\n\n\nvar\n \naverageSalaryOfPeopleNamedBob\n \n=\n \nawait\n \nquery\n.\nreduce\n.\nsum\n((\nu\n)\n \n=\n \nu\n.\nsalary\n);\n\n\n\n\n\n\nFallbacks\n\n\nYou may always execute arbitrary SQL with \nPersistentStore.execute\n. Note that the objects returned will be a \nList\nList\ndynamic\n - a list of rows, for each a list of columns.\n\n\nYou may also provide raw WHERE clauses with \nQuery.predicate\n. A \nQueryPredicate\n is a \nString\n that is set as the query's where clause. A \nQueryPredicate\n has two properties, a format string and a \nMap\nString, dynamic\n of parameter values. The \nformat\n string can (and should) parameterize any input values. Parameters are indicated in the format string using the \n@\n token:\n\n\n// Creates a predicate that would only include instances where some column \nid\n is less than 2\n\n\nvar\n \npredicate\n \n=\n \nnew\n \nQueryPredicate\n(\nid \n @idVariable\n,\n \n{\nidVariable\n \n:\n \n2\n});\n\n\n\n\n\n\nThe text following the \n@\n token may contain \n[A-Za-z0-9_]\n. The resulting where clause will be formed by replacing each token with the matching key in the parameters map. The value is not transformed in any way, so it must be the appropriate type for the column. If a key is not present in the \nMap\n, an exception will be thrown. Extra keys will be ignored.", 
            "title": "Advanced Queries"
        }, 
        {
            "location": "/db/advanced_queries/#advanced-queries-filtering-joins-paging-and-reduce", 
            "text": "", 
            "title": "Advanced Queries: Filtering, Joins, Paging and Reduce"
        }, 
        {
            "location": "/db/advanced_queries/#paging-fetched-result-sets", 
            "text": "In larger data sets, it may make sense to only return a portion of rows from a database. For example, in a social media application, a user could have thousands of pieces of content they had created over many years. The likely use case for fetching their content would be to grab only the most recent content, and only grab earlier content as necessary. Aqueduct has two mechanisms in  Query T  for building queries that can fetch a subset of rows within a certain range.  Naive paging can be accomplished using the  fetchLimit  and  offset  properties of a  Query T . For example, if a table contains 100 rows, and you would like to grab 10 at a time, each query would have a value of 10 for its  fetchLimit . The first query would have an  offset  of 0, then 10, then 20, and so on. Especially when using  sortBy , this type of paging can be effective. One of the drawbacks to this type of paging is that it can skip or duplicate rows if rows are being added or deleted between fetches.   For example, consider the seven objects above that are ordered by time. If we page by two objects at a time ( fetchLimit=2 ) starting at the first item ( offset=0 ), our first result set is the first two objects. The next page is the original offset plus the same limit - we grab the next two rows. But before the next page is fetched, a new object is inserted and its at an index that we already fetched. The next page would return  3:00pm  again. A similar problem occurs if a row is deleted when paging in this way.  It is really annoying for client applications to have to check for and merge duplicates. Another paging technique that doesn't suffer from this problem relies on the client sending a value from the last object in the previous page, instead of an offset. So in the above example, instead of asking for offset 2 in the second query, it'd send the value  1:30pm . The query filters out rows with a value less than the one it was sent, orders the remaining rows and then fetches the newest from the top.  Query.pageBy  uses this technique. Its usage is similar to  sortBy :  var   firstQuery   =   new   Query Post () \n   .. pageBy (( p )   =   p . dateCreated ,   QuerySortOrder . descending ) \n   .. fetchLimit   =   10 ;  var   firstQueryResults   =   await   firstQuery . fetch ();  var   oldestPostWeGot   =   firstQueryResults . last . dateCreated ;  var   nextQuery   =   new   Query Post () \n   .. pageBy (( p )   =   p . dateCreated ,   QuerySortOrder . descending ,   boundingValue:   oldestPostWeGot ) \n   .. fetchLimit   =   10 ;   This query would fetch the newest 10 posts. Then, it fetches the next 10 after skipping past all of the ones newer than the oldest post it got in the first result set.  When paging, the query must have a  fetchLimit  - otherwise you're just sorting and returning every row. The  pageBy  method takes a closure to identify which property is being used to sort the rows. The closure is passed an instance of  Post  and it returns one of its properties. (This pattern of using a closure to identify a property like this is common to all of the advanced querying methods and is described elsewhere in this document.) The second argument to  pageBy  defines the order the rows will be sorted in.  When you first start paging, you don't have any results yet, so you can't send a value from the last result set. In this case, the  boundingValue  of  pageBy  is null - meaning start from the beginning. Once the first set has been fetched, the  boundingValue  is the value of the paging property in the last object returned.  This is often accomplished by adding a query parameter to an endpoint that takes in a bounding value. (See  ManagedObjectController T  as an example.)  A  pageBy  query will return an empty list of objects when no more values are left. If the number of objects remaining in the last page are less than the  fetchLimit , only those objects will be returned. For example, if there four more objects left and the  fetchLimit  is 10, the number of objects returned will be four.  You should index properties that will be paged by:  @ ManagedColumnAttributes ( indexed:   true )  int   pageableProperty ;", 
            "title": "Paging Fetched Result Sets"
        }, 
        {
            "location": "/db/advanced_queries/#filtering-results-of-a-fetch-operation", 
            "text": "Fetching every row of a table usually doesn't make sense. Instead, we want a specific object or a set of objects matching some condition.  Query.where  is a safe and  elegant way to build a query. The  where  property allows you to assign  matchers  to the properties of a  ManagedObject T . A matcher applies a condition - like equal to or less than - to the property it is assigned to. (This follows the same Hamcrest matcher style that the Dart test framework uses.)  Query.where  is the same type as the object being fetched. For each property of  where  that is assigned a matcher, an expression will be added to the SQL where clause. Here's an example of a query that finds a  User  with an  id  equal to 1:  var   query   =   new   Query User () \n   .. where . id   =   whereEqualTo ( 1 );   (The generated SQL here would be 'SELECT _user.id, _user.name, ... FROM _user WHERE _user.id = 1'.)  All matchers begins with the word  where . Other examples are  whereGreaterThan ,  whereBetween , and  whereIn . Every matcher set on a  where  is combined using logical 'and'. In other words, the following query will find all users whose  name  is \"Bob\"  and   email  is not null:  var   query   =   new   Query User () \n   .. where . id   =   whereEqualTo ( Bob ) \n   .. where . email   =   whereNotNull ;   There are  where  methods for other operators and string comparisons, see the API reference for more.  Relationship properties can be have matchers, too. For example, the following query will fetch all parents who have children that are less than 10 years old:  var   query   =   new   Query Parent () \n   .. where . children . haveAtLeastOneWhere . age   =   whereLessThan ( 10 );   When building  where  with relationship properties, there are some important things to understand. First, the values for any relationship properties are not returned in the results. In the previous query, that means that a list of  Parent s would be returned - but their  children  property wouldn't be populated. (To actually include relationship values, the next section talks about  join .)  Most matchers applied to a relationship property will incur a SQL join, which can be more expensive than a typical fetch. The only time a relationship matcher doesn't incur a SQL join is when matching the value of a foreign key column. That is, a belongs-to relationship property where we're only checking the primary key of the related object. There are two ways of doing this:  var   preferredQuery   =   new   Query Child () \n   .. where . parent   =   whereRelatedByValue ( 23 );  var   sameQuery   =   new   Query Child () \n   .. where . parent . id   =   whereEqualTo ( 23 );   The  whereRelatedByValue  approach is preferred because it's clear to the reader what's happening. A query can be filtered by whether or not it has a value for its relationships. For example, the following queries return people with and without children:  var   peopleWithoutChildren   =   new   Query Person () \n   .. where . children   =   whereNull ;  var   peopleWithChildren   =   new   Query Person () \n   .. where . children   =   whereNotNull ;   The only matchers that can be applied directly to a relationship property are the three shown in these examples:  whereRelatedByValue ,  whereNull  and  whereNotNull . Properties of a relationship property, i.e.  where.parent.age = whereGreaterThan(40) , don't have these restrictions.  You can access relationship properties of relationships, too. The following would fetch every child whose parent is a doctor.  var   childrenWithDoctorParents   =   new   Query Child () \n   .. where . parent . job . title   =   whereEqualTo ( Doctor );   When assigning matchers to the properties of has-many relationships, you may use the  haveAtLeastOneWhere  property. For example, the following returns all parents who have at least one child that is under 10 years old - but they could have other children that are not:  var   query   =   new   Query Parent () \n   .. where . children . haveAtLeastOneWhere . age   =   whereLessThan ( 10 );   The filter is applied to the returned  Parent s - if a parent doesn't have a child that is younger than 10, it will be removed from the result set. If just one of a parent's children is less than 10, it will be included. No children are fetched, either.", 
            "title": "Filtering Results of a Fetch Operation"
        }, 
        {
            "location": "/db/advanced_queries/#including-relationships-in-a-fetch-aka-joins", 
            "text": "A  Query T  can also fetch relationship properties. This allows queries to fetch entire model graphs and reduces the number of round-trips to a database. (This type of fetch will execute a SQL LEFT OUTER JOIN.)  By default, relationship properties are not fetched in a query and therefore aren't included in an object's  asMap() . For example, consider the following two  ManagedObject T s, where a  User  has-many  Task s:  class   User   extends   ManagedObject _User   implements   _User   {}  class   _User   { \n   @ managedPrimaryKey   int   id ; \n\n   String   name ; \n   ManagedSet Task   tasks ;    }  class   Task   extends   ManagedObject _Task   implements   _Task   {}  class   _Task   { \n   @ managedPrimaryKey   int   id ; \n\n   @ ManagedColumnAttributes ( # tasks ) \n   User   user ; \n\n   String   contents ;  }   A  Query User  will fetch the  name  and  id  of each  User . A  User 's  tasks  are not fetched, so the data returned looks like this:  var   q   =   new   Query User ();  var   users   =   await   q . fetch ();  users . first . asMap ()   ==   { \n   id :   1 , \n   name :   Bob  };   // yup   The method  join()  will tell a  Query T  to also include a particular has-many relationship, here, a user's  tasks :  var   q   =   new   Query User () \n   .. join ( set :   ( u )   =   u . tasks );  var   users   =   await   q . fetch ();  users . first . asMap ()   ==   { \n   id :   1 , \n   name :   Bob , \n   tasks :   [ \n       { id :   1 ,   contents :   Take out trash ,   user   :   { id :   1 }}, \n       ... \n   ]  };   // yup   Notice that the  tasks  are in fact included in this query.  When joining a has-many relationship, the  set:  argument is given a closure that returns a  ManagedSet T  property of the type being queried.  The method  join()  actually returns a new  Query T , where  T  is the type of object in the relationship property. That is, the above code could also be written as such:  var   q   =   new   Query User ();  // type annotation added for clarity  Query Task   taskSubQuery   =   q . join ( set :   ( u )   =   u . tasks );   Just like any other  Query T , the set of returning properties can be modified through  returningProperties :  var   q   =   new   Query User () \n   .. returningProperties (( u )   =   [ u . id ,   u . name ]);  q . join ( set :   ( u )   =   u . tasks )   \n   .. returningProperties (( t )   =   [ t . id ,   t . contents ]);   When joining on a has-one or a belongs-to relationship, use  join(object:)  instead of  join(set:) :  var   q   =   new   Query Task () \n   .. join ( object:   ( t )   =   t . user );  var   results   =   await   q . fetch ();  results . first . asMap ()   ==   { \n   id :   1 , \n   contents :   Take out trash , \n   user :   { \n     id :   1 , \n     name :   Bob \n   }  };   // yup   Notice that the results of this query include all of the details for a  Task.user  - not just its  id .  A subquery created through  join  can also be filtered through its  where  property. For example, the following query would return user's named 'Bob' and their overdue tasks only:  var   q   =   new   Query User () \n   .. where . name   =   whereEquals ( Bob );  q . join ( set :   ( u )   =   u . tasks )   \n   .. where . overdue   =   whereEqualTo ( true );   Note that the  where  property on the subquery is an instance of  Task , whereas  where  on the  User  query is  User . More on that in a bit.  More than one join can be applied to a query, and subqueries can be nested. So, this is all valid, assuming the relationship properties exist:  var   q   =   new   Query User () \n   .. join ( object:   ( u )   =   u . address );  q . join ( set :   ( u )   =   u . tasks ) \n   .. join ( object:   ( u )   =   u . location );   This would fetch all users, their addresses, all of their tasks, and the location for each of their tasks. You'd get a nice sized tree of objects here.  It's important to understand how objects are filtered when using  where  and subqueries. Matchers applied to the top-level query will filter out those types of objects. A  where  on a subquery has no impact on the number of objects returned at the top-level.  Let's say there were 10 total users, each with 10 total tasks. The following query returns all 10 user objects, but each user's  tasks  would only contains those that are overdue. So a user might have 0, 1, or 10 tasks returned - even though there are a total of 10 available.  var   q   =   new   Query User ();  q . join ( set :   ( u )   =   u . tasks ) \n   .. where . overdue   =   whereEqualTo ( true );   However, the following query would return less than 10 users, but for each user returned, they would have all 10 of their tasks:  var   q   =   new   Query User () \n   .. where . name   =   whereEqualTo ( Bob ) \n   .. join ( set :   ( u )   =   u . tasks );   Note that a query will always fetch the primary key of all objects, even if it is omitted in  returningProperties .", 
            "title": "Including Relationships in a Fetch (aka, Joins)"
        }, 
        {
            "location": "/db/advanced_queries/#reduce-functions-aka-aggregate-functions", 
            "text": "Queries can also be used to perform functions like  count ,  sum ,  average ,  min  and  max . Here's an example:  var   query   =   new   Query User ();  var   numberOfUsers   =   await   query . reduce . count ();   For reduce functions that use the value of some property, a property selector is used to identify that property.  var   averageSalary   =   await   query . reduce . sum (( u )   =   u . salary );   Any values configured in a  Query T  also impact the  reduce  function. For example, applying a  Query.where  and then executing a  sum  function will only sum the rows that meet the criteria of the where clause:  var   query   =   new   Query User () \n   .. where . name   =   Bob ;  var   averageSalaryOfPeopleNamedBob   =   await   query . reduce . sum (( u )   =   u . salary );", 
            "title": "Reduce Functions (aka, Aggregate Functions)"
        }, 
        {
            "location": "/db/advanced_queries/#fallbacks", 
            "text": "You may always execute arbitrary SQL with  PersistentStore.execute . Note that the objects returned will be a  List List dynamic  - a list of rows, for each a list of columns.  You may also provide raw WHERE clauses with  Query.predicate . A  QueryPredicate  is a  String  that is set as the query's where clause. A  QueryPredicate  has two properties, a format string and a  Map String, dynamic  of parameter values. The  format  string can (and should) parameterize any input values. Parameters are indicated in the format string using the  @  token:  // Creates a predicate that would only include instances where some column  id  is less than 2  var   predicate   =   new   QueryPredicate ( id   @idVariable ,   { idVariable   :   2 });   The text following the  @  token may contain  [A-Za-z0-9_] . The resulting where clause will be formed by replacing each token with the matching key in the parameters map. The value is not transformed in any way, so it must be the appropriate type for the column. If a key is not present in the  Map , an exception will be thrown. Extra keys will be ignored.", 
            "title": "Fallbacks"
        }, 
        {
            "location": "/db/validations/", 
            "text": "Validating Data\n\n\nData is added to a database through \nupdate\n and \ninsert\n queries. As part of these two operations, a \nManagedObject\nT\n will ensure that its properties have valid values. For example, a \nPerson\n object might ensure that its name starts with a capital letter and that its phone number has only numeric values.  If one or more validation fails, the update or insert operation will fail and the data is not sent to the database. A validation failure will throw a \nQueryException\n, which automatically sends an HTTP response with error messaging to help the client correct their request.\n\n\nThe preferred way of setting a validation is to add \nValidate\n metadata to properties of a persistent type of a \nManagedObject\nT\n. Here's an example of a validation that ensures a tweet is less than 140 characters:\n\n\nclass\n \nTweet\n \nextends\n \nManagedObject\n_Tweet\n \nimplements\n \n_Tweet\n \n{}\n\n\nclass\n \n_Tweet\n \n{\n\n  \n@\nmanagedPrimaryKey\n\n  \nint\n \nid\n;\n\n\n  \n@\nValidate\n.\nlength\n(\nlessThan:\n \n140\n)\n\n  \nString\n \nmessage\n;\n\n\n}\n\n\n\n\n\n\nBuilt-in Validators\n\n\nThere are a handful of built-in validations for common operations. For example, it is common to apply a regular expression to a value to ensure it formatted correctly or restrict the possible values to a list of available options. Common validators are available as named constructors on the \nValidate\n class. Here is an example:\n\n\nclass\n \n_Story\n \n{\n\n  \n@\nmanagedPrimaryKey\n\n  \nint\n \nid\n;\n\n\n  \n@\nValidate\n.\noneOf\n(\nconst\n \n[\nstarted\n,\n \naccepted\n,\n \nrejected\n,\n \ndelivered\n])\n\n  \nString\n \nstate\n;\n\n\n}\n\n\n\n\n\n\nA built-in validator is useful because it automatically generates an error message that is returned in an HTTP response. For example, the previous code snippet indicates that the \nstate\n property must be one of the four listed strings. If a value other than one of those four strings is used, the error message returned to the HTTP client would be:\n\n\nThe value `invalidValue` is not valid for \na\n \nhref=\nhttps://www.dartdocs.org/documentation/aqueduct/latest/aqueduct/AuthCodeController/state.html\nstate\n/a\n. Valid values are: \nstarted\n, \naccepted\n, \nrejected\n, \ndelivered\n.\n.\n\n\n\n\n\nSee the API reference for \nValidate\n and its named constructors for possible options.\n\n\nValidate\n metadata on transient properties have no effect. This metadata is only valid for database-backed properties declared in a persistent type.\n\n\nCustom Validators\n\n\nThere will be times where the built-in validators are not sufficient for your application's use case. You may create subclasses of \nValidate\n to provide custom validation behavior.For example, a \nValidate\n subclass you have declared named \nValidatePhoneNumber\n would be used like so:\n\n\nclass\n \n_Person\n \n{\n\n  \n@\nmanagedPrimaryKey\n\n  \nint\n \nid\n;\n\n\n  \n@\nValidatePhoneNumber\n()\n\n  \nString\n \nphoneNumber\n;\n\n\n}\n\n\n\n\n\n\nA subclass of \nValidate\n must override \nValidate.validate()\n and call the superclass' primary constructor when instantiated. Here's an example of that phone number validator:\n\n\nclass\n \nValidatePhoneNumber\n \nextends\n \nValidate\nString\n \n{\n\n  \nValidatePhoneNumber\n({\nbool\n \nonUpdate:\n \ntrue\n,\n \nbool\n \nonInsert:\n \ntrue\n})\n \n:\n\n    \nsuper\n(\nonUpdate:\n \nonUpdate\n,\n \nonInsert:\n \nonInsert\n);\n\n\n  \n@\noverride\n\n  \nbool\n \nvalidate\n(\n\n      \nValidateOperation\n \noperation\n,\n\n      \nManagedAttributeDescription\n \nproperty\n,\n\n      \nString\n \nvalue\n,\n\n      \nList\nString\n \nerrors\n)\n \n{\n  \n    \nif\n \n(\nvalue\n.\nlength\n \n!=\n \n15\n)\n \n{\n\n      \nerrors\n.\nadd\n(\n\n        \n${\nproperty\n.\nname\n}\n has invalid length of \n${\nvalue\n.\nlength\n}\n, must be 15 digits.\n);\n\n      \nreturn\n \nfalse\n;\n\n    \n}\n\n\n    \nif\n \n(\ncontainsNonNumericValues\n(\nvalue\n))\n \n{\n\n      \nerrors\n.\nadd\n(\n\n        \n${\nproperty\n.\nname\n}\n has invalid format, must contain characters 0-9 only.\n);\n\n      \nreturn\n \nfalse\n;\n\n    \n}\n\n\n    \nreturn\n \ntrue\n;\n\n  \n}\n\n\n}\n\n\n\n\n\n\nNote that \nValidate\n is generic - you may provide a type argument which must match the type of the \nvalue\n argument to \nValidate.validate()\n. Omitting the type argument defaults to \ndynamic\n and therefore \nvalue\n must be \ndynamic\n.\n\n\nThe \nvalidate\n method must return \nfalse\n if validation failed, otherwise it should return \ntrue\n. It should add error messages for any failed validations. These error messages are returned in the HTTP response.\n\n\nValidation Behavior\n\n\nA property may have more than one \nValidate\n metadata. In this case, all of the validations for a property must pass. The order in which multiple validations are performed is undefined and subject to change. Here's an example of validations that ensure a property's value is 10 characters long and only contains 10 alphabetic capital letters:\n\n\n@\nValidate\n.\nlength\n(\nequalTo:\n \n10\n)\n\n\n@\nValidate\n.\nmatches\n(\nr\n$[A-Z]+^\n)\n\n\nString\n \ntenCapitalLetters\n;\n\n\n\n\n\n\nBy default, validations are executed when a \nQuery\nT\n's \ninsert\n or \nupdate\n method is invoked. A validator can be restricted to only run on \ninsert\n or \nupdate\n by passing values for its optional constructor arguments \nonUpdate\n and \nonInsert\n:\n\n\n@\nValidate\n.\nmatches\n(\nr\n^[A-Z]+$\n,\n \nonInsert:\n \ntrue\n,\n \nonUpdate:\n \nfalse\n)\n\n\nString\n \nvalidateOnInsertOnly\n;\n\n\n\n\n\n\nIt is important to understand how validations work when a value for a property is \nnot\n specified in an insert or update query. For example, consider a \nPerson\n with a \nname\n and \nemail\n property and then inserted in a query that omits \nemail\n:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nPerson\n()\n\n  \n..\nvalues\n.\nname\n \n=\n \nBob\n;\n\n\n\nawait\n \nquery\n.\ninsert\n();\n\n\n\n\n\n\nBecause \nemail\n was not set on \nQuery.values\n, validations will not be run on that property.\n\n\nThere are two special validators that can require a property to be set, or require that a property \nnot\n be set. \nValidate.present()\n requires that the associated property must have a value. A property with this validator must be provided each time the object is inserted or updated. Like all validators, \nValidate.present()\n can be adjusted to be invoked on only inserts or only updates. For example, the following declaration requires that \nemail\n is set on insertion, but doesn't have to be for updates:\n\n\n@\nValidate\n.\npresent\n(\nonUpdate:\n \nfalse\n,\n \nonInsert:\n \ntrue\n)\n\n\nString\n \nemail\n;\n\n\n\n\n\n\nThe inverse of \nValidate.present()\n is \nValidate.absent()\n. This validation prevents a property from being set. This is useful when a value should be included during insertion, but can't be updated. Here's an example:\n\n\n@\nValidate\n.\nabsent\n(\nonUpdate:\n \ntrue\n,\n \nonInsert:\n \nfalse\n)\n\n\nString\n \ncanOnlyBeSetOnce\n;\n\n\n\n\n\n\nIn the above declaration, the validator is only run on update operations and ensures that the property \ncanOnlyBeSetOnce\n does not have a value. Because this validator is not run on insert operations, there is no restriction when the object is first inserted.\n\n\nValidators are not run when a value is null. For example, the following insertion explicitly inserts \nnull\n for the property \nemail\n:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nPerson\n()\n\n  \n..\nvalues\n.\nemail\n \n=\n \nnull\n\n  \n..\nvalues\n.\nname\n \n=\n \nBob\n;\n\n\n\nawait\n \nquery\n.\ninsert\n();\n\n\n\n\n\n\nNullability is enforced by \nManagedColumnAttributes.isNullable\n property. Consider the following declaration:\n\n\n@\nManagedColumnAttributes\n(\nnullable:\n \nfalse\n)\n\n\n@\nValidate\n.\nlength\n(\ngreaterThan:\n \n10\n)\n\n\nString\n \nname\n;\n\n\n\n\n\n\nHere, the property \nname\n must not be null and must be greater than 10 characters long. The behavior of inserting or updating this property is shown in the following table.\n\n\n\n\n\n\n\n\nInput Value for Name\n\n\nValidation Runs?\n\n\nOutcome\n\n\n\n\n\n\n\n\n\n\nInsert value longer than 10 characters\n\n\nYes\n\n\nSuccessful database insert\n\n\n\n\n\n\nInsert value shorter than 10 characters\n\n\nYes\n\n\nDatabase insert not executed, exception thrown\n\n\n\n\n\n\nInsert value not specified\n\n\nNo\n\n\nDatabase insert fails with non-null violation, exception thrown\n\n\n\n\n\n\nInsert value is null\n\n\nNo\n\n\nDatabase insert fails with non-null violation, exception thrown\n\n\n\n\n\n\nUpdate value longer than 10 characters\n\n\nYes\n\n\nSuccessful database update\n\n\n\n\n\n\nUpdate value shorter than 10 characters\n\n\nYes\n\n\nDatabase update not executed, exception thrown\n\n\n\n\n\n\nUpdate value not specified\n\n\nNo\n\n\nSuccessful database update\n\n\n\n\n\n\nUpdate value is explicit null\n\n\nNo\n\n\nSuccessful database update\n\n\n\n\n\n\n\n\nThis behavior allows \nManagedObject\nT\n instances to be partially updated, which also allows for partial PUTs. Partial PUTs can be prevented by adding \nValidate.present()\n metadata to all properties. While partial PUTs are not idiomatic REST, they are pragmatic software development.\n\n\nThis also means that any custom validator can safely assume that a value passed to \nValidate.validate()\n is non-null.\n\n\nOther Validator Behavior\n\n\nFor validators that can't be built by subclassing \nValidate\n, you may override \nManagedObject\nT\n.validate()\n. This method is useful when a validation involves more than one property.\n\n\nThis method is passed the type of operation triggering the validation - either an insert or update. Here's an example:\n\n\nclass\n \nPerson\n \nextends\n \nManagedObject\n_Person\n \nimplements\n \n_Person\n \n{\n\n  \n@\noverride\n\n  \nbool\n \nvalidate\n(\n\n      \n{\nValidateOperation\n \nforOperation:\n \nValidateOperation\n.\ninsert\n,\n\n      \nList\nString\n \ncollectErrorsIn\n})\n \n{\n\n   \nvar\n \nvalid\n \n=\n \nsuper\n.\nvalidate\n(\n\n     \nforOperation:\n \nforOperation\n,\n \ncollectErrorsIn:\n \ncollectErrorsIn\n);\n\n\n    \nif\n \n(\na\n \n+\n \nb\n \n \n10\n)\n \n{\n\n      \nvalid\n \n=\n \nfalse\n;\n\n      \ncollectErrorsIn\n.\nadd\n(\na + b must be greater than 10\n);\n\n    \n}\n\n\n    \nreturn\n \nvalid\n;\n\n  \n}\n\n\n}\n\n\n\n\n\n\nWhen overriding this method, the \nsuper\n implementation must be invoked to run validations for individual fields. Of course, if it returns \nfalse\n, the overridden method must return \nfalse\n.\n\n\nSkipping Validations\n\n\nValidations are only run when values are set via \nQuery\nT\n.values\n. Values set via \nQuery\nT\n.valueMap\n are not validated. Therefore, objects should typically be inserted and updated using \nQuery\nT\n.values\n unless validation must be ignored. Here's an example of skipping validation:\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nPerson\n()\n\n  \n..\nvalueMap\n \n=\n \n{\n\n    \nname\n \n:\n \nxyz\n,\n\n    \nemail\n \n:\n \nwhatever\n\n  \n};\n\n\n\n\n\n\nSkipping validation should be rare.\n\n\nUpdate and Insert Callbacks\n\n\nManagedObject\nT\n subclasses may override \nwillUpdate\n and \nwillInsert\n to modify its properties prior to being updated or inserted by a \nQuery\nT\n. For example, a managed object may have updated and created dates that can be guaranteed to be set when inserted or updated:\n\n\nclass\n \nPerson\n \nextends\n \nManagedObject\n_Person\n \nimplements\n \n_Person\n \n{\n\n  \n@\noverride\n\n  \nvoid\n \nwillUpdate\n()\n \n{\n\n    \nupdatedAt\n \n=\n \nnew\n \nDateTime\n.\nnow\n().\ntoUtc\n();\n\n  \n}\n\n\n  \n@\noverride\n\n  \nvoid\n \nwillInsert\n()\n \n{\n\n    \ncreatedAt\n \n=\n \nnew\n \nDateTime\n.\nnow\n().\ntoUtc\n();\n\n  \n}\n\n\n}\n\n\nclass\n \n_Person\n \n{\n\n  \n@\nmanagedPrimaryKey\n\n  \nint\n \nid\n;\n\n\n  \nString\n \nname\n;\n\n  \nDateTime\n \ncreatedAt\n;\n\n  \nDateTime\n \nupdatedAt\n;\n\n\n}\n\n\n\n\n\n\nNote that all operations must be synchronous in these methods.\n\n\nBoth \nwillUpdate\n and \nwillInsert\n are invoked prior the validation phase. Thus, any values set in these methods will be subject to the declared validations of the instance.\n\n\nLike validations, \nwillUpdate\n and \nwillInsert\n  are skipped when using \nQuery.valueMap\n.", 
            "title": "Validations"
        }, 
        {
            "location": "/db/validations/#validating-data", 
            "text": "Data is added to a database through  update  and  insert  queries. As part of these two operations, a  ManagedObject T  will ensure that its properties have valid values. For example, a  Person  object might ensure that its name starts with a capital letter and that its phone number has only numeric values.  If one or more validation fails, the update or insert operation will fail and the data is not sent to the database. A validation failure will throw a  QueryException , which automatically sends an HTTP response with error messaging to help the client correct their request.  The preferred way of setting a validation is to add  Validate  metadata to properties of a persistent type of a  ManagedObject T . Here's an example of a validation that ensures a tweet is less than 140 characters:  class   Tweet   extends   ManagedObject _Tweet   implements   _Tweet   {}  class   _Tweet   { \n   @ managedPrimaryKey \n   int   id ; \n\n   @ Validate . length ( lessThan:   140 ) \n   String   message ;  }", 
            "title": "Validating Data"
        }, 
        {
            "location": "/db/validations/#built-in-validators", 
            "text": "There are a handful of built-in validations for common operations. For example, it is common to apply a regular expression to a value to ensure it formatted correctly or restrict the possible values to a list of available options. Common validators are available as named constructors on the  Validate  class. Here is an example:  class   _Story   { \n   @ managedPrimaryKey \n   int   id ; \n\n   @ Validate . oneOf ( const   [ started ,   accepted ,   rejected ,   delivered ]) \n   String   state ;  }   A built-in validator is useful because it automatically generates an error message that is returned in an HTTP response. For example, the previous code snippet indicates that the  state  property must be one of the four listed strings. If a value other than one of those four strings is used, the error message returned to the HTTP client would be:  The value `invalidValue` is not valid for  a   href= https://www.dartdocs.org/documentation/aqueduct/latest/aqueduct/AuthCodeController/state.html state /a . Valid values are:  started ,  accepted ,  rejected ,  delivered . .  See the API reference for  Validate  and its named constructors for possible options.  Validate  metadata on transient properties have no effect. This metadata is only valid for database-backed properties declared in a persistent type.", 
            "title": "Built-in Validators"
        }, 
        {
            "location": "/db/validations/#custom-validators", 
            "text": "There will be times where the built-in validators are not sufficient for your application's use case. You may create subclasses of  Validate  to provide custom validation behavior.For example, a  Validate  subclass you have declared named  ValidatePhoneNumber  would be used like so:  class   _Person   { \n   @ managedPrimaryKey \n   int   id ; \n\n   @ ValidatePhoneNumber () \n   String   phoneNumber ;  }   A subclass of  Validate  must override  Validate.validate()  and call the superclass' primary constructor when instantiated. Here's an example of that phone number validator:  class   ValidatePhoneNumber   extends   Validate String   { \n   ValidatePhoneNumber ({ bool   onUpdate:   true ,   bool   onInsert:   true })   : \n     super ( onUpdate:   onUpdate ,   onInsert:   onInsert ); \n\n   @ override \n   bool   validate ( \n       ValidateOperation   operation , \n       ManagedAttributeDescription   property , \n       String   value , \n       List String   errors )   {   \n     if   ( value . length   !=   15 )   { \n       errors . add ( \n         ${ property . name }  has invalid length of  ${ value . length } , must be 15 digits. ); \n       return   false ; \n     } \n\n     if   ( containsNonNumericValues ( value ))   { \n       errors . add ( \n         ${ property . name }  has invalid format, must contain characters 0-9 only. ); \n       return   false ; \n     } \n\n     return   true ; \n   }  }   Note that  Validate  is generic - you may provide a type argument which must match the type of the  value  argument to  Validate.validate() . Omitting the type argument defaults to  dynamic  and therefore  value  must be  dynamic .  The  validate  method must return  false  if validation failed, otherwise it should return  true . It should add error messages for any failed validations. These error messages are returned in the HTTP response.", 
            "title": "Custom Validators"
        }, 
        {
            "location": "/db/validations/#validation-behavior", 
            "text": "A property may have more than one  Validate  metadata. In this case, all of the validations for a property must pass. The order in which multiple validations are performed is undefined and subject to change. Here's an example of validations that ensure a property's value is 10 characters long and only contains 10 alphabetic capital letters:  @ Validate . length ( equalTo:   10 )  @ Validate . matches ( r $[A-Z]+^ )  String   tenCapitalLetters ;   By default, validations are executed when a  Query T 's  insert  or  update  method is invoked. A validator can be restricted to only run on  insert  or  update  by passing values for its optional constructor arguments  onUpdate  and  onInsert :  @ Validate . matches ( r ^[A-Z]+$ ,   onInsert:   true ,   onUpdate:   false )  String   validateOnInsertOnly ;   It is important to understand how validations work when a value for a property is  not  specified in an insert or update query. For example, consider a  Person  with a  name  and  email  property and then inserted in a query that omits  email :  var   query   =   new   Query Person () \n   .. values . name   =   Bob ;  await   query . insert ();   Because  email  was not set on  Query.values , validations will not be run on that property.  There are two special validators that can require a property to be set, or require that a property  not  be set.  Validate.present()  requires that the associated property must have a value. A property with this validator must be provided each time the object is inserted or updated. Like all validators,  Validate.present()  can be adjusted to be invoked on only inserts or only updates. For example, the following declaration requires that  email  is set on insertion, but doesn't have to be for updates:  @ Validate . present ( onUpdate:   false ,   onInsert:   true )  String   email ;   The inverse of  Validate.present()  is  Validate.absent() . This validation prevents a property from being set. This is useful when a value should be included during insertion, but can't be updated. Here's an example:  @ Validate . absent ( onUpdate:   true ,   onInsert:   false )  String   canOnlyBeSetOnce ;   In the above declaration, the validator is only run on update operations and ensures that the property  canOnlyBeSetOnce  does not have a value. Because this validator is not run on insert operations, there is no restriction when the object is first inserted.  Validators are not run when a value is null. For example, the following insertion explicitly inserts  null  for the property  email :  var   query   =   new   Query Person () \n   .. values . email   =   null \n   .. values . name   =   Bob ;  await   query . insert ();   Nullability is enforced by  ManagedColumnAttributes.isNullable  property. Consider the following declaration:  @ ManagedColumnAttributes ( nullable:   false )  @ Validate . length ( greaterThan:   10 )  String   name ;   Here, the property  name  must not be null and must be greater than 10 characters long. The behavior of inserting or updating this property is shown in the following table.     Input Value for Name  Validation Runs?  Outcome      Insert value longer than 10 characters  Yes  Successful database insert    Insert value shorter than 10 characters  Yes  Database insert not executed, exception thrown    Insert value not specified  No  Database insert fails with non-null violation, exception thrown    Insert value is null  No  Database insert fails with non-null violation, exception thrown    Update value longer than 10 characters  Yes  Successful database update    Update value shorter than 10 characters  Yes  Database update not executed, exception thrown    Update value not specified  No  Successful database update    Update value is explicit null  No  Successful database update     This behavior allows  ManagedObject T  instances to be partially updated, which also allows for partial PUTs. Partial PUTs can be prevented by adding  Validate.present()  metadata to all properties. While partial PUTs are not idiomatic REST, they are pragmatic software development.  This also means that any custom validator can safely assume that a value passed to  Validate.validate()  is non-null.", 
            "title": "Validation Behavior"
        }, 
        {
            "location": "/db/validations/#other-validator-behavior", 
            "text": "For validators that can't be built by subclassing  Validate , you may override  ManagedObject T .validate() . This method is useful when a validation involves more than one property.  This method is passed the type of operation triggering the validation - either an insert or update. Here's an example:  class   Person   extends   ManagedObject _Person   implements   _Person   { \n   @ override \n   bool   validate ( \n       { ValidateOperation   forOperation:   ValidateOperation . insert , \n       List String   collectErrorsIn })   { \n    var   valid   =   super . validate ( \n      forOperation:   forOperation ,   collectErrorsIn:   collectErrorsIn ); \n\n     if   ( a   +   b     10 )   { \n       valid   =   false ; \n       collectErrorsIn . add ( a + b must be greater than 10 ); \n     } \n\n     return   valid ; \n   }  }   When overriding this method, the  super  implementation must be invoked to run validations for individual fields. Of course, if it returns  false , the overridden method must return  false .", 
            "title": "Other Validator Behavior"
        }, 
        {
            "location": "/db/validations/#skipping-validations", 
            "text": "Validations are only run when values are set via  Query T .values . Values set via  Query T .valueMap  are not validated. Therefore, objects should typically be inserted and updated using  Query T .values  unless validation must be ignored. Here's an example of skipping validation:  var   query   =   new   Query Person () \n   .. valueMap   =   { \n     name   :   xyz , \n     email   :   whatever \n   };   Skipping validation should be rare.", 
            "title": "Skipping Validations"
        }, 
        {
            "location": "/db/validations/#update-and-insert-callbacks", 
            "text": "ManagedObject T  subclasses may override  willUpdate  and  willInsert  to modify its properties prior to being updated or inserted by a  Query T . For example, a managed object may have updated and created dates that can be guaranteed to be set when inserted or updated:  class   Person   extends   ManagedObject _Person   implements   _Person   { \n   @ override \n   void   willUpdate ()   { \n     updatedAt   =   new   DateTime . now (). toUtc (); \n   } \n\n   @ override \n   void   willInsert ()   { \n     createdAt   =   new   DateTime . now (). toUtc (); \n   }  }  class   _Person   { \n   @ managedPrimaryKey \n   int   id ; \n\n   String   name ; \n   DateTime   createdAt ; \n   DateTime   updatedAt ;  }   Note that all operations must be synchronous in these methods.  Both  willUpdate  and  willInsert  are invoked prior the validation phase. Thus, any values set in these methods will be subject to the declared validations of the instance.  Like validations,  willUpdate  and  willInsert   are skipped when using  Query.valueMap .", 
            "title": "Update and Insert Callbacks"
        }, 
        {
            "location": "/db/db_tools/", 
            "text": "Database Migration and Tooling\n\n\nThe \naqueduct db\n command line tool creates and executes \nmigration files\n. A migration file contains SQL commands that create and modify database tables to match your application's data model.\n\n\nMigration Files\n\n\nDatabase tables are described by \nManagedObject\nT\n subclasses and their persistent type. Migration files describe a series of database commands that will create or modify a database schema to match an application's \nManagedObject\nT\n declarations. Migration files are executed on a database when an application is first deployed and when changes to the data model occur - like adding new \nManagedObject\nT\n subclasses or changing the name of a \nManagedObject\nT\n property.\n\n\nEach migration file contains only the changes made since the last migration file was generated. For example, let's say that version 1 of your application has two \nManagedObject\nT\n subclasses, \nUser\n and \nPost\n. Before you launch, you create a migration file that creates two tables, one for \nUser\n and one for \nPost\n. A month later, you have developed version 1.1 of your application and now you have a third \nManagedObject\nT\n named \nLocation\n. Prior to deploying version 1.1, you generate a new migration file and execute it. This migration file only contains instructions to create a table for \nLocation\n. The 'final product' of your database is the sum of both migration files.\n\n\nFor this reason, migrations files should be stored in source control.\n\n\nGenerating Migration Files\n\n\nMigration files are automatically generated by running \naqueduct db generate\n in an Aqueduct project directory. This tool finds every \nManagedObject\nT\n subclass and adds commands to the migration file to create a database table that matches its declaration. When subsequent migration files are generated, the difference between the schema created by existing migration files is compared to the current schema declared in an application's code. The commands to rectify those differences are added to the new migration file.\n\n\nThis tool will find \nManagedObject\nT\n subclasses in an application. As a convention, every \nManagedObject\nT\n subclass is declared in its own file in \nlib/model/\n. For example, a \nUser\n class is defined in \nlib/model/user.dart\n.\n\n\nMigration files are stored in an application's \nmigrations\n directory. Migration files are prefixed with a version number, a \"0\" padded eight digit number, ad suffixed with \n.migration.dart\n. For example, \n00000001_Initial.migration.dart\n is a migration filename. The version number portion of the filename is required, as is the \n.migration.dart\n suffix. The underscore and remainder of the filename are optional and have no effect, they are just a way to name the file. Here is an example of two migration file names:\n\n\n00000001_initial.migration.dart\n00000002_add_user_nickname.migration.dart\n\n\n\n\n\nThe version number of migration files indicate the order in which they are applied. Leading zeros are stripped from the filenames before their version numbers are compared. Version numbers do not necessarily have to be continuous, but doing otherwise is not recommended.\n\n\nMigration files may be created manually or altered after they are generated by \naqueduct db generate\n. A migration file's \nMigration.upgrade\n method makes calls to \nMigration.database\n (an instance of \nSchemaBuilder\n) property to add, remove, and modify tables and columns. Each method invocation creates one or more SQL commands. When a migration file is executed, its \nupgrade\n method is invoked and each command is collected and run within a transaction. The commands are executed in order, and it's important to note that the order often matters.\n\n\nThere are scenarios where there are more than one operation can rectify a change in the data model. It is important to review migration files after they have been generated to ensure the expected behavior.\n\n\nValidating Migration Files\n\n\nMigration files may be altered after they have been generated. This is often the case if \naqueduct db generate\n can't say for certain how a database should change. For example, is renaming a property just renaming a column, or is it deleting a column and creating a new column? The \naqueduct db validate\n tool ensures that the database schema after running all migration files matches the database schema declared by an application's \nManagedObject\nT\ns. Any generated migration file will pass \naqueduct db validate\n. The validate tool will display differences found between the schema in code and the schema created by migration files.\n\n\nListing Migration Files\n\n\nUse \naqueduct db list\n to list all database migration files and their resolved version number.\n\n\nExecuting Migration Files\n\n\nThe tool \naqueduct db upgrade\n will apply migration files to a running database. This tool is run in an application's directory and finds migration files in the \nmigrations\n directory. The connection info for a the running database is provided with the \n--connect\n option. For example, the following would execute migration files on a PostgreSQL database:\n\n\naqueduct db upgrade --connect postgres://username:password@localhost:5432/my_application\n\n\n\n\n\nThe first time \naqueduct db upgrade\n is executed, it creates a version table that keeps the version number and dates of upgrades. When \naqueduct db upgrade\n is ran after the initial migration, the version number is fetched from the database. The tool only runs migration files after the version number stored in the database.\n\n\nConnection information can also be stored in a database configuration file named \ndatabase.yaml\n in the application directory. If this file exists with the following format, \n--connect\n can be omitted and connection information will be read from this file:\n\n\nusername: \nuser\n\npassword: \npassword\n\nhost: \nhost\n\nport: port\ndatabaseName: \ndatabase\n\n\n\n\n\n\nGetting a Database's Version\n\n\nYou can fetch a database's current version number with \naqueduct db get-version\n. This command takes \n--connect\n or a \ndatabase.yaml\n file as described in the previous section to get connection info for the database.\n\n\nWhen to Execute Migration Files\n\n\nDuring development, there is no need to create a migration file for each change. Execute migration files prior to deployment of a new version of an application.\n\n\nYou may delete migration files. When \naqueduct db generate\n is run again, will replay only the existing migration files before determining which commands to add to the new migration file. For example, if you have 10 migration files over time and delete them all - the next generated migration file will contain commands to recreate the entire database schema.", 
            "title": "Migration and Tooling"
        }, 
        {
            "location": "/db/db_tools/#database-migration-and-tooling", 
            "text": "The  aqueduct db  command line tool creates and executes  migration files . A migration file contains SQL commands that create and modify database tables to match your application's data model.", 
            "title": "Database Migration and Tooling"
        }, 
        {
            "location": "/db/db_tools/#migration-files", 
            "text": "Database tables are described by  ManagedObject T  subclasses and their persistent type. Migration files describe a series of database commands that will create or modify a database schema to match an application's  ManagedObject T  declarations. Migration files are executed on a database when an application is first deployed and when changes to the data model occur - like adding new  ManagedObject T  subclasses or changing the name of a  ManagedObject T  property.  Each migration file contains only the changes made since the last migration file was generated. For example, let's say that version 1 of your application has two  ManagedObject T  subclasses,  User  and  Post . Before you launch, you create a migration file that creates two tables, one for  User  and one for  Post . A month later, you have developed version 1.1 of your application and now you have a third  ManagedObject T  named  Location . Prior to deploying version 1.1, you generate a new migration file and execute it. This migration file only contains instructions to create a table for  Location . The 'final product' of your database is the sum of both migration files.  For this reason, migrations files should be stored in source control.", 
            "title": "Migration Files"
        }, 
        {
            "location": "/db/db_tools/#generating-migration-files", 
            "text": "Migration files are automatically generated by running  aqueduct db generate  in an Aqueduct project directory. This tool finds every  ManagedObject T  subclass and adds commands to the migration file to create a database table that matches its declaration. When subsequent migration files are generated, the difference between the schema created by existing migration files is compared to the current schema declared in an application's code. The commands to rectify those differences are added to the new migration file.  This tool will find  ManagedObject T  subclasses in an application. As a convention, every  ManagedObject T  subclass is declared in its own file in  lib/model/ . For example, a  User  class is defined in  lib/model/user.dart .  Migration files are stored in an application's  migrations  directory. Migration files are prefixed with a version number, a \"0\" padded eight digit number, ad suffixed with  .migration.dart . For example,  00000001_Initial.migration.dart  is a migration filename. The version number portion of the filename is required, as is the  .migration.dart  suffix. The underscore and remainder of the filename are optional and have no effect, they are just a way to name the file. Here is an example of two migration file names:  00000001_initial.migration.dart\n00000002_add_user_nickname.migration.dart  The version number of migration files indicate the order in which they are applied. Leading zeros are stripped from the filenames before their version numbers are compared. Version numbers do not necessarily have to be continuous, but doing otherwise is not recommended.  Migration files may be created manually or altered after they are generated by  aqueduct db generate . A migration file's  Migration.upgrade  method makes calls to  Migration.database  (an instance of  SchemaBuilder ) property to add, remove, and modify tables and columns. Each method invocation creates one or more SQL commands. When a migration file is executed, its  upgrade  method is invoked and each command is collected and run within a transaction. The commands are executed in order, and it's important to note that the order often matters.  There are scenarios where there are more than one operation can rectify a change in the data model. It is important to review migration files after they have been generated to ensure the expected behavior.", 
            "title": "Generating Migration Files"
        }, 
        {
            "location": "/db/db_tools/#validating-migration-files", 
            "text": "Migration files may be altered after they have been generated. This is often the case if  aqueduct db generate  can't say for certain how a database should change. For example, is renaming a property just renaming a column, or is it deleting a column and creating a new column? The  aqueduct db validate  tool ensures that the database schema after running all migration files matches the database schema declared by an application's  ManagedObject T s. Any generated migration file will pass  aqueduct db validate . The validate tool will display differences found between the schema in code and the schema created by migration files.", 
            "title": "Validating Migration Files"
        }, 
        {
            "location": "/db/db_tools/#listing-migration-files", 
            "text": "Use  aqueduct db list  to list all database migration files and their resolved version number.", 
            "title": "Listing Migration Files"
        }, 
        {
            "location": "/db/db_tools/#executing-migration-files", 
            "text": "The tool  aqueduct db upgrade  will apply migration files to a running database. This tool is run in an application's directory and finds migration files in the  migrations  directory. The connection info for a the running database is provided with the  --connect  option. For example, the following would execute migration files on a PostgreSQL database:  aqueduct db upgrade --connect postgres://username:password@localhost:5432/my_application  The first time  aqueduct db upgrade  is executed, it creates a version table that keeps the version number and dates of upgrades. When  aqueduct db upgrade  is ran after the initial migration, the version number is fetched from the database. The tool only runs migration files after the version number stored in the database.  Connection information can also be stored in a database configuration file named  database.yaml  in the application directory. If this file exists with the following format,  --connect  can be omitted and connection information will be read from this file:  username:  user \npassword:  password \nhost:  host \nport: port\ndatabaseName:  database", 
            "title": "Executing Migration Files"
        }, 
        {
            "location": "/db/db_tools/#getting-a-databases-version", 
            "text": "You can fetch a database's current version number with  aqueduct db get-version . This command takes  --connect  or a  database.yaml  file as described in the previous section to get connection info for the database.", 
            "title": "Getting a Database's Version"
        }, 
        {
            "location": "/db/db_tools/#when-to-execute-migration-files", 
            "text": "During development, there is no need to create a migration file for each change. Execute migration files prior to deployment of a new version of an application.  You may delete migration files. When  aqueduct db generate  is run again, will replay only the existing migration files before determining which commands to add to the new migration file. For example, if you have 10 migration files over time and delete them all - the next generated migration file will contain commands to recreate the entire database schema.", 
            "title": "When to Execute Migration Files"
        }, 
        {
            "location": "/auth/overview/", 
            "text": "Tasks\n\n\nAqueduct has types to manage authentication and authorization according to the \nOAuth 2.0 specification\n. The following tasks are important for this behavior:\n\n\n\n\nCreating \nAuthServer\n instances to enable OAuth 2.0 in an Aqueduct application\n\n\nUsing \nManagedAuthStorage\nT\n to manage storage of authorization objects, e.g. storing tokens in a database.\n\n\nUsing \nAuthCodeController\n and \nAuthController\n to expose endpoints for exchanging credentials for authorization tokens.\n\n\nAdding \nAuthorizer\ns to a request channel to allow only authorized requests.\n\n\nManaging OAuth 2.0 Client identifiers, secrets and scopes with the \naqueduct auth\n tool\n\n\n\n\n\n\nGuides\n\n\n\n\nWhat is OAuth 2.0?\n\n\nCreating and Using AuthServers\n\n\nSecuring Routes with Authorizer\n\n\nAdding Auth Endpoints\n\n\nUsing Scopes to Control Access\n\n\nManaging OAuth 2.0 Clients", 
            "title": "Overview"
        }, 
        {
            "location": "/auth/overview/#tasks", 
            "text": "Aqueduct has types to manage authentication and authorization according to the  OAuth 2.0 specification . The following tasks are important for this behavior:   Creating  AuthServer  instances to enable OAuth 2.0 in an Aqueduct application  Using  ManagedAuthStorage T  to manage storage of authorization objects, e.g. storing tokens in a database.  Using  AuthCodeController  and  AuthController  to expose endpoints for exchanging credentials for authorization tokens.  Adding  Authorizer s to a request channel to allow only authorized requests.  Managing OAuth 2.0 Client identifiers, secrets and scopes with the  aqueduct auth  tool", 
            "title": "Tasks"
        }, 
        {
            "location": "/auth/overview/#guides", 
            "text": "What is OAuth 2.0?  Creating and Using AuthServers  Securing Routes with Authorizer  Adding Auth Endpoints  Using Scopes to Control Access  Managing OAuth 2.0 Clients", 
            "title": "Guides"
        }, 
        {
            "location": "/auth/server/", 
            "text": "Creating AuthServers to Authenticate and Authorize\n\n\nAn instance of \nAuthServer\n handles creating, verifying and refreshing authorization tokens. They are created in a \nRequestSink\n constructor and are used by instances of \nAuthorizer\n, \nAuthController\n and \nAuthCodeController\n.\n\n\nAn \nAuthServer\n must persist the data it uses and creates - like client identifiers and access tokens. Storage is often performed by a database, but it can be in memory, a cache or some files. For that reason, an \nAuthServer\n doesn't perform any storage itself - it relies on an instance of \nAuthStorage\n specific to your use case.\n\n\nThis allows storage to be independent of verification logic.\n\n\nCreating Instances of AuthServer and AuthStorage\n\n\nAn instance of \nAuthServer\n is created in a \nRequestSink\n's constructor along with its instance of \nAuthStorage\n. \nAuthStorage\n is just an interface - storage is implemented by providing an implementation for each of its methods.\n\n\nBecause storage can be quite complex and sensitive, the type \nManagedAuthStorage\nT\n implements storage with \nManagedObject\nT\ns and \nQuery\nT\ns. It is highly recommended to use this type instead of implementing your own storage because it has been thoroughly tested and handles cleaning up expired data correctly.\n\n\nManagedAuthStorage\nT\n is declared in a sub-library, \nmanaged_auth\n, that is part of the \naqueduct\n package but not part of the \naqueduct\n library. Therefore, it must be explicitly imported. Here's an example of creating an \nAuthServer\n and \nManagedAuthStorage\nT\n:\n\n\nimport\n \npackage:aqueduct/aqueduct.dart\n;\n\n\nimport\n \npackage:aqueduct/managed_auth.dart\n;\n\n\n\nclass\n \nMyRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nMyRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n    \nvar\n \ncontext\n \n=\n \nnew\n \nManagedContext\n(...);\n\n    \nvar\n \nstorage\n \n=\n \nnew\n \nManagedAuthStorage\nUser\n(\ncontext\n);\n\n    \nauthServer\n \n=\n \nnew\n \nAuthServer\n(\nstorage\n);\n\n  \n}\n\n\n  \nAuthServer\n \nauthServer\n;\n\n\n  \n...\n\n\n}\n\n\n\n\n\n\n(Notice that \nManagedAuthStorage\n has a type argument - this will be covered in the next section.)\n\n\nWhile \nAuthServer\n has methods for handling authorization tasks, it is rarely used directly. Instead, \nAuthCodeController\n and \nAuthController\n are hooked up to routes to grant authorization tokens via the API. Instances of \nAuthorizer\n secure routes in request channels. All of these types invoke the appropriate methods on the \nAuthServer\n.\n\n\nTherefore, a full authorization implementation rarely extends past a \nRequestSink\n. Here's an example \nRequestSink\n subclass that sets up and uses authorization:\n\n\nimport\n \npackage:aqueduct/aqueduct.dart\n;\n\n\nimport\n \npackage:aqueduct/managed_auth.dart\n;\n\n\n\nclass\n \nMyRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nMyRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n    \ncontext\n \n=\n \nnew\n \nManagedContext\n(...);\n\n    \nvar\n \nstorage\n \n=\n \nnew\n \nManagedAuthStorage\nUser\n(\ncontext\n);\n\n    \nauthServer\n \n=\n \nnew\n \nAuthServer\n(\nstorage\n);\n\n  \n}\n\n\n  \nAuthServer\n \nauthServer\n;\n\n  \nManagedContext\n \ncontext\n;\n\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n    \n// Set up auth token route- this grants and refresh tokens\n\n    \nrouter\n.\nroute\n(\n/auth/token\n).\ngenerate\n(()\n \n=\n \nnew\n \nAuthController\n(\nauthServer\n));\n\n\n    \n// Set up auth code route- this grants temporary access codes that can be exchanged for token\n\n    \nrouter\n.\nroute\n(\n/auth/code\n).\ngenerate\n(()\n \n=\n \nnew\n \nAuthCodeController\n(\nauthServer\n));\n\n\n    \n// Set up protected route\n\n    \nrouter\n\n      \n.\nroute\n(\n/protected\n)\n\n      \n.\npipe\n(\nnew\n \nAuthorizer\n.\nbearer\n(\nauthServer\n))\n\n      \n.\ngenerate\n(()\n \n=\n \nnew\n \nProtectedController\n());\n\n  \n}\n\n\n}\n\n\n\n\n\n\nFor more details on authorization controllers like \nAuthController\n, see \nAuthorization Controllers\n. For more details on securing routes, see \nAuthorizers\n.\n\n\nUsing ManagedAuthStorage\n\n\nManagedAuthStorage\nT\n is a concrete implementation of \nAuthStorage\n, providing storage of authorization tokens and clients for \nAuthServer\n. Storage is accomplished by Aqueduct's ORM. \nManagedAuthStorage\nT\n, by default, is not part of the standard \naqueduct/aqueduct\n library. To use this class, an application must import \npackage:aqueduct/managed_auth.dart\n.\n\n\nThe type argument to \nManagedAuthStorage\nT\n represents the application's concept of a 'user' or 'account' - OAuth 2.0 terminology would refer to this type as a \nresource owner\n.\n\n\nThe type argument must be a \nManagedObject\nT\n subclass that is specific to your application. Its persistent type \nmust extend\n \nManagedAuthenticatable\n and the instance type must implement \nManagedAuthResourceOwner\n. A basic definition may look like this:\n\n\nclass\n \nUser\n \nextends\n \nManagedObject\n_User\n\n    \nimplements\n \n_User\n,\n \nManagedAuthResourceOwner\n \n{\n\n\n}\n\n\n\nclass\n \n_User\n \nextends\n \nManagedAuthenticatable\n \n{\n\n  \n@\nManagedColumnAttributes\n(\nunique:\n \ntrue\n)\n\n  \nString\n \nemail\n;\n\n\n}\n\n\n\n\n\n\nBy extending \nManagedAuthenticatable\n, the database table has the following four columns:\n\n\n\n\nan integer primary key named \nid\n\n\na unique string \nusername\n\n\na password hash\n\n\na salt used to generate the password hash\n\n\n\n\nA \nManagedAuthenticatable\n also has a \nManagedSet\n of \ntokens\n for each token that has been granted on its behalf.\n\n\nThe interface \nManagedAuthResourceOwner\n is a requirement that ensures the type argument is both a \nManagedObject\nT\n and \nManagedAuthenticatable\n, and serves no other purpose than to restrict \nManagedAuthStorage\nT\n's type parameter.\n\n\nThis structure allows an application to declare its own 'user' type while still enforcing the needs of Aqueduct's OAuth 2.0 implementation.\n\n\nThe \nmanaged_auth\n library also declares two \nManagedObject\nT\n subclasses. \nManagedToken\n represents instances of authorization tokens and codes, and \nManagedClient\n represents instances of OAuth 2.0 clients. This means that an Aqueduct application that uses \nManagedAuthStorage\nT\n has a minimum of three database tables: users, tokens and clients.\n\n\nManagedAuthStorage\nT\n will delete authorization tokens and codes when they are no longer in use. This is determined by how many tokens a resource owner has and the tokens expiration dates. Once a resource owner acquires more than 40 tokens/codes, the oldest tokens/codes (determined by expiration date) are deleted. Effectively, the resource owner is limited to 40 tokens. This number can be changed when instantiating \nManagedAuthStorage\nT\n:\n\n\nvar\n \nstorage\n \n=\n \nnew\n \nManagedAuthStorage\n(\ncontext\n,\n \ntokenLimit:\n \n20\n);\n\n\n\n\n\n\nConfiguring the Database\n\n\nManagedAuthStorage\nT\n requires database tables for its users, tokens and clients. Use the \ndatabase command-line tool\n on your project to generate migration scripts and execute them against a database. This tool will see the declarations for your user type, \nManagedToken\n and \nManagedClient\n and create the appropriate tables.", 
            "title": "Setting up Authorization"
        }, 
        {
            "location": "/auth/server/#creating-authservers-to-authenticate-and-authorize", 
            "text": "An instance of  AuthServer  handles creating, verifying and refreshing authorization tokens. They are created in a  RequestSink  constructor and are used by instances of  Authorizer ,  AuthController  and  AuthCodeController .  An  AuthServer  must persist the data it uses and creates - like client identifiers and access tokens. Storage is often performed by a database, but it can be in memory, a cache or some files. For that reason, an  AuthServer  doesn't perform any storage itself - it relies on an instance of  AuthStorage  specific to your use case.  This allows storage to be independent of verification logic.", 
            "title": "Creating AuthServers to Authenticate and Authorize"
        }, 
        {
            "location": "/auth/server/#creating-instances-of-authserver-and-authstorage", 
            "text": "An instance of  AuthServer  is created in a  RequestSink 's constructor along with its instance of  AuthStorage .  AuthStorage  is just an interface - storage is implemented by providing an implementation for each of its methods.  Because storage can be quite complex and sensitive, the type  ManagedAuthStorage T  implements storage with  ManagedObject T s and  Query T s. It is highly recommended to use this type instead of implementing your own storage because it has been thoroughly tested and handles cleaning up expired data correctly.  ManagedAuthStorage T  is declared in a sub-library,  managed_auth , that is part of the  aqueduct  package but not part of the  aqueduct  library. Therefore, it must be explicitly imported. Here's an example of creating an  AuthServer  and  ManagedAuthStorage T :  import   package:aqueduct/aqueduct.dart ;  import   package:aqueduct/managed_auth.dart ;  class   MyRequestSink   extends   RequestSink   { \n   MyRequestSink ( ApplicationConfiguration   config )   :   super ( config )   { \n     var   context   =   new   ManagedContext (...); \n     var   storage   =   new   ManagedAuthStorage User ( context ); \n     authServer   =   new   AuthServer ( storage ); \n   } \n\n   AuthServer   authServer ; \n\n   ...  }   (Notice that  ManagedAuthStorage  has a type argument - this will be covered in the next section.)  While  AuthServer  has methods for handling authorization tasks, it is rarely used directly. Instead,  AuthCodeController  and  AuthController  are hooked up to routes to grant authorization tokens via the API. Instances of  Authorizer  secure routes in request channels. All of these types invoke the appropriate methods on the  AuthServer .  Therefore, a full authorization implementation rarely extends past a  RequestSink . Here's an example  RequestSink  subclass that sets up and uses authorization:  import   package:aqueduct/aqueduct.dart ;  import   package:aqueduct/managed_auth.dart ;  class   MyRequestSink   extends   RequestSink   { \n   MyRequestSink ( ApplicationConfiguration   config )   :   super ( config )   { \n     context   =   new   ManagedContext (...); \n     var   storage   =   new   ManagedAuthStorage User ( context ); \n     authServer   =   new   AuthServer ( storage ); \n   } \n\n   AuthServer   authServer ; \n   ManagedContext   context ; \n\n   void   setupRouter ( Router   router )   { \n     // Set up auth token route- this grants and refresh tokens \n     router . route ( /auth/token ). generate (()   =   new   AuthController ( authServer )); \n\n     // Set up auth code route- this grants temporary access codes that can be exchanged for token \n     router . route ( /auth/code ). generate (()   =   new   AuthCodeController ( authServer )); \n\n     // Set up protected route \n     router \n       . route ( /protected ) \n       . pipe ( new   Authorizer . bearer ( authServer )) \n       . generate (()   =   new   ProtectedController ()); \n   }  }   For more details on authorization controllers like  AuthController , see  Authorization Controllers . For more details on securing routes, see  Authorizers .", 
            "title": "Creating Instances of AuthServer and AuthStorage"
        }, 
        {
            "location": "/auth/server/#using-managedauthstorage", 
            "text": "ManagedAuthStorage T  is a concrete implementation of  AuthStorage , providing storage of authorization tokens and clients for  AuthServer . Storage is accomplished by Aqueduct's ORM.  ManagedAuthStorage T , by default, is not part of the standard  aqueduct/aqueduct  library. To use this class, an application must import  package:aqueduct/managed_auth.dart .  The type argument to  ManagedAuthStorage T  represents the application's concept of a 'user' or 'account' - OAuth 2.0 terminology would refer to this type as a  resource owner .  The type argument must be a  ManagedObject T  subclass that is specific to your application. Its persistent type  must extend   ManagedAuthenticatable  and the instance type must implement  ManagedAuthResourceOwner . A basic definition may look like this:  class   User   extends   ManagedObject _User \n     implements   _User ,   ManagedAuthResourceOwner   {  }  class   _User   extends   ManagedAuthenticatable   { \n   @ ManagedColumnAttributes ( unique:   true ) \n   String   email ;  }   By extending  ManagedAuthenticatable , the database table has the following four columns:   an integer primary key named  id  a unique string  username  a password hash  a salt used to generate the password hash   A  ManagedAuthenticatable  also has a  ManagedSet  of  tokens  for each token that has been granted on its behalf.  The interface  ManagedAuthResourceOwner  is a requirement that ensures the type argument is both a  ManagedObject T  and  ManagedAuthenticatable , and serves no other purpose than to restrict  ManagedAuthStorage T 's type parameter.  This structure allows an application to declare its own 'user' type while still enforcing the needs of Aqueduct's OAuth 2.0 implementation.  The  managed_auth  library also declares two  ManagedObject T  subclasses.  ManagedToken  represents instances of authorization tokens and codes, and  ManagedClient  represents instances of OAuth 2.0 clients. This means that an Aqueduct application that uses  ManagedAuthStorage T  has a minimum of three database tables: users, tokens and clients.  ManagedAuthStorage T  will delete authorization tokens and codes when they are no longer in use. This is determined by how many tokens a resource owner has and the tokens expiration dates. Once a resource owner acquires more than 40 tokens/codes, the oldest tokens/codes (determined by expiration date) are deleted. Effectively, the resource owner is limited to 40 tokens. This number can be changed when instantiating  ManagedAuthStorage T :  var   storage   =   new   ManagedAuthStorage ( context ,   tokenLimit:   20 );", 
            "title": "Using ManagedAuthStorage"
        }, 
        {
            "location": "/auth/server/#configuring-the-database", 
            "text": "ManagedAuthStorage T  requires database tables for its users, tokens and clients. Use the  database command-line tool  on your project to generate migration scripts and execute them against a database. This tool will see the declarations for your user type,  ManagedToken  and  ManagedClient  and create the appropriate tables.", 
            "title": "Configuring the Database"
        }, 
        {
            "location": "/auth/authorizer/", 
            "text": "Securing Routes with Authorizer\n\n\nInstances of \nAuthorizer\n are added to a request controller channel to verify HTTP request's authorization information before passing the request onwards. They protect channel access and typically come right after \nroute\n. Here's an example:\n\n\n@\noverride\n\n\nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n  \nrouter\n\n    \n.\nroute\n(\n/protected\n)\n\n    \n.\npipe\n(\nnew\n \nAuthorizer\n.\nbearer\n(\nauthServer\n))\n\n    \n.\ngenerate\n(()\n \n=\n \nnew\n \nProtectedController\n());\n\n\n  \nrouter\n\n    \n.\nroute\n(\n/other\n)\n\n    \n.\npipe\n(\nnew\n \nAuthorizer\n.\nbasic\n(\nauthServer\n))\n\n    \n.\ngenerate\n(()\n \n=\n \nnew\n \nOtherProtectedController\n());\n\n\n}\n\n\n\n\n\n\nAn \nAuthorizer\n has no state itself, so it is added to a channel via \npipe\n; i.e., it does not need to be \ngenerate\nd.\n\n\nAn \nAuthorizer\n parses the Authorization header of an HTTP request. The named constructors of \nAuthorizer\n indicate the required format of Authorization header. The \nAuthorization.bearer()\n constructor expects an OAuth 2.0 bearer token in the header, which has the following format:\n\n\nAuthorization\n:\n \nBearer\n \n768\niuzjkx82jkasjkd9z9\n\n\n\n\n\n\nAuthorizer.basic\n expects HTTP Basic Authentication, where the username and password are joined with the colon character (\n:\n) and Base 64-encoded:\n\n\n// \ndXNlcjpwYXNzd29yZA==\n is \nuser:password\n\nAuthorization: Basic dXNlcjpwYXNzd29yZA==\n\n\n\n\n\nIf the header can't be parsed, doesn't exist or is in the wrong format, an \nAuthorizer\n responds to the request with a 401 status code and prevents the next controller from receiving the request.\n\n\nOnce parsed, an \nAuthorizer\n sends the information - either the bearer token, or the username and password - to its \nAuthServer\n for verification. If the \nAuthServer\n rejects the authorization info, the \nAuthorizer\n responds to the request with a 401 status code and prevents the next controller from receiving the request. Otherwise, the request continues to the next controller.\n\n\nFor \nAuthorizer.bearer\n, the value in a request's header must be a valid, unexpired access token. These types of authorizers are used when an endpoint requires a logged in user.\n\n\nFor \nAuthorizer.basic\n authorizers, credentials are verified by finding an OAuth 2.0 client identifier and ensuring its client secret matches. Routes with this type of authorizer are known as \nclient authenticated\n routes. These types of authorizers are used when an endpoint requires a valid client application, but not a logged in user.\n\n\nAuthorizer and OAuth 2.0 Scope\n\n\nAn \nAuthorizer\n may restrict access to controllers based on the scope of the request's bearer token. By default, an \nAuthorizer.bearer\n allows any valid bearer token to pass through it. If desired, an \nAuthorizer\n is initialized with a list of required scopes. A request may only pass the \nAuthorizer\n if it has access to \nall\n scopes listed in the \nAuthorizer\n. For example, the following requires at least \nuser:posts\n and \nlocation\n scope:\n\n\nrouter\n\n  \n.\nroute\n(\n/checkin\n)\n\n  \n.\npipe\n(\nnew\n \nAuthorizer\n.\nbearer\n(\nauthServer\n,\n \nscopes:\n \n[\nuser:posts\n,\n \nlocation\n]))\n\n  \n.\ngenerate\n(()\n \n=\n \nnew\n \nCheckInController\n());\n\n\n\n\n\n\nNote that you don't have to use an \nAuthorizer\n to restrict access based on scope. A controller has access to scope information after the request has passed through an \nAuthorizer\n, so it can use the scope to make more granular authorization decisions.\n\n\nAuthorization Objects\n\n\nA bearer token represents a granted authorization - at some point in the past, a user provided their credentials and the token is the proof of that. When a bearer token is sent in the authorization header of an HTTP request, the application can look up which user the token is for and the client application it was issued for. This information is stored in an instance of \nAuthorization\n after the token has been verified and is assigned to \nRequest.authorization\n.\n\n\nControllers protected by an \nAuthorizer\n can access this information to further determine their behavior. For example, a social networking application might have a \n/news_feed\n endpoint protected by an \nAuthorizer\n. When an authenticated user makes a request for \n/news_feed\n, the controller will return that user's news feed. It can determine this by using the \nAuthorization\n:\n\n\nclass\n \nNewsFeedController\n \nextends\n \nHTTPController\n \n{\n\n  \n@\nhttpGet\n\n  \nFuture\nResponse\n \ngetNewsFeed\n()\n \nasync\n \n{\n\n    \nvar\n \nforUserID\n \n=\n \nrequest\n.\nauthorization\n.\nresourceOwnerIdentifier\n;\n\n\n    \nvar\n \nquery\n \n=\n \nnew\n \nQuery\nPost\n()\n\n      \n..\nwhere\n.\nauthor\n \n=\n \nwhereRelatedByValue\n(\nforUserID\n);\n\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\nawait\n \nquery\n.\nfetch\n());\n\n  \n}\n\n\n}\n\n\n\n\n\n\nIn the above controller, it's impossible for a user to access another user's posts.\n\n\nAuthorization\n objects also retain the scope of an access token so that a controller can make more granular decisions about the information/action in the endpoint. Checking whether an \nAuthorization\n has access to a particular scope is accomplished by either looking at the list of its \nscopes\n or using \nauthorizedForScope\n:\n\n\nclass\n \nNewsFeedController\n \nextends\n \nHTTPController\n \n{\n\n  \n@\nhttpGet\n\n  \nFuture\nResponse\n \ngetNewsFeed\n()\n \nasync\n \n{\n\n    \nif\n \n(\n!\nrequest\n.\nauthorization\n.\nauthorizedForScope\n(\nuser:feed\n))\n \n{\n\n      \nreturn\n \nnew\n \nResponse\n.\nunauthorized\n();\n\n    \n}\n\n\n    \nvar\n \nforUserID\n \n=\n \nrequest\n.\nauthorization\n.\nresourceOwnerIdentifier\n;\n\n\n    \nvar\n \nquery\n \n=\n \nnew\n \nQuery\nPost\n()\n\n      \n..\nwhere\n.\nauthor\n \n=\n \nwhereRelatedByValue\n(\nforUserID\n);\n\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\nawait\n \nquery\n.\nfetch\n());\n\n  \n}\n\n\n}\n\n\n\n\n\n\nUsing Authorizers Without AuthServer\n\n\nThroughout this guide, the argument to an instance of \nAuthorizer\n has been referred to as an \nAuthServer\n. This is true - but only because \nAuthServer\n implements \nAuthValidator\n. \nAuthValidator\n is an interface for verifying bearer tokens and username/password credentials.\n\n\nYou may use \nAuthorizer\n without using \nAuthServer\n. For example, an application that doesn't use OAuth 2.0 could provide its own \nAuthValidator\n interface to simply verify the username and password of every request:\n\n\nclass\n \nBasicValidator\n \nimplements\n \nAuthValidator\n \n{\n\n  \n@\noverride\n\n  \nFuture\nAuthorization\n \nfromBasicCredentials\n(\n\n      \nAuthBasicCredentials\n \nusernameAndPassword\n)\n \n{\n    \n    \nvar\n \nuser\n \n=\n \nawait\n \nuserForName\n(\nusernameAndPassword\n.\nusername\n);\n\n    \nif\n \n(\nuser\n.\npassword\n \n==\n \nhash\n(\nusernameAndPassword\n.\npassword\n,\n \nuser\n.\nsalt\n))\n \n{\n\n      \nreturn\n \nnew\n \nAuthorization\n(...);\n\n    \n}\n\n\n    \n// Will end up creating a 401 Not Authorized Response\n\n    \nreturn\n \nnull\n;\n\n  \n}\n\n\n  \n@\noverride\n\n  \nFuture\nAuthorization\n \nfromBearerToken\n(\n\n      \nString\n \nbearerToken\n,\n \n{\nList\nAuthScope\n \nscopesRequired\n})\n \n{\n\n    \nthrow\n \nnew\n \nHTTPResponseException\n(\n\n      \n400\n,\n \nInvalid Authorization. Bearer tokens not supported.\n);\n\n  \n}\n\n\n  \n// Used by `aqueduct document`.\n\n  \n@\noverride\n\n  \nList\nAPISecurityRequirement\n \nrequirementsForStrategy\n(\nAuthStrategy\n \nstrategy\n)\n \n=\n \n[];\n\n\n}", 
            "title": "Protecting Routes"
        }, 
        {
            "location": "/auth/authorizer/#securing-routes-with-authorizer", 
            "text": "Instances of  Authorizer  are added to a request controller channel to verify HTTP request's authorization information before passing the request onwards. They protect channel access and typically come right after  route . Here's an example:  @ override  void   setupRouter ( Router   router )   { \n   router \n     . route ( /protected ) \n     . pipe ( new   Authorizer . bearer ( authServer )) \n     . generate (()   =   new   ProtectedController ()); \n\n   router \n     . route ( /other ) \n     . pipe ( new   Authorizer . basic ( authServer )) \n     . generate (()   =   new   OtherProtectedController ());  }   An  Authorizer  has no state itself, so it is added to a channel via  pipe ; i.e., it does not need to be  generate d.  An  Authorizer  parses the Authorization header of an HTTP request. The named constructors of  Authorizer  indicate the required format of Authorization header. The  Authorization.bearer()  constructor expects an OAuth 2.0 bearer token in the header, which has the following format:  Authorization :   Bearer   768 iuzjkx82jkasjkd9z9   Authorizer.basic  expects HTTP Basic Authentication, where the username and password are joined with the colon character ( : ) and Base 64-encoded:  //  dXNlcjpwYXNzd29yZA==  is  user:password \nAuthorization: Basic dXNlcjpwYXNzd29yZA==  If the header can't be parsed, doesn't exist or is in the wrong format, an  Authorizer  responds to the request with a 401 status code and prevents the next controller from receiving the request.  Once parsed, an  Authorizer  sends the information - either the bearer token, or the username and password - to its  AuthServer  for verification. If the  AuthServer  rejects the authorization info, the  Authorizer  responds to the request with a 401 status code and prevents the next controller from receiving the request. Otherwise, the request continues to the next controller.  For  Authorizer.bearer , the value in a request's header must be a valid, unexpired access token. These types of authorizers are used when an endpoint requires a logged in user.  For  Authorizer.basic  authorizers, credentials are verified by finding an OAuth 2.0 client identifier and ensuring its client secret matches. Routes with this type of authorizer are known as  client authenticated  routes. These types of authorizers are used when an endpoint requires a valid client application, but not a logged in user.", 
            "title": "Securing Routes with Authorizer"
        }, 
        {
            "location": "/auth/authorizer/#authorizer-and-oauth-20-scope", 
            "text": "An  Authorizer  may restrict access to controllers based on the scope of the request's bearer token. By default, an  Authorizer.bearer  allows any valid bearer token to pass through it. If desired, an  Authorizer  is initialized with a list of required scopes. A request may only pass the  Authorizer  if it has access to  all  scopes listed in the  Authorizer . For example, the following requires at least  user:posts  and  location  scope:  router \n   . route ( /checkin ) \n   . pipe ( new   Authorizer . bearer ( authServer ,   scopes:   [ user:posts ,   location ])) \n   . generate (()   =   new   CheckInController ());   Note that you don't have to use an  Authorizer  to restrict access based on scope. A controller has access to scope information after the request has passed through an  Authorizer , so it can use the scope to make more granular authorization decisions.", 
            "title": "Authorizer and OAuth 2.0 Scope"
        }, 
        {
            "location": "/auth/authorizer/#authorization-objects", 
            "text": "A bearer token represents a granted authorization - at some point in the past, a user provided their credentials and the token is the proof of that. When a bearer token is sent in the authorization header of an HTTP request, the application can look up which user the token is for and the client application it was issued for. This information is stored in an instance of  Authorization  after the token has been verified and is assigned to  Request.authorization .  Controllers protected by an  Authorizer  can access this information to further determine their behavior. For example, a social networking application might have a  /news_feed  endpoint protected by an  Authorizer . When an authenticated user makes a request for  /news_feed , the controller will return that user's news feed. It can determine this by using the  Authorization :  class   NewsFeedController   extends   HTTPController   { \n   @ httpGet \n   Future Response   getNewsFeed ()   async   { \n     var   forUserID   =   request . authorization . resourceOwnerIdentifier ; \n\n     var   query   =   new   Query Post () \n       .. where . author   =   whereRelatedByValue ( forUserID ); \n\n     return   new   Response . ok ( await   query . fetch ()); \n   }  }   In the above controller, it's impossible for a user to access another user's posts.  Authorization  objects also retain the scope of an access token so that a controller can make more granular decisions about the information/action in the endpoint. Checking whether an  Authorization  has access to a particular scope is accomplished by either looking at the list of its  scopes  or using  authorizedForScope :  class   NewsFeedController   extends   HTTPController   { \n   @ httpGet \n   Future Response   getNewsFeed ()   async   { \n     if   ( ! request . authorization . authorizedForScope ( user:feed ))   { \n       return   new   Response . unauthorized (); \n     } \n\n     var   forUserID   =   request . authorization . resourceOwnerIdentifier ; \n\n     var   query   =   new   Query Post () \n       .. where . author   =   whereRelatedByValue ( forUserID ); \n\n     return   new   Response . ok ( await   query . fetch ()); \n   }  }", 
            "title": "Authorization Objects"
        }, 
        {
            "location": "/auth/authorizer/#using-authorizers-without-authserver", 
            "text": "Throughout this guide, the argument to an instance of  Authorizer  has been referred to as an  AuthServer . This is true - but only because  AuthServer  implements  AuthValidator .  AuthValidator  is an interface for verifying bearer tokens and username/password credentials.  You may use  Authorizer  without using  AuthServer . For example, an application that doesn't use OAuth 2.0 could provide its own  AuthValidator  interface to simply verify the username and password of every request:  class   BasicValidator   implements   AuthValidator   { \n   @ override \n   Future Authorization   fromBasicCredentials ( \n       AuthBasicCredentials   usernameAndPassword )   {     \n     var   user   =   await   userForName ( usernameAndPassword . username ); \n     if   ( user . password   ==   hash ( usernameAndPassword . password ,   user . salt ))   { \n       return   new   Authorization (...); \n     } \n\n     // Will end up creating a 401 Not Authorized Response \n     return   null ; \n   } \n\n   @ override \n   Future Authorization   fromBearerToken ( \n       String   bearerToken ,   { List AuthScope   scopesRequired })   { \n     throw   new   HTTPResponseException ( \n       400 ,   Invalid Authorization. Bearer tokens not supported. ); \n   } \n\n   // Used by `aqueduct document`. \n   @ override \n   List APISecurityRequirement   requirementsForStrategy ( AuthStrategy   strategy )   =   [];  }", 
            "title": "Using Authorizers Without AuthServer"
        }, 
        {
            "location": "/auth/controllers/", 
            "text": "Issue Access Tokens with AuthController\n\n\nAn application using Aqueduct's Auth framework must have endpoints to exchange credentials for access tokens. While a developer could implement these endpoints themselves and talk directly to an \nAuthServer\n, the OAuth 2.0 specification is where happiness goes to die. Therefore, there exist two \nRequestController\ns in Aqueduct that handle granting and refreshing authorization tokens - \nAuthController\n and \nAuthCodeController\n.\n\n\nIssue, Refresh and Exchange Tokens with AuthController\n\n\nAn \nAuthController\n grants access tokens and refreshes them. It also exchanges authorization codes obtained from \nAuthCodeController\n for access tokens.\n\n\nUsing an \nAuthController\n in an application is straightforward - hook it up to a \nRouter\n and pass it an \nAuthServer\n.\n\n\n@\noverride\n\n\nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n  \nrouter\n\n    \n.\nroute\n(\n/auth/token\n)\n\n    \n.\ngenerate\n(()\n \n=\n \nnew\n \nAuthController\n(\nauthServer\n));\n\n\n}\n\n\n\n\n\n\nTo grant an access token, a client application sends a HTTP \nPOST\n to the controller. The request must have:\n\n\n\n\nan Authorization header with the Client ID and Client Secret (if one exists) and,\n\n\na \nx-www-form-urlencoded\n body with the username and password of the authenticating user.\n\n\n\n\nThe body must also contain the key-value pair \ngrant_type=password\n. For example, the following Dart code will initiate successful authentication:\n\n\nvar\n \nclientID\n \n=\n \ncom.app.demo\n;\n\n\nvar\n \nclientSecret\n \n=\n \nmySecret\n;\n\n\nvar\n \nbody\n \n=\n \nusername=bob@stablekernel.com\npassword=foobar\ngrant_type=password\n;\n\n\nvar\n \nclientCredentials\n \n=\n \nnew\n \nBase64Encoder\n().\nconvert\n(\n$\nclientID\n:\n$\nclientSecret\n.\ncodeUnits\n);\n\n\n\nvar\n \nresponse\n \n=\n \nawait\n \nhttp\n.\npost\n(\n\n  \nhttps://stablekernel.com/auth/token\n,\n\n  \nheaders:\n \n{\n\n    \nContent-Type\n:\n \napplication/x-www-form-urlencoded\n,\n\n    \nAuthorization\n:\n \nBasic \n$\nclientCredentials\n\n  \n},\n\n  \nbody:\n \nbody\n);\n\n\n\n\n\n\nIf the OAuth 2.0 client ID is public - that is, it does not have a client secret - the secret is omitted from the authorization header:\n\n\n// Notice that the separating colon (:) is still present.\n\n\nvar\n \nclientCredentials\n \n=\n \nnew\n \nBase64Encoder\n().\nconvert\n(\n$\nclientID\n:\n.\ncodeUnits\n);\n\n\n\n\n\n\nThe response to a password token request is a JSON body that follows the OAuth 2.0 specification:\n\n\n{\n  \naccess_token\n: \n...\n\n  \nrefresh_token\n: \n...\n,\n  \nexpires_in\n: 3600,\n  \ntoken_type\n: \nbearer\n\n}\n\n\n\n\n\n\n\nThe \nexpires_in\n field is a computed property based on the delta of the issue date and expiration date. You should avoid manually editing the values for the columns \nissuedate\n and \nexpirationdate\n\n\n\n\nTokens are refreshed through the same endpoint, but with a payload that only contains the refresh token and \ngrant_type=refresh_token\n.\n\n\ngrant_type=refresh_token\nrefresh_token=kjasdiuz9u3namnsd\n\n\n\n\n\nSee \nAqueduct Auth CLI\n for more details on creating OAuth 2.0 client identifier and secrets.\n\n\nIf an Aqueduct application is using scope, an additional \nscope\n parameter can contain a space-delimited list of requested authorization scope. Only allowed scopes are returned and granted, and if no scopes are allowed then the request fails. If scope is provided, granted scope will be available in the response body.\n\n\nIt is important that an \nAuthorizer\n \nmust not\n protect instances of \nAuthController\n. The Authorization header is parsed and verified by \nAuthController\n.\n\n\nOnce granted, an access token can be used to pass \nAuthorizer.bearer()\ns in the request controller channel.\n\n\nIssue Authorization Codes with AuthCodeController\n\n\nAn \nAuthCodeController\n manages the OAuth 2.0 authorization code flow. The authorization code flow is used when an Aqueduct application allows third party applications access to authorized resources.\n\n\nLet's say you've built an Aqueduct application that allows people to store notes for themselves. Now, a friend approaches you with their application that is a to-do list. Instead of building their own note-taking feature, your friend wants users of their application to access the notes the user has stored in your application. While trustworthy, you don't want your friend to have access to the username and passwords of your subscribers.\n\n\nYour friend adds a link to their application that takes the user to an HTML page hosted by your server. The user enters their credentials in this page, which sends a \nPOST\n request to your server. Your server responds by redirecting the user's browser back into your friend's application. An \nauthorization code\n is included in the query string of the redirect URL.\n\n\nYour friend's application parses the code from the URL and sends it to \ntheir\n server. Behind the scenes, their server exchanges this code with your server for an access token.\n\n\nAn \nAuthCodeController\n responds to both \nGET\n and \nPOST\n requests. When issued a \nGET\n, it serves up a HTML page with a login form. This login form's submit action sends a \nPOST\n to the same endpoint with the username and password of the user. Upon success, the response from the \nPOST\n is a 302 redirect with an authorization code.\n\n\nSetting up an \nAuthCodeController\n is nearly as simple as setting up an \nAuthController\n, but requires a function that renders the HTML login form. Here's an example:\n\n\n@\noverride\n\n\nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n  \nrouter\n\n    \n.\nroute\n(\n/auth/code\n)\n\n    \n.\ngenerate\n(()\n \n=\n \nnew\n \nAuthCodeController\n(\n\n      \nauthServer\n,\n \nrenderAuthorizationPageHTML:\n \nrenderLogin\n));\n\n\n}\n\n\n\nFuture\nString\n \nrenderLogin\n(\n\n    \nAuthCodeController\n \nrequestingController\n,\n\n    \nURI\n \nrequestURI\n,\n\n    \nMap\nString\n,\n \nString\n \nqueryParameters\n)\n \n{\n\n  \nvar\n \nhtml\n \n=\n \nHTMLRenderer\n.\ntemplateWithSubstitutions\n(\n\n    \nweb/login.html\n,\n \nrequestURI\n,\n \nqueryParameters\n);\n\n\n  \nreturn\n \nhtml\n;\n\n\n}\n\n\n\n\n\n\nIt is important that all values passed to HTML rendering function are sent in the form's query parameters - they contain necessary security components and scope information.\n\n\nWhen your friend's application links to your login page - \nGET /auth/code\n - they must include three query parameters: \nstate\n, \nclient_id\n, \nresponse_type\n. They may optionally include \nscope\n.\n\n\nhttps://stablekernel.com/auth/code?client_id=friend.app\nresponse_type=code\nstate=87uijn3rkja\n\n\n\n\n\nThe value of \nclient_id\n must be created specifically for your friend's application and stored in your database. (See more on generating client identifiers with \naqueduct auth\n in \nAqueduct Auth CLI\n.) The \nresponse_type\n must always be \ncode\n. The \nstate\n must be a value your friend's application creates - it is often some random value like a session cookie.\n\n\nWhen a user of your friend's application goes through this process, they are redirected back into your friend's application. Both the generated authorization code and the value for \nstate\n will be query parameters in the URL. That redirect URL will look like:\n\n\nhttps://friends.app/code_callback?code=abcd672kk\nstate=87uijn3rkja\n\n\n\n\n\nThe redirect URL is pre-determined when generating the client identifier with \naqueduct auth\n.\n\n\nYour friend's application verifies that \nstate\n matches the \nstate\n they sent in \nGET /auth/code\n. They then send the \ncode\n to their server. The server then exchanges this code with your server by issuing a \nPOST\n to an \nAuthController\n - \nNOT\n the \nAuthCodeController\n - with the following \napplication/x-www-form-urlencoded\n body:\n\n\ngrant_type=authorization_code\ncode=abcd672kk\n\n\n\n\n\nAn access token will be returned to the server which your friend then stores in their database. Whenever one of their users makes a request that requires accessing your application's data, they will execute requests with that access token.", 
            "title": "Issuing Access Tokens"
        }, 
        {
            "location": "/auth/controllers/#issue-access-tokens-with-authcontroller", 
            "text": "An application using Aqueduct's Auth framework must have endpoints to exchange credentials for access tokens. While a developer could implement these endpoints themselves and talk directly to an  AuthServer , the OAuth 2.0 specification is where happiness goes to die. Therefore, there exist two  RequestController s in Aqueduct that handle granting and refreshing authorization tokens -  AuthController  and  AuthCodeController .", 
            "title": "Issue Access Tokens with AuthController"
        }, 
        {
            "location": "/auth/controllers/#issue-refresh-and-exchange-tokens-with-authcontroller", 
            "text": "An  AuthController  grants access tokens and refreshes them. It also exchanges authorization codes obtained from  AuthCodeController  for access tokens.  Using an  AuthController  in an application is straightforward - hook it up to a  Router  and pass it an  AuthServer .  @ override  void   setupRouter ( Router   router )   { \n   router \n     . route ( /auth/token ) \n     . generate (()   =   new   AuthController ( authServer ));  }   To grant an access token, a client application sends a HTTP  POST  to the controller. The request must have:   an Authorization header with the Client ID and Client Secret (if one exists) and,  a  x-www-form-urlencoded  body with the username and password of the authenticating user.   The body must also contain the key-value pair  grant_type=password . For example, the following Dart code will initiate successful authentication:  var   clientID   =   com.app.demo ;  var   clientSecret   =   mySecret ;  var   body   =   username=bob@stablekernel.com password=foobar grant_type=password ;  var   clientCredentials   =   new   Base64Encoder (). convert ( $ clientID : $ clientSecret . codeUnits );  var   response   =   await   http . post ( \n   https://stablekernel.com/auth/token , \n   headers:   { \n     Content-Type :   application/x-www-form-urlencoded , \n     Authorization :   Basic  $ clientCredentials \n   }, \n   body:   body );   If the OAuth 2.0 client ID is public - that is, it does not have a client secret - the secret is omitted from the authorization header:  // Notice that the separating colon (:) is still present.  var   clientCredentials   =   new   Base64Encoder (). convert ( $ clientID : . codeUnits );   The response to a password token request is a JSON body that follows the OAuth 2.0 specification:  {\n   access_token :  ... \n   refresh_token :  ... ,\n   expires_in : 3600,\n   token_type :  bearer \n}   The  expires_in  field is a computed property based on the delta of the issue date and expiration date. You should avoid manually editing the values for the columns  issuedate  and  expirationdate   Tokens are refreshed through the same endpoint, but with a payload that only contains the refresh token and  grant_type=refresh_token .  grant_type=refresh_token refresh_token=kjasdiuz9u3namnsd  See  Aqueduct Auth CLI  for more details on creating OAuth 2.0 client identifier and secrets.  If an Aqueduct application is using scope, an additional  scope  parameter can contain a space-delimited list of requested authorization scope. Only allowed scopes are returned and granted, and if no scopes are allowed then the request fails. If scope is provided, granted scope will be available in the response body.  It is important that an  Authorizer   must not  protect instances of  AuthController . The Authorization header is parsed and verified by  AuthController .  Once granted, an access token can be used to pass  Authorizer.bearer() s in the request controller channel.", 
            "title": "Issue, Refresh and Exchange Tokens with AuthController"
        }, 
        {
            "location": "/auth/controllers/#issue-authorization-codes-with-authcodecontroller", 
            "text": "An  AuthCodeController  manages the OAuth 2.0 authorization code flow. The authorization code flow is used when an Aqueduct application allows third party applications access to authorized resources.  Let's say you've built an Aqueduct application that allows people to store notes for themselves. Now, a friend approaches you with their application that is a to-do list. Instead of building their own note-taking feature, your friend wants users of their application to access the notes the user has stored in your application. While trustworthy, you don't want your friend to have access to the username and passwords of your subscribers.  Your friend adds a link to their application that takes the user to an HTML page hosted by your server. The user enters their credentials in this page, which sends a  POST  request to your server. Your server responds by redirecting the user's browser back into your friend's application. An  authorization code  is included in the query string of the redirect URL.  Your friend's application parses the code from the URL and sends it to  their  server. Behind the scenes, their server exchanges this code with your server for an access token.  An  AuthCodeController  responds to both  GET  and  POST  requests. When issued a  GET , it serves up a HTML page with a login form. This login form's submit action sends a  POST  to the same endpoint with the username and password of the user. Upon success, the response from the  POST  is a 302 redirect with an authorization code.  Setting up an  AuthCodeController  is nearly as simple as setting up an  AuthController , but requires a function that renders the HTML login form. Here's an example:  @ override  void   setupRouter ( Router   router )   { \n   router \n     . route ( /auth/code ) \n     . generate (()   =   new   AuthCodeController ( \n       authServer ,   renderAuthorizationPageHTML:   renderLogin ));  }  Future String   renderLogin ( \n     AuthCodeController   requestingController , \n     URI   requestURI , \n     Map String ,   String   queryParameters )   { \n   var   html   =   HTMLRenderer . templateWithSubstitutions ( \n     web/login.html ,   requestURI ,   queryParameters ); \n\n   return   html ;  }   It is important that all values passed to HTML rendering function are sent in the form's query parameters - they contain necessary security components and scope information.  When your friend's application links to your login page -  GET /auth/code  - they must include three query parameters:  state ,  client_id ,  response_type . They may optionally include  scope .  https://stablekernel.com/auth/code?client_id=friend.app response_type=code state=87uijn3rkja  The value of  client_id  must be created specifically for your friend's application and stored in your database. (See more on generating client identifiers with  aqueduct auth  in  Aqueduct Auth CLI .) The  response_type  must always be  code . The  state  must be a value your friend's application creates - it is often some random value like a session cookie.  When a user of your friend's application goes through this process, they are redirected back into your friend's application. Both the generated authorization code and the value for  state  will be query parameters in the URL. That redirect URL will look like:  https://friends.app/code_callback?code=abcd672kk state=87uijn3rkja  The redirect URL is pre-determined when generating the client identifier with  aqueduct auth .  Your friend's application verifies that  state  matches the  state  they sent in  GET /auth/code . They then send the  code  to their server. The server then exchanges this code with your server by issuing a  POST  to an  AuthController  -  NOT  the  AuthCodeController  - with the following  application/x-www-form-urlencoded  body:  grant_type=authorization_code code=abcd672kk  An access token will be returned to the server which your friend then stores in their database. Whenever one of their users makes a request that requires accessing your application's data, they will execute requests with that access token.", 
            "title": "Issue Authorization Codes with AuthCodeController"
        }, 
        {
            "location": "/auth/cli/", 
            "text": "Manage OAuth 2.0 Clients\n\n\nThe \naqueduct auth\n command line tool creates OAuth 2.0 client application identifiers and inserts them into an application's database. To use this tool, you must use \nManagedAuthStorage\nT\n and\nyour database must be contain the tables to support it (see \nthis guide\n for more details).\n\n\nExchanging a username and password for an authorization token requires a registered client identifier. A token belongs to both the authenticating user and the client application. Clients are represented by instances of \nManagedClient\n from \naqueduct/managed_auth\n. Authenticating clients must provide their client ID (and client secret, if applicable) in the Authorization header when requesting access tokens.\n\n\nAn OAuth 2.0 client must have a string identifier that uniquely identifies the client. For example, \ncom.food_app.mobile\n may be a client identifier for the mobile applications for some 'Food App'.\n\n\nTo create a simple OAuth 2.0 client, the following command line utility can be run:\n\n\naqueduct auth add-client \\\n  --id com.food_app.mobile \\\n  --connect postgres://user:password@dbhost:5432/food_app\n\n\n\n\n\nThe \nconnect\n option identifies the database for the application, which this tool will connect to and insert a record into the \nManagedClient\n database table. The identifier is provided through the \nid\n option.\n\n\nAn OAuth 2.0 client created in this way is a \npublic\n client; there is no client secret. An OAuth 2.0 client that uses the resource owner grant flow, but cannot secure its client secret, should use this type of client. An application can't secure its client secret if its source code is viewable - like any JavaScript application. It is suggested that native mobile applications also use public clients because their source code could potentially disassembled to reveal a client secret, but isn't necessarily required.\n\n\nWhen making requests to client authenticated endpoints (those protected with \nAuthorizer.basic\n), the client secret is omitted from the authorization header. The string to base64 encode is \nclientID:\n, where the colon (\n:\n) is required. For example, to generate an authorization header in Dart for a public client:\n\n\nvar\n \nclientID\n \n=\n \ncom.foobar.xyz\n;\n\n\nvar\n \nclientCredentials\n \n=\n \nnew\n \nBase64Encoder\n()\n.\nconvert\n(\n$clientID:\n.\ncodeUnits\n);\n\n\nvar\n \nheader\n \n=\n \nBasic $clientCredentials\n;\n\n\n\n\n\n\nConfidential Clients\n\n\nAn OAuth 2.0 client is \nconfidential\n if it has a client secret. Client secrets can be provided with the \nauth\n tool:\n\n\naqueduct auth add-client \\\n  --id com.food_app.mobile \\\n  --secret myspecialsecret \\\n  --connect postgres://user:password@dbhost:5432/food_app\n\n\n\n\n\nClient secrets are hashed (many times) with a randomly generated salt before they are stored. Therefore, their actual value must be stored securely elsewhere. (We use LastPass, for example.)\n\n\nRedirect URIs\n\n\nTo allow the authorization code flow (provided by \nAuthCodeController\n), a client must have a redirect URI. This is the URI that an authenticating user's browser will be redirected to after entering their username and password. A client must be a confidential client to have a redirect URI.\n\n\naqueduct auth add-client \\\n  --id com.food_app.mobile \\\n  --secret myspecialsecret \\\n  --redirect-uri https://someapp.com/callback \\\n  --connect postgres://user:password@dbhost:5432/food_app\n\n\n\n\n\nScopes\n\n\nIf an application is using OAuth 2.0 scopes, a client can have scopes that it allows tokens to have access to. This allows scopes to be restricted by the client they are authenticating with.\n\n\naqueduct auth add-client \\\n  --id com.food_app.mobile \\\n  --secret myspecialsecret \\\n  --allowed-scopes \nscopeA scopeB scopeC.readonly\n \\\n  --connect postgres://user:password@dbhost:5432/food_app\n\n\n\n\n\nScopes are space-delimited and must be enclosed in quotes so that your shell will treat the entire string as one value.\n\n\nScope may be set after a client has already been created with \naqueduct auth set-scope\n:\n\n\naqueduct auth set-scope \\\n  --id com.food_app.mobile \\\n  --scopes \nscopeA scopeC\n \\\n  --connect postgres://user:password@dbhost:5432/food_app\n\n\n\n\n\nOther Info\n\n\nLike all \naqueduct\n commands that send commands to a database, the \nconnect\n option can be replaced by a \ndatabase.yaml\n file in the project directory with the following format:\n\n\nusername: \nuser\n\npassword: \npassword\n\nhost: \nhost\n\nport: 5432\ndatabaseName: \nmy_app", 
            "title": "Managing OAuth 2.0 Clients"
        }, 
        {
            "location": "/auth/cli/#manage-oauth-20-clients", 
            "text": "The  aqueduct auth  command line tool creates OAuth 2.0 client application identifiers and inserts them into an application's database. To use this tool, you must use  ManagedAuthStorage T  and\nyour database must be contain the tables to support it (see  this guide  for more details).  Exchanging a username and password for an authorization token requires a registered client identifier. A token belongs to both the authenticating user and the client application. Clients are represented by instances of  ManagedClient  from  aqueduct/managed_auth . Authenticating clients must provide their client ID (and client secret, if applicable) in the Authorization header when requesting access tokens.  An OAuth 2.0 client must have a string identifier that uniquely identifies the client. For example,  com.food_app.mobile  may be a client identifier for the mobile applications for some 'Food App'.  To create a simple OAuth 2.0 client, the following command line utility can be run:  aqueduct auth add-client \\\n  --id com.food_app.mobile \\\n  --connect postgres://user:password@dbhost:5432/food_app  The  connect  option identifies the database for the application, which this tool will connect to and insert a record into the  ManagedClient  database table. The identifier is provided through the  id  option.  An OAuth 2.0 client created in this way is a  public  client; there is no client secret. An OAuth 2.0 client that uses the resource owner grant flow, but cannot secure its client secret, should use this type of client. An application can't secure its client secret if its source code is viewable - like any JavaScript application. It is suggested that native mobile applications also use public clients because their source code could potentially disassembled to reveal a client secret, but isn't necessarily required.  When making requests to client authenticated endpoints (those protected with  Authorizer.basic ), the client secret is omitted from the authorization header. The string to base64 encode is  clientID: , where the colon ( : ) is required. For example, to generate an authorization header in Dart for a public client:  var   clientID   =   com.foobar.xyz ;  var   clientCredentials   =   new   Base64Encoder () . convert ( $clientID: . codeUnits );  var   header   =   Basic $clientCredentials ;", 
            "title": "Manage OAuth 2.0 Clients"
        }, 
        {
            "location": "/auth/cli/#confidential-clients", 
            "text": "An OAuth 2.0 client is  confidential  if it has a client secret. Client secrets can be provided with the  auth  tool:  aqueduct auth add-client \\\n  --id com.food_app.mobile \\\n  --secret myspecialsecret \\\n  --connect postgres://user:password@dbhost:5432/food_app  Client secrets are hashed (many times) with a randomly generated salt before they are stored. Therefore, their actual value must be stored securely elsewhere. (We use LastPass, for example.)", 
            "title": "Confidential Clients"
        }, 
        {
            "location": "/auth/cli/#redirect-uris", 
            "text": "To allow the authorization code flow (provided by  AuthCodeController ), a client must have a redirect URI. This is the URI that an authenticating user's browser will be redirected to after entering their username and password. A client must be a confidential client to have a redirect URI.  aqueduct auth add-client \\\n  --id com.food_app.mobile \\\n  --secret myspecialsecret \\\n  --redirect-uri https://someapp.com/callback \\\n  --connect postgres://user:password@dbhost:5432/food_app", 
            "title": "Redirect URIs"
        }, 
        {
            "location": "/auth/cli/#scopes", 
            "text": "If an application is using OAuth 2.0 scopes, a client can have scopes that it allows tokens to have access to. This allows scopes to be restricted by the client they are authenticating with.  aqueduct auth add-client \\\n  --id com.food_app.mobile \\\n  --secret myspecialsecret \\\n  --allowed-scopes  scopeA scopeB scopeC.readonly  \\\n  --connect postgres://user:password@dbhost:5432/food_app  Scopes are space-delimited and must be enclosed in quotes so that your shell will treat the entire string as one value.  Scope may be set after a client has already been created with  aqueduct auth set-scope :  aqueduct auth set-scope \\\n  --id com.food_app.mobile \\\n  --scopes  scopeA scopeC  \\\n  --connect postgres://user:password@dbhost:5432/food_app", 
            "title": "Scopes"
        }, 
        {
            "location": "/auth/cli/#other-info", 
            "text": "Like all  aqueduct  commands that send commands to a database, the  connect  option can be replaced by a  database.yaml  file in the project directory with the following format:  username:  user \npassword:  password \nhost:  host \nport: 5432\ndatabaseName:  my_app", 
            "title": "Other Info"
        }, 
        {
            "location": "/auth/auth_scopes/", 
            "text": "Granular Authorization with OAuth 2.0 Scopes\n\n\nA simple Aqueduct Auth implementation grants access tokens and protects resources by ensuring a request has a valid access token. As an application grows, it may need more granular protection than simply, \"Oh, you have a token? Ok, access whatever you want!\" For example, an ordinary user shouldn't be able to access administration resources.\n\n\nOne approach to granular authorization is using \nroles\n. A role - like \nadmin\n and \nuser\n - exist in a hierarchical model for granting permission. An \nadmin\n can access everything a \nuser\n can and more. This approach is a valid part of a granular authorization scheme, but a simple \nadmin \n user\n model falls apart as permissions diverge. Likewise, it is difficult to integrate third party applications that should only have access to a subset of a user's data.\n\n\nFor example, Google offers lots of services - from email, to analytics data, to document hosting - all of this is accessible to users identified by their Gmail address. These services have their own application, and third party applications may access these services on behalf of the user. As a user, I do not want an third-party application that presents my documents from Google Drive to also have access my email. But I still want to login to both services with the same email and password.\n\n\nOAuth 2.0 solves this problem with \nscope\n. A scope is a string that identifies access to some resource or action. For example, the scope to read, send and delete your email might be simply called \"gmail\". But another scope, \"gmail.readonly\", can only read email - it can't send or delete it. Likewise, the \"analytics\" scope may let me read analytic data for my websites, but it'll never see my email, much less send one.\n\n\nScope is different than a role because it belongs to the access token, not the user. A user can have multiple access tokens for different applications, each with different scope and therefore different access control.\n\n\nScope Format and Hierarchy\n\n\nThere is no definitive guide on what a scope string should look like, other than being restricted to alphanumeric characters and some symbols. Aqueduct, however, imposes a simple scoping structure.\n\n\nHierarchy is specified by the \n:\n character. For example, the following is a hierarchy of scopes related to a user and its belongings:\n\n\n\n\nuser\n\n\nuser:email\n\n\nuser:documents\n\n\nuser:documents:spreadsheets\n\n\n\n\nNotice how these scopes form a hierarchy. Each segment makes the scope more restrictive. A scope that begins with the same segments has access to a scope with more segments, e.g. \nuser:documents\n has access to \nuser:documents:spreadsheets\n, but \nuser:documents\n cannot access \nuser:email\n. The \nuser\n scope can access email, documents and anything else a user might have.\n\n\nScopes are validated by the method \nAuthorization.authorizedForScope()\n. Once a \nRequest\n passes through an \nAuthorizer\n, it will have a valid \nauthorization\n property. If the access token has scopes, this method can be used to ensure it has the appropriate scope for the resource or action. For example, the following will verify that a request has at least \nuser:email\n access - either \nuser:email\n \nor\n the \nuser\n scope.\n\n\n@\nhttpGet\n\n\nFuture\nResponse\n \ngetInbox\n()\n \nasync\n \n{\n\n  \nif\n \n(\n!\nrequest\n.\nauthorization\n.\nauthorizedForScope\n(\nuser:email\n))\n \n{\n\n    \nreturn\n \nnew\n \nResponse\n.\nunauthorized\n();\n\n  \n}\n\n\n  \n...\n\n\n}\n\n\n\n\n\n\nIt is often the case where a scope might have further restrictions - like readonly vs. write. You may introduce scopes like \nuser:email:read\n and \nuser:email:write\n, but \nuser:email:write\n would not have access to \nuser:email:read\n following the previous logic.\n\n\nThis is where \nscope modifiers\n come in. A scope modifier is a \n.\n-prefixed string at the end of a scope. For example, \nuser:email.readonly\n grants readonly access to a user's email. An access token without a modifier has access to a scope with the same hierarchy and \nany\n modifier. Thus, \nuser\n and \nuser:email\n can both access \nuser:email.readonly\n protected resources and actions, but \nuser:email.readonly\n cannot access things protected by \nuser:email\n.\n\n\nA scope modifier is only valid for the last segment of a scope string. That is, \nuser:documents.readonly:spreadsheets\n is not valid, but \nuser:documents:spreadsheets.readonly\n is.\n\n\nRequesting Scope\n\n\nScope is requested by a client application when it is authenticating a user. For example, the form data to request the \nuser:email\n scope on behalf of \nbob@stablekernel.com\n looks like this:\n\n\nusername=bob@stablekernel.com\npassword=foobarxyz123\ngrant_type=password\nscope=user:email\n\n\nMultiple scopes can be requested for an access token, which \nmust\n be separated by spaces. (Note these query parameters must be percent-encoded, but are shown here without percent-encoding to aid visibility.)\n\n\nusername=bob@stablekernel.com\npassword=foobarxyz123\ngrant_type=password\nscope=user:email user:documents\n\n\nWhen using the authorization code flow, the requested scope is provided by the third party application in the query string of the initial \nGET /auth/code\n:\n\n\nGET /auth/code?grant_type=code\nclient_id=com.foo.bar\nstate=k3j4kjas\nscope=user:email\n\n\nThe webpage served by from this endpoint should alert the user to the scopes the application is requesting.\n\n\nAdding and Managing Scope\n\n\nAn \nAuthServer\n validates that the scopes requested for an access token are valid for the authenticating client application. Therefore, each client identifier (a \nManagedClient\n) may have a list of allowed scopes. The allowed scopes are configured with the \naqueduct auth command-line tool\n. For example, the following creates a new client identifier with access to the scopes \nuser:email\n and \nuser:documents\n, and then later adds \nuser:location\n:\n\n\naqueduct auth add-client \n\\\n\n  --id com.app.mobile \n\\\n\n  --secret myspecialsecret \n\\\n\n  --allowed-scopes \nuser:email user:documents\n \n\\\n\n  --connect postgres://user:password@dbhost:5432/db_name\n\naqueduct auth set-scope \n\\\n\n  --id com.app.mobile \n\\\n\n  --scopes \nuser:email user:documents user:location\n \n\\\n\n  --connect postgres://user:password@dbhost:5432/db_name\n\n\n\n\n\nOnce a client has scopes, any access token request from this client \nmust\n contain a list of desired scopes. Aqueduct does not implicitly grant scopes when a request omits them.\n\n\nThe \nAuthServer\n will only grant scopes that the client has access to. If some of the scopes in a request aren't valid for the client, the token may still be granted, but any disallowed scopes will be removed. For example, requesting the scopes \nuser:email\n and \nuser:settings\n would return an access token that only granted \nuser:email\n:\n\n\n{\n\n  \naccess_token\n:\n \n...\n,\n\n  \nrefresh_token\n:\n \n...\n,\n\n  \ntoken_type\n:\n \nbearer\n,\n\n  \nexpires_in\n:\n \n3600\n,\n\n  \nscopes\n:\n \nuser:email\n\n\n}\n\n\n\n\n\n\nIf none of the requested scopes are allowed, the access token will \nnot\n be granted and the request will yield an error response.\n\n\nIf the client identifier has not been configured with scopes - either because the application doesn't use scopes or this particular client doesn't have any - scopes specified in an authenticating request are ignored. A token will be granted in this scenario, but will have no scope. The \nscope\n key is omitted from the token payload.\n\n\nIt is important to ensure that an application that uses scope has protections on its resources (see \na later section\n).\n\n\nUser-based Scope Management\n\n\nAdding scopes to client identifiers is a requirement for any application that wishes to use scoping. An application may optionally add restrictions to scope depending on some attribute(s) of the user. When authenticating, the server first filters the list of requested scopes by what is allowed for the client, and then filters the resulting list by what is allowed for the user.\n\n\nThis user-level filtering is done by overriding \nallowedScopesForAuthenticatable\n in \nAuthStorage\n. By default, this method returns \nAuthScope.Any\n - which means there are no restrictions. If the client application allows the scope, then any user that logs in with that application can request that scope.\n\n\nThis method may return a list of \nAuthScope\ns that are valid for the authenticating user. The following example shows a \nManagedAuthStorage\nT\n subclass that allows any scope for \n@stablekernel.com\n usernames, no scopes for \n@hotmail.com\n addresses and some limited scope for everyone else:\n\n\nclass\n \nDomainBasedAuthStorage\n \nextends\n \nManagedAuthStorage\nUser\n \n{\n\n  \nDomainBasedAuthStorage\n(\nManagedContext\n \ncontext\n,\n \n{\nint\n \ntokenLimit:\n \n40\n})\n \n:\n\n        \nsuper\n(\ncontext\n,\n \ntokenLimit:\n \ntokenLimit\n);\n\n\n  \n@\noverride\n\n  \nList\nAuthScope\n \nallowedScopesForAuthenticatable\n(\ncovariant\n \nUser\n \nuser\n)\n \n{\n\n    \nif\n \n(\nuser\n.\nusername\n.\nendsWith\n(\n@stablekernel.com\n))\n \n{\n\n      \nreturn\n \nAuthScope\n.\nAny\n;\n\n    \n}\n \nelse\n \nif\n \n(\nuser\n.\nusername\n.\nendsWith\n(\n@hotmail.com\n))\n \n{\n\n      \nreturn\n \n[];\n\n    \n}\n \nelse\n \n{\n\n      \nreturn\n \n[\nnew\n \nAuthScope\n(\nuser\n)];\n\n    \n}\n\n  \n}\n      \n\n}\n\n\n\n\n\n\nThe \nuser\n passed to \nallowedScopesForAuthenticatable\n is the user being authenticated. It will have previously been fetched by the \nAuthServer\n. The \nAuthServer\n fetches this object by invoking \nAuthStorage.fetchAuthenticatableByUsername()\n. The default implementation of this method for \nManagedAuthStorage\nT\n only fetches the \nid\n, \nusername\n, \nsalt\n and \nhashedPassword\n of the user. This is for two reasons:\n\n\n\n\nThese properties are needed to verify and grant an access token.\n\n\nThe \nAuthServer\n can only guarantee that the \nUser\n implements \nAuthenticatable\n, and those are the only properties it has.\n\n\n\n\nWhen using some other attribute of an application's user object to restrict allowed scopes, you must also override \nfetchAuthenticatableByUsername\n to fetch these attributes. For example, if your application's user has a \nrole\n attribute, you must fetch it and the other four required properties. Here's an example implementation:\n\n\nclass\n \nRoleBasedAuthStorage\n \nextends\n \nManagedAuthStorage\nUser\n \n{\n\n  \nRoleBasedAuthStorage\n(\nManagedContext\n \ncontext\n,\n \n{\nint\n \ntokenLimit:\n \n40\n})\n \n:\n\n        \nsuper\n(\ncontext\n,\n \ntokenLimit:\n \ntokenLimit\n);\n\n\n  \n@\noverride\n\n  \nFuture\nUser\n \nfetchAuthenticatableByUsername\n(\n\n      \nAuthServer\n \nserver\n,\n \nString\n \nusername\n)\n \n{\n\n    \nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n(\ncontext\n)\n\n      \n..\nwhere\n.\nusername\n \n=\n \nusername\n\n      \n..\nreturningProperties\n((\nt\n)\n \n=\n\n        \n[\nt\n.\nid\n,\n \nt\n.\nusername\n,\n \nt\n.\nhashedPassword\n,\n \nt\n.\nsalt\n,\n \nt\n.\nrole\n]);\n\n\n    \nreturn\n \nquery\n.\nfetchOne\n();\n\n  \n}\n\n\n  \n@\noverride\n\n  \nList\nAuthScope\n \nallowedScopesForAuthenticatable\n(\ncovariant\n \nUser\n \nuser\n)\n \n{\n\n    \nvar\n \nscopeStrings\n \n=\n \n[];\n\n    \nif\n \n(\nuser\n.\nrole\n \n==\n \nadmin\n)\n \n{\n\n      \nscopeStrings\n \n=\n \n[\nadmin\n,\n \nuser\n];\n\n    \n}\n \nelse\n \nif\n \n(\nuser\n.\nrole\n \n==\n \nuser\n)\n \n{\n\n      \nscopeStrings\n \n=\n \n[\nuser:email\n];\n\n    \n}\n\n\n    \nreturn\n \nscopeStrings\n.\nmap\n((\nstr\n)\n \n=\n \nnew\n \nAuthScope\n(\nstr\n)).\ntoList\n();\n\n  \n}\n\n\n}\n\n\n\n\n\n\nIf you do not fetch the four required properties declared in \nAuthenticatable\n, an \nAuthServer\n will fail in spectacular ways.\n\n\nUsing Scope to Protect Resources\n\n\nAn \nAuthorizer.bearer()\n can require an access token to have certain scopes before passing it down the channel:\n\n\nrouter\n\n  \n.\nroute\n(\n/email_attachments\n)\n\n  \n.\npipe\n(\nnew\n \nAuthorizer\n.\nbearer\n(\nauthServer\n,\n \nscopes:\n \n[\nuser:email\n,\n \nuser:documents\n]))\n\n  \n.\ngenerate\n(()\n \n=\n \nnew\n \nSecureStuffController\n());\n\n\n\n\n\n\nA request's token must have all of the scopes declared by the \nAuthorizer\n - in this case, \nboth\n \"user:email\" and \"user:documents\" (or \"user\", of course).\n\n\nThis type of protection is often useful, but within a particular controller you may want finer control. For example, you may want to require a different level of access to \nPOST\n than \nGET\n. You may check if an authorization has valid scopes at any time:\n\n\nclass\n \nEmailController\n \nextend\n \nHTTPController\n \n{\n\n  \n@\nhttpGet\n\n  \nFuture\nResponse\n \ngetEmail\n()\n \nasync\n \n{\n\n    \nif\n \n(\n!\nrequest\n.\nauthorization\n.\nauthorizedForScope\n(\nuser:email.readonly\n))\n \n{\n\n      \nreturn\n \nnew\n \nResponse\n.\nunauthorized\n();\n\n    \n}\n\n\n    \nvar\n \ninbox\n \n=\n \nawait\n \nemailForUser\n(\nrequest\n.\nauthorization\n.\nresourceOwnerIdentifier\n);\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\ninbox\n);\n\n  \n}\n\n\n  \n@\nhttpPost\n\n  \nFuture\nResponse\n \nsendEmail\n(\n@\nHTTPBody\n()\n \nEmail\n \nemail\n)\n \nasync\n \n{\n\n    \nif\n \n(\n!\nrequest\n.\nauthorization\n.\nauthorizedForScope\n(\nuser:email\n))\n \n{\n\n      \nreturn\n \nnew\n \nResponse\n.\nunauthorized\n();\n\n    \n}\n\n    \nawait\n \nsendEmail\n(\nemail\n);\n\n\n    \nreturn\n \nnew\n \nResponse\n.\naccepted\n();\n\n  \n}\n\n\n}\n\n\n\n\n\n\nNote that scopes are not the only way to secure resources, even if they are being used. For example, you may want to restrict the endpoint \n/user/1/settings\n to only allow the user with \nid=1\n to access it:\n\n\n@\nhttpGet\n\n\nFuture\nResponse\n \ngetUserSettings\n(\n@\nHTTPPath\n(\nid\n)\n \nint\n \nid\n)\n \nasync\n \n{\n\n  \nif\n \n(\nrequest\n.\nauthorization\n.\nresourceOwnerIdentifier\n \n!=\n \nid\n)\n \n{\n\n    \nreturn\n \nnew\n \nResponse\n.\nunauthorized\n();\n\n  \n}\n\n\n  \nreturn\n \nnew\n \nResponse\n.\nok\n(\nawait\n \nsettingsForUser\n(\nid\n));\n\n\n}", 
            "title": "OAuth 2.0 Scoping"
        }, 
        {
            "location": "/auth/auth_scopes/#granular-authorization-with-oauth-20-scopes", 
            "text": "A simple Aqueduct Auth implementation grants access tokens and protects resources by ensuring a request has a valid access token. As an application grows, it may need more granular protection than simply, \"Oh, you have a token? Ok, access whatever you want!\" For example, an ordinary user shouldn't be able to access administration resources.  One approach to granular authorization is using  roles . A role - like  admin  and  user  - exist in a hierarchical model for granting permission. An  admin  can access everything a  user  can and more. This approach is a valid part of a granular authorization scheme, but a simple  admin   user  model falls apart as permissions diverge. Likewise, it is difficult to integrate third party applications that should only have access to a subset of a user's data.  For example, Google offers lots of services - from email, to analytics data, to document hosting - all of this is accessible to users identified by their Gmail address. These services have their own application, and third party applications may access these services on behalf of the user. As a user, I do not want an third-party application that presents my documents from Google Drive to also have access my email. But I still want to login to both services with the same email and password.  OAuth 2.0 solves this problem with  scope . A scope is a string that identifies access to some resource or action. For example, the scope to read, send and delete your email might be simply called \"gmail\". But another scope, \"gmail.readonly\", can only read email - it can't send or delete it. Likewise, the \"analytics\" scope may let me read analytic data for my websites, but it'll never see my email, much less send one.  Scope is different than a role because it belongs to the access token, not the user. A user can have multiple access tokens for different applications, each with different scope and therefore different access control.", 
            "title": "Granular Authorization with OAuth 2.0 Scopes"
        }, 
        {
            "location": "/auth/auth_scopes/#scope-format-and-hierarchy", 
            "text": "There is no definitive guide on what a scope string should look like, other than being restricted to alphanumeric characters and some symbols. Aqueduct, however, imposes a simple scoping structure.  Hierarchy is specified by the  :  character. For example, the following is a hierarchy of scopes related to a user and its belongings:   user  user:email  user:documents  user:documents:spreadsheets   Notice how these scopes form a hierarchy. Each segment makes the scope more restrictive. A scope that begins with the same segments has access to a scope with more segments, e.g.  user:documents  has access to  user:documents:spreadsheets , but  user:documents  cannot access  user:email . The  user  scope can access email, documents and anything else a user might have.  Scopes are validated by the method  Authorization.authorizedForScope() . Once a  Request  passes through an  Authorizer , it will have a valid  authorization  property. If the access token has scopes, this method can be used to ensure it has the appropriate scope for the resource or action. For example, the following will verify that a request has at least  user:email  access - either  user:email   or  the  user  scope.  @ httpGet  Future Response   getInbox ()   async   { \n   if   ( ! request . authorization . authorizedForScope ( user:email ))   { \n     return   new   Response . unauthorized (); \n   } \n\n   ...  }   It is often the case where a scope might have further restrictions - like readonly vs. write. You may introduce scopes like  user:email:read  and  user:email:write , but  user:email:write  would not have access to  user:email:read  following the previous logic.  This is where  scope modifiers  come in. A scope modifier is a  . -prefixed string at the end of a scope. For example,  user:email.readonly  grants readonly access to a user's email. An access token without a modifier has access to a scope with the same hierarchy and  any  modifier. Thus,  user  and  user:email  can both access  user:email.readonly  protected resources and actions, but  user:email.readonly  cannot access things protected by  user:email .  A scope modifier is only valid for the last segment of a scope string. That is,  user:documents.readonly:spreadsheets  is not valid, but  user:documents:spreadsheets.readonly  is.", 
            "title": "Scope Format and Hierarchy"
        }, 
        {
            "location": "/auth/auth_scopes/#requesting-scope", 
            "text": "Scope is requested by a client application when it is authenticating a user. For example, the form data to request the  user:email  scope on behalf of  bob@stablekernel.com  looks like this:  username=bob@stablekernel.com password=foobarxyz123 grant_type=password scope=user:email  Multiple scopes can be requested for an access token, which  must  be separated by spaces. (Note these query parameters must be percent-encoded, but are shown here without percent-encoding to aid visibility.)  username=bob@stablekernel.com password=foobarxyz123 grant_type=password scope=user:email user:documents  When using the authorization code flow, the requested scope is provided by the third party application in the query string of the initial  GET /auth/code :  GET /auth/code?grant_type=code client_id=com.foo.bar state=k3j4kjas scope=user:email  The webpage served by from this endpoint should alert the user to the scopes the application is requesting.", 
            "title": "Requesting Scope"
        }, 
        {
            "location": "/auth/auth_scopes/#adding-and-managing-scope", 
            "text": "An  AuthServer  validates that the scopes requested for an access token are valid for the authenticating client application. Therefore, each client identifier (a  ManagedClient ) may have a list of allowed scopes. The allowed scopes are configured with the  aqueduct auth command-line tool . For example, the following creates a new client identifier with access to the scopes  user:email  and  user:documents , and then later adds  user:location :  aqueduct auth add-client  \\ \n  --id com.app.mobile  \\ \n  --secret myspecialsecret  \\ \n  --allowed-scopes  user:email user:documents   \\ \n  --connect postgres://user:password@dbhost:5432/db_name\n\naqueduct auth set-scope  \\ \n  --id com.app.mobile  \\ \n  --scopes  user:email user:documents user:location   \\ \n  --connect postgres://user:password@dbhost:5432/db_name  Once a client has scopes, any access token request from this client  must  contain a list of desired scopes. Aqueduct does not implicitly grant scopes when a request omits them.  The  AuthServer  will only grant scopes that the client has access to. If some of the scopes in a request aren't valid for the client, the token may still be granted, but any disallowed scopes will be removed. For example, requesting the scopes  user:email  and  user:settings  would return an access token that only granted  user:email :  { \n   access_token :   ... , \n   refresh_token :   ... , \n   token_type :   bearer , \n   expires_in :   3600 , \n   scopes :   user:email  }   If none of the requested scopes are allowed, the access token will  not  be granted and the request will yield an error response.  If the client identifier has not been configured with scopes - either because the application doesn't use scopes or this particular client doesn't have any - scopes specified in an authenticating request are ignored. A token will be granted in this scenario, but will have no scope. The  scope  key is omitted from the token payload.  It is important to ensure that an application that uses scope has protections on its resources (see  a later section ).", 
            "title": "Adding and Managing Scope"
        }, 
        {
            "location": "/auth/auth_scopes/#user-based-scope-management", 
            "text": "Adding scopes to client identifiers is a requirement for any application that wishes to use scoping. An application may optionally add restrictions to scope depending on some attribute(s) of the user. When authenticating, the server first filters the list of requested scopes by what is allowed for the client, and then filters the resulting list by what is allowed for the user.  This user-level filtering is done by overriding  allowedScopesForAuthenticatable  in  AuthStorage . By default, this method returns  AuthScope.Any  - which means there are no restrictions. If the client application allows the scope, then any user that logs in with that application can request that scope.  This method may return a list of  AuthScope s that are valid for the authenticating user. The following example shows a  ManagedAuthStorage T  subclass that allows any scope for  @stablekernel.com  usernames, no scopes for  @hotmail.com  addresses and some limited scope for everyone else:  class   DomainBasedAuthStorage   extends   ManagedAuthStorage User   { \n   DomainBasedAuthStorage ( ManagedContext   context ,   { int   tokenLimit:   40 })   : \n         super ( context ,   tokenLimit:   tokenLimit ); \n\n   @ override \n   List AuthScope   allowedScopesForAuthenticatable ( covariant   User   user )   { \n     if   ( user . username . endsWith ( @stablekernel.com ))   { \n       return   AuthScope . Any ; \n     }   else   if   ( user . username . endsWith ( @hotmail.com ))   { \n       return   []; \n     }   else   { \n       return   [ new   AuthScope ( user )]; \n     } \n   }        }   The  user  passed to  allowedScopesForAuthenticatable  is the user being authenticated. It will have previously been fetched by the  AuthServer . The  AuthServer  fetches this object by invoking  AuthStorage.fetchAuthenticatableByUsername() . The default implementation of this method for  ManagedAuthStorage T  only fetches the  id ,  username ,  salt  and  hashedPassword  of the user. This is for two reasons:   These properties are needed to verify and grant an access token.  The  AuthServer  can only guarantee that the  User  implements  Authenticatable , and those are the only properties it has.   When using some other attribute of an application's user object to restrict allowed scopes, you must also override  fetchAuthenticatableByUsername  to fetch these attributes. For example, if your application's user has a  role  attribute, you must fetch it and the other four required properties. Here's an example implementation:  class   RoleBasedAuthStorage   extends   ManagedAuthStorage User   { \n   RoleBasedAuthStorage ( ManagedContext   context ,   { int   tokenLimit:   40 })   : \n         super ( context ,   tokenLimit:   tokenLimit ); \n\n   @ override \n   Future User   fetchAuthenticatableByUsername ( \n       AuthServer   server ,   String   username )   { \n     var   query   =   new   Query User ( context ) \n       .. where . username   =   username \n       .. returningProperties (( t )   = \n         [ t . id ,   t . username ,   t . hashedPassword ,   t . salt ,   t . role ]); \n\n     return   query . fetchOne (); \n   } \n\n   @ override \n   List AuthScope   allowedScopesForAuthenticatable ( covariant   User   user )   { \n     var   scopeStrings   =   []; \n     if   ( user . role   ==   admin )   { \n       scopeStrings   =   [ admin ,   user ]; \n     }   else   if   ( user . role   ==   user )   { \n       scopeStrings   =   [ user:email ]; \n     } \n\n     return   scopeStrings . map (( str )   =   new   AuthScope ( str )). toList (); \n   }  }   If you do not fetch the four required properties declared in  Authenticatable , an  AuthServer  will fail in spectacular ways.", 
            "title": "User-based Scope Management"
        }, 
        {
            "location": "/auth/auth_scopes/#using-scope-to-protect-resources", 
            "text": "An  Authorizer.bearer()  can require an access token to have certain scopes before passing it down the channel:  router \n   . route ( /email_attachments ) \n   . pipe ( new   Authorizer . bearer ( authServer ,   scopes:   [ user:email ,   user:documents ])) \n   . generate (()   =   new   SecureStuffController ());   A request's token must have all of the scopes declared by the  Authorizer  - in this case,  both  \"user:email\" and \"user:documents\" (or \"user\", of course).  This type of protection is often useful, but within a particular controller you may want finer control. For example, you may want to require a different level of access to  POST  than  GET . You may check if an authorization has valid scopes at any time:  class   EmailController   extend   HTTPController   { \n   @ httpGet \n   Future Response   getEmail ()   async   { \n     if   ( ! request . authorization . authorizedForScope ( user:email.readonly ))   { \n       return   new   Response . unauthorized (); \n     } \n\n     var   inbox   =   await   emailForUser ( request . authorization . resourceOwnerIdentifier ); \n     return   new   Response . ok ( inbox ); \n   } \n\n   @ httpPost \n   Future Response   sendEmail ( @ HTTPBody ()   Email   email )   async   { \n     if   ( ! request . authorization . authorizedForScope ( user:email ))   { \n       return   new   Response . unauthorized (); \n     } \n     await   sendEmail ( email ); \n\n     return   new   Response . accepted (); \n   }  }   Note that scopes are not the only way to secure resources, even if they are being used. For example, you may want to restrict the endpoint  /user/1/settings  to only allow the user with  id=1  to access it:  @ httpGet  Future Response   getUserSettings ( @ HTTPPath ( id )   int   id )   async   { \n   if   ( request . authorization . resourceOwnerIdentifier   !=   id )   { \n     return   new   Response . unauthorized (); \n   } \n\n   return   new   Response . ok ( await   settingsForUser ( id ));  }", 
            "title": "Using Scope to Protect Resources"
        }, 
        {
            "location": "/auth/what_is_oauth/", 
            "text": "What is OAuth 2.0?\n\n\nMost applications have the concept of a user. To prevent just anyone from saying they are this user, the user has a password. When a user wants to use your application, they send their username and password to the server so it can ensure they are who they say they are.\n\n\nThe simple way to do this is to send the username and password in an Authorization header for every request. Two reasons this is bad: first, every time the user wants to do something, their password is sent in every request and that can be unsafe. Second, any application that wants to keep the user logged in has to store that password - and that is unsafe.\n\n\nIn OAuth 2.0, the user gives their username and password to a client application once. The application sends those credentials to a server, and the server gives the application back an access token. A access token is a long, random string that no one can guess. When the user wants to do more stuff, the application sends the token with every request, instead of the user's password. The server checks the token, makes sure its not expired, and then lets the application's request go through. The application doesn't have to store the password and doesn't have to ask the user for their password again.\n\n\nThis credential-for-token exchange happens by sending a POST request to some endpoint, where the username and password are sent in the request body. Typically, an Aqueduct application's route for this is \n/auth/token\n and handled by an instance of \nAuthController\n.\n\n\nOAuth 2.0 makes a subtle distinction: a user and the application they are using are not the same thing. It's intuitive to think of a user as \"making a request to the server\", but in reality, the user makes a request to an application and \nthe application\n makes the request to the server. The server \ngrants\n the application access on behalf of the user. In other words, when a user enters their credentials into an application, the application goes to the server and says \"Hey, this user said I can do stuff for them. Look, this is their secret password!\"\n\n\nThis is an important distinction, because an OAuth 2.0 server doesn't just verify users: it also verifies applications. An application has a identifier that has been registered with the server. So, for an ecosystem that has a web client, an Android and an iOS app there would likely be three identifiers - one for each. The client application usually stores that identifier in a database. This identifier is called a \nclient identifier\n. Client identifiers are added to Aqueduct applications with the \naqueduct auth\n tool (see \nAqueduct Auth CLI\n).\n\n\nWhen the user is logging in through an application, they submit their username and password. The user doesn't provide the client identifier - in fact, the user doesn't know it. The application sends a request with the user's username and password in the request body, and the client identifier in the Authorization header. All three have to check out for the server to give the application back a token. The full request in pseudo-code looks something like:\n\n\nvar\n \nrequest\n \n=\n \nnew\n \nHTTPRequest\n(\n/auth/token\n);\n\n\nrequest\n.\nmethod\n \n=\n \nPOST\n;\n\n\nrequest\n.\ncontentType\n \n=\n \napplication/x-www-form-urlencoded\n;\n\n\nrequest\n.\nauthorization\n \n=\n \nBase64\n.\nencode\n(\n$\nclientID\n:\n);\n\n\nrequest\n.\nbody\n \n=\n \n{\n\n  \nusername\n \n:\n \nbob@stablekernel.com\n,\n\n  \npassword\n \n:\n \nsupersecretstuff\n,\n\n  \ngrant_type\n \n:\n \npassword\n\n\n};\n\n\n\n\n\n\nAn access token can expire. How long it takes to expire is up to the server - Aqueduct defaults to 24 hours. At first glance, this means that the application would have to ask the user for a password again. But, tokens can also be refreshed. Refreshing a token grants a brand new access token, but without having to ask for the password. This is possible because an access token comes with a \nrefresh token\n. The refresh token is another long, random string. So, the JSON the server sends back when granting a token looks like this:\n\n\n{\n\n  \naccess_token\n \n:\n \nAbca09zzzza2o2kelmzlli3ijlka\n,\n\n  \ntoken_type\n \n:\n \nbearer\n,\n\n  \nrefresh_token\n \n:\n \nlkmLIAmooa898nm20jannnnnxaww\n,\n\n  \nexpire_in\n \n:\n \n3600\n\n\n}\n\n\n\n\n\n\nThe application hangs on to both an access token and a refresh token. When the token expires, it will send the refresh token back to the server to get a replacement access token. This is done through the same route that the access token came from - \n/auth/token\n - except the parameters are a bit different:\n\n\nvar\n \nrequest\n \n=\n \nnew\n \nHTTPRequest\n(\n/auth/token\n);\n\n\nrequest\n.\nmethod\n \n=\n \nPOST\n;\n\n\nrequest\n.\ncontentType\n \n=\n \napplication/x-www-form-urlencoded\n;\n\n\nrequest\n.\nauthorization\n \n=\n \nBase64\n.\nencode\n(\n$\nclientID\n:\n);\n\n\nrequest\n.\nbody\n \n=\n \n{\n\n  \nrefresh_token\n \n:\n \nlkmLIAmooa898nm20jannnnnxaww\n,\n\n  \ngrant_type\n \n:\n \nrefresh_token\n\n\n};\n\n\n\n\n\n\nExchanging a refresh token has the same response as the initial exchange for username and password - except a few values will have changed.\n\n\nThe verification and storage of authorization and authentication information is managed by an \nAuthServer\n.\n\n\nOther Methods for Obtaining Authorization\n\n\nThe method of getting a token above - sending a username and password to \n/auth/token\n - is just one of four possible methods OAuth 2.0 uses to authenticate a user. This particular one is called the \nresource owner password credentials grant\n. A resource owner is a fancy word for a 'user'. We can shorten it up to just the 'password flow'. It's probably the most common flow - mobiles applications and front-end web applications often use this flow. When you enter your credentials, the client application sends them directly to the server.\n\n\nThe other commonly used flow prevents the client application from ever seeing the user's credentials. For example, you might sign into Pivotal Tracker with your Google account. Your account on Pivotal Tracker doesn't have a password. Instead, it is linked to your Google account - which does. Pivotal Tracker never sees your Google password. When you login to Pivotal Tracker in this way, it takes you to Google's authentication page - owned and operated by Google. When you login successfully, Google gives Pivotal Tracker your token. Pivotal Tracker is now an application that can do things on your behalf.\n\n\nThis is called the \nauthorization code grant\n - or just 'auth code flow'. An instance of \nAuthCodeController\n handles granting authorization codes. Once a code is received, it can be exchanged for a token via an \nAuthController\n.", 
            "title": "What is OAuth 2.0?"
        }, 
        {
            "location": "/auth/what_is_oauth/#what-is-oauth-20", 
            "text": "Most applications have the concept of a user. To prevent just anyone from saying they are this user, the user has a password. When a user wants to use your application, they send their username and password to the server so it can ensure they are who they say they are.  The simple way to do this is to send the username and password in an Authorization header for every request. Two reasons this is bad: first, every time the user wants to do something, their password is sent in every request and that can be unsafe. Second, any application that wants to keep the user logged in has to store that password - and that is unsafe.  In OAuth 2.0, the user gives their username and password to a client application once. The application sends those credentials to a server, and the server gives the application back an access token. A access token is a long, random string that no one can guess. When the user wants to do more stuff, the application sends the token with every request, instead of the user's password. The server checks the token, makes sure its not expired, and then lets the application's request go through. The application doesn't have to store the password and doesn't have to ask the user for their password again.  This credential-for-token exchange happens by sending a POST request to some endpoint, where the username and password are sent in the request body. Typically, an Aqueduct application's route for this is  /auth/token  and handled by an instance of  AuthController .  OAuth 2.0 makes a subtle distinction: a user and the application they are using are not the same thing. It's intuitive to think of a user as \"making a request to the server\", but in reality, the user makes a request to an application and  the application  makes the request to the server. The server  grants  the application access on behalf of the user. In other words, when a user enters their credentials into an application, the application goes to the server and says \"Hey, this user said I can do stuff for them. Look, this is their secret password!\"  This is an important distinction, because an OAuth 2.0 server doesn't just verify users: it also verifies applications. An application has a identifier that has been registered with the server. So, for an ecosystem that has a web client, an Android and an iOS app there would likely be three identifiers - one for each. The client application usually stores that identifier in a database. This identifier is called a  client identifier . Client identifiers are added to Aqueduct applications with the  aqueduct auth  tool (see  Aqueduct Auth CLI ).  When the user is logging in through an application, they submit their username and password. The user doesn't provide the client identifier - in fact, the user doesn't know it. The application sends a request with the user's username and password in the request body, and the client identifier in the Authorization header. All three have to check out for the server to give the application back a token. The full request in pseudo-code looks something like:  var   request   =   new   HTTPRequest ( /auth/token );  request . method   =   POST ;  request . contentType   =   application/x-www-form-urlencoded ;  request . authorization   =   Base64 . encode ( $ clientID : );  request . body   =   { \n   username   :   bob@stablekernel.com , \n   password   :   supersecretstuff , \n   grant_type   :   password  };   An access token can expire. How long it takes to expire is up to the server - Aqueduct defaults to 24 hours. At first glance, this means that the application would have to ask the user for a password again. But, tokens can also be refreshed. Refreshing a token grants a brand new access token, but without having to ask for the password. This is possible because an access token comes with a  refresh token . The refresh token is another long, random string. So, the JSON the server sends back when granting a token looks like this:  { \n   access_token   :   Abca09zzzza2o2kelmzlli3ijlka , \n   token_type   :   bearer , \n   refresh_token   :   lkmLIAmooa898nm20jannnnnxaww , \n   expire_in   :   3600  }   The application hangs on to both an access token and a refresh token. When the token expires, it will send the refresh token back to the server to get a replacement access token. This is done through the same route that the access token came from -  /auth/token  - except the parameters are a bit different:  var   request   =   new   HTTPRequest ( /auth/token );  request . method   =   POST ;  request . contentType   =   application/x-www-form-urlencoded ;  request . authorization   =   Base64 . encode ( $ clientID : );  request . body   =   { \n   refresh_token   :   lkmLIAmooa898nm20jannnnnxaww , \n   grant_type   :   refresh_token  };   Exchanging a refresh token has the same response as the initial exchange for username and password - except a few values will have changed.  The verification and storage of authorization and authentication information is managed by an  AuthServer .", 
            "title": "What is OAuth 2.0?"
        }, 
        {
            "location": "/auth/what_is_oauth/#other-methods-for-obtaining-authorization", 
            "text": "The method of getting a token above - sending a username and password to  /auth/token  - is just one of four possible methods OAuth 2.0 uses to authenticate a user. This particular one is called the  resource owner password credentials grant . A resource owner is a fancy word for a 'user'. We can shorten it up to just the 'password flow'. It's probably the most common flow - mobiles applications and front-end web applications often use this flow. When you enter your credentials, the client application sends them directly to the server.  The other commonly used flow prevents the client application from ever seeing the user's credentials. For example, you might sign into Pivotal Tracker with your Google account. Your account on Pivotal Tracker doesn't have a password. Instead, it is linked to your Google account - which does. Pivotal Tracker never sees your Google password. When you login to Pivotal Tracker in this way, it takes you to Google's authentication page - owned and operated by Google. When you login successfully, Google gives Pivotal Tracker your token. Pivotal Tracker is now an application that can do things on your behalf.  This is called the  authorization code grant  - or just 'auth code flow'. An instance of  AuthCodeController  handles granting authorization codes. Once a code is received, it can be exchanged for a token via an  AuthController .", 
            "title": "Other Methods for Obtaining Authorization"
        }, 
        {
            "location": "/testing/overview/", 
            "text": "Tasks\n\n\nAqueduct applications can be run, tested, debugged and profiled.\n\n\nGuides\n\n\n\n\nBest Practices for Aqueduct Development\n\n\nUsing a Local Database\n\n\nDeveloping Client Applications\n\n\nUsing the Debugger and Profiling\n\n\nWriting Tests\n\n\nUse Mock Services", 
            "title": "Overview"
        }, 
        {
            "location": "/testing/overview/#tasks", 
            "text": "Aqueduct applications can be run, tested, debugged and profiled.", 
            "title": "Tasks"
        }, 
        {
            "location": "/testing/overview/#guides", 
            "text": "Best Practices for Aqueduct Development  Using a Local Database  Developing Client Applications  Using the Debugger and Profiling  Writing Tests  Use Mock Services", 
            "title": "Guides"
        }, 
        {
            "location": "/testing/database/", 
            "text": "Using a Local Database\n\n\nAqueduct's ORM uses PostgreSQL as its database. To run the application or its automated tests locally, you must have PostgreSQL installed locally. On macOS, \nPostgres.app\n is a simple, self-contained PostgreSQL instance that you can run as a normal application. (See \nPostgreSQL installation for other platforms\n.)\n\n\nLocal Database for Tests\n\n\nAn application running automated tests defaults to connecting to a database with the following configuration:\n\n\nusername\n:\n \ndart\n\n\npassword\n:\n \ndart\n\n\nhost\n:\n \nlocalhost\n\n\nport\n:\n \n5432\n\n\ndatabaseName\n:\n \ndart_test\n\n\n\n\n\n\nOnce PostgreSQL has been installed locally, you may create a database user and database that matches this connection info by running the following command:\n\n\naqueduct setup\n\n\n\n\n\nAqueduct tests create a temporary database schema that matches your application schema in the \ndart_test\n database. The tables and data in this database are discarded when the tests complete. For this reason, no other tables should be created in this database to avoid conflicts with tests. This default behavior of Aqueduct tests is provided by a \ntest harness\n.\n\n\nLocal Database for Running an Application\n\n\nA database separate from the test database should be used for \nrunning\n an application locally. You can create a database locally by running \npsql\n to open a PostgreSQL terminal and run the following commands:\n\n\nCREATE DATABASE my_local_app_db;\nCREATE USER my_local_app_user WITH PASSWORD \nmypassword\n;\nGRANT ALL ON DATABASE my_local_app_db TO my_local_app_user;\n\n\n\n\n\nAdd your schema to the local database by generating and executing migration scripts:\n\n\naqueduct db generate\naqueduct db upgrade --connect postgres://my_local_app_user:mypassword@localhost:5432/my_local_app_db\n\n\n\n\n\nUse Local Configuration Files\n\n\nUse \nconfiguration files\n to manage which database an application connects to. This may or may not be checked into source control, depending on a team's preference. Control which file is loaded with command-line options to \naqueduct serve\n or the \nbin/main.dart\n script:\n\n\naqueduct serve -c local.yaml\n\n\n\n\n\nHave Scripts to Provision Based on Scenarios\n\n\nIt is often the case that you will want to have a certain set of data in an local database for the purpose of testing a client application. Create \nbin\n scripts to provision the database and add the desired data. For example, you might have a script named \nbin/ios_integration.dart\n that re-provisions a database and inserts data into it using \nQuery\nT\n instances and the \nManagedObject\nT\ns declared in your application.\n\n\nimport\n \ndart:io\n;\n\n\nimport\n \npackage:myapp/myapp.dart\n;\n\n\n\nFuture\n \nmain\n()\n \nasync\n \n{\n\n  \nawait\n \nprovisionDatabase\n();\n\n\n  \nvar\n \ndefaultUser\n \n=\n \nnew\n \nUser\n(...);\n\n  \nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUser\n()..\nvalues\n \n=\n \ndefaultUser\n;\n\n  \nawait\n \nquery\n.\ninsert\n();\n\n\n  \n...\n\n\n}\n\n\n\nFuture\n \nprovisionDatabase\n()\n \nasync\n \n{\n\n  \nvar\n \ncommands\n \n=\n \n[\n\n    \nCREATE DATABASE local_app;\n,\n\n    \nCREATE USER local_user WITH PASSWORD \nlocal\n;\n,\n\n    \nGRANT ALL ON DATABASE local_app TO local_user;\n\n  \n];\n\n\n  \nawait\n \nFuture\n.\nforEach\n(\ncommands\n,\n \n(\ncmd\n)\n \n{\n\n    \nList\nString\n \nargs\n \n=\n \n[\n-c\n,\n \ncmd\n,\n \n-U\n,\n \ngrantingUser\n];\n\n    \nreturn\n \nProcess\n.\nrun\n(\npsql\n,\n \nargs\n,\n \nrunInShell:\n \ntrue\n);\n\n  \n});\n\n\n}", 
            "title": "Using a Local Database"
        }, 
        {
            "location": "/testing/database/#using-a-local-database", 
            "text": "Aqueduct's ORM uses PostgreSQL as its database. To run the application or its automated tests locally, you must have PostgreSQL installed locally. On macOS,  Postgres.app  is a simple, self-contained PostgreSQL instance that you can run as a normal application. (See  PostgreSQL installation for other platforms .)", 
            "title": "Using a Local Database"
        }, 
        {
            "location": "/testing/database/#local-database-for-tests", 
            "text": "An application running automated tests defaults to connecting to a database with the following configuration:  username :   dart  password :   dart  host :   localhost  port :   5432  databaseName :   dart_test   Once PostgreSQL has been installed locally, you may create a database user and database that matches this connection info by running the following command:  aqueduct setup  Aqueduct tests create a temporary database schema that matches your application schema in the  dart_test  database. The tables and data in this database are discarded when the tests complete. For this reason, no other tables should be created in this database to avoid conflicts with tests. This default behavior of Aqueduct tests is provided by a  test harness .", 
            "title": "Local Database for Tests"
        }, 
        {
            "location": "/testing/database/#local-database-for-running-an-application", 
            "text": "A database separate from the test database should be used for  running  an application locally. You can create a database locally by running  psql  to open a PostgreSQL terminal and run the following commands:  CREATE DATABASE my_local_app_db;\nCREATE USER my_local_app_user WITH PASSWORD  mypassword ;\nGRANT ALL ON DATABASE my_local_app_db TO my_local_app_user;  Add your schema to the local database by generating and executing migration scripts:  aqueduct db generate\naqueduct db upgrade --connect postgres://my_local_app_user:mypassword@localhost:5432/my_local_app_db", 
            "title": "Local Database for Running an Application"
        }, 
        {
            "location": "/testing/database/#use-local-configuration-files", 
            "text": "Use  configuration files  to manage which database an application connects to. This may or may not be checked into source control, depending on a team's preference. Control which file is loaded with command-line options to  aqueduct serve  or the  bin/main.dart  script:  aqueduct serve -c local.yaml", 
            "title": "Use Local Configuration Files"
        }, 
        {
            "location": "/testing/database/#have-scripts-to-provision-based-on-scenarios", 
            "text": "It is often the case that you will want to have a certain set of data in an local database for the purpose of testing a client application. Create  bin  scripts to provision the database and add the desired data. For example, you might have a script named  bin/ios_integration.dart  that re-provisions a database and inserts data into it using  Query T  instances and the  ManagedObject T s declared in your application.  import   dart:io ;  import   package:myapp/myapp.dart ;  Future   main ()   async   { \n   await   provisionDatabase (); \n\n   var   defaultUser   =   new   User (...); \n   var   query   =   new   Query User ().. values   =   defaultUser ; \n   await   query . insert (); \n\n   ...  }  Future   provisionDatabase ()   async   { \n   var   commands   =   [ \n     CREATE DATABASE local_app; , \n     CREATE USER local_user WITH PASSWORD  local ; , \n     GRANT ALL ON DATABASE local_app TO local_user; \n   ]; \n\n   await   Future . forEach ( commands ,   ( cmd )   { \n     List String   args   =   [ -c ,   cmd ,   -U ,   grantingUser ]; \n     return   Process . run ( psql ,   args ,   runInShell:   true ); \n   });  }", 
            "title": "Have Scripts to Provision Based on Scenarios"
        }, 
        {
            "location": "/testing/debugger/", 
            "text": "Using the IntelliJ IDE Debugger\n\n\nThe debugger may be used when running tests or developing client applications locally.\n\n\nEnabling the Debugger\n\n\nApplications created by \naqueduct create\n ship with a \nbin/main.dart\n script that starts the application. When developing, running this script from an IDE is often preferred to \naqueduct serve\n because the IDE can leverage its full capabilities. One such capability is a debugger. In IntelliJ IDEA (and its suite of products), you may right-click on this file and select 'Debug' from the pop-up menu to run the application in the debugger.\n\n\n\n\nSetting Breakpoints\n\n\nA valuable feature of a debugger is the ability to set breakpoints. A breakpoint pauses execution of your application at a specific point in the code, allowing you to view variable values and monitor the code path line-by-line as execution continues.\n\n\nTo set a breakpoint, you simply click on the gutter area next to the line of code you wish to stop at.\n\n\n\n\nOnce a debugger stops, you can view variable values in the center pane of the debugging panel. The left pane (labeled 'Frames') shows the current stack trace. The row of buttons above the stack trace allow you to continue executing code line-by-line.\n\n\nEach button in this row has a slightly different behavior. From left to right:\n\n\n\n\nThe red arrow with the stack of lines continues execution until the next breakpoint is encountered.\n\n\nThe blue downwards arrow executes the current line and moves to the next line.\n\n\nThe blue right/downward arrow continues execution into the function that is about to be called and stops on its first line.\n\n\nThe red right/downward arrow is the same as above, but will also jump into dependency code.\n\n\nThe blue right/upwards arrow completes execution of the current method and stops right after the callsite.\n\n\n\n\nNote that currently asynchronous invocations confuse the debugger a bit. To continue past an asynchronous method to the next line, set a breakpoint on that next line and hit the big green arrow on the left column of buttons.\n\n\nTo jump into an asynchronous method, command-click on its definition and set a breakpoint on its first line and then click the big green arrow.\n\n\nProfiling with Observatory\n\n\nYou may also use \nObservatory\n to profile applications. Profiling consists of monitoring memory usage, allocations and how much time a function spends executing relative to the rest of your application.\n\n\nBoth \naqueduct serve\n and \nbin/main.dart\n support starting Observatory. When running the application with \naqueduct serve\n, add the \n--observe\n flag and Observatory will start listening on port 8181 and a web browser will automatically be opened.\n\n\naqueduct serve --observe\n\n\n\n\n\nWhen running the application through IntelliJ, Observatory will start listening on a random port. In the run console in IntelliJ, you may click on the Observatory hyperlink to open it in your web browser.", 
            "title": "Using the Debugger"
        }, 
        {
            "location": "/testing/debugger/#using-the-intellij-ide-debugger", 
            "text": "The debugger may be used when running tests or developing client applications locally.", 
            "title": "Using the IntelliJ IDE Debugger"
        }, 
        {
            "location": "/testing/debugger/#enabling-the-debugger", 
            "text": "Applications created by  aqueduct create  ship with a  bin/main.dart  script that starts the application. When developing, running this script from an IDE is often preferred to  aqueduct serve  because the IDE can leverage its full capabilities. One such capability is a debugger. In IntelliJ IDEA (and its suite of products), you may right-click on this file and select 'Debug' from the pop-up menu to run the application in the debugger.", 
            "title": "Enabling the Debugger"
        }, 
        {
            "location": "/testing/debugger/#setting-breakpoints", 
            "text": "A valuable feature of a debugger is the ability to set breakpoints. A breakpoint pauses execution of your application at a specific point in the code, allowing you to view variable values and monitor the code path line-by-line as execution continues.  To set a breakpoint, you simply click on the gutter area next to the line of code you wish to stop at.   Once a debugger stops, you can view variable values in the center pane of the debugging panel. The left pane (labeled 'Frames') shows the current stack trace. The row of buttons above the stack trace allow you to continue executing code line-by-line.  Each button in this row has a slightly different behavior. From left to right:   The red arrow with the stack of lines continues execution until the next breakpoint is encountered.  The blue downwards arrow executes the current line and moves to the next line.  The blue right/downward arrow continues execution into the function that is about to be called and stops on its first line.  The red right/downward arrow is the same as above, but will also jump into dependency code.  The blue right/upwards arrow completes execution of the current method and stops right after the callsite.   Note that currently asynchronous invocations confuse the debugger a bit. To continue past an asynchronous method to the next line, set a breakpoint on that next line and hit the big green arrow on the left column of buttons.  To jump into an asynchronous method, command-click on its definition and set a breakpoint on its first line and then click the big green arrow.", 
            "title": "Setting Breakpoints"
        }, 
        {
            "location": "/testing/debugger/#profiling-with-observatory", 
            "text": "You may also use  Observatory  to profile applications. Profiling consists of monitoring memory usage, allocations and how much time a function spends executing relative to the rest of your application.  Both  aqueduct serve  and  bin/main.dart  support starting Observatory. When running the application with  aqueduct serve , add the  --observe  flag and Observatory will start listening on port 8181 and a web browser will automatically be opened.  aqueduct serve --observe  When running the application through IntelliJ, Observatory will start listening on a random port. In the run console in IntelliJ, you may click on the Observatory hyperlink to open it in your web browser.", 
            "title": "Profiling with Observatory"
        }, 
        {
            "location": "/testing/best_practices/", 
            "text": "Best Practices for Developing Aqueduct Applications\n\n\nKeep Projects Separate\n\n\nBecause Dart is cross-platform, developers may use the same project for both an Aqueduct application and a client application. Avoid this. When projects share dependencies, it may force one of them to use older versions of libraries. Two projects in one also creates a more confusing codebase.\n\n\nUse Test Driven Development (or something close to it)\n\n\nIn Aqueduct, testing is a first-class citizen. The preferred method of development is to write tests using \nTestClient\n - as opposed to using a client application to 'eyeball' test an endpoint in development.\n\n\nDevelopers should at minimum write tests for the 'success case' of an endpoint and should use automated testing to verify that code written in the future does not impact code written in the past. Aqueduct has behavior and tooling for testing applications that interface with a database or other external services.\n\n\nUse a bin Script to Verify Assumptions\n\n\nKeep a simple Dart script file in the \nbin/\n directory that imports your project. Use this file as a scratchpad to test exploratory code before committing to a test suite. Don't check this file into source control.\n\n\nimport\n \npackage:myapp/myapp.dart\n;\n\n\n\nFuture\n \nmain\n()\n \nasync\n \n{\n\n  \nvar\n \nwhatIsThis\n \n=\n \nawait\n \nsomeYetToBeNamedUsefullyMethod\n();\n\n  \nprint\n(\n$\nwhatIsThis\n);\n\n\n}\n\n\n\n\n\n\nCreate New Projects from a Template\n\n\nUse \naqueduct create\n to create applications with the appropriate structure and boilerplate. There are templates for different kinds of applications; view these templates with \naqueduct create list-templates\n.\n\n\nUse a Debugger\n\n\nUse a debugger while running tests to stop execution, view variable values and verify program flow. Right-click on this file and select 'Debug' to run the tests in the debugger. Use breakpoints (by clicking on the gutter area to the left of the text editing area) to stop execution, view variable values and verify program flow.\n\n\nEach project also has a \nbin/main.dart\n script to run the application without \naqueduct serve\n. You may also right-click on this file and select 'Debug' to run the application with the debugger turned on. Use a client application, CURL or tools like Paw and Postman to issue requests while the debugger is running. Use \naqueduct document\n to generate an OpenAPI specification that can be imported into HTTP client applications like Postman.\n\n\nUse the Suggested Project Directory Structure\n\n\nSee \nAqueduct Project Structure\n.\n\n\nPass Services to Controllers in setupRouter\n\n\nPass service objects to a controller in \nsetupRouter\n and only pass the services the controller will use.\n\n\n@\noverride\n\n\nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n  \nrouter\n.\nroute\n(\n/data\n).\ngenerate\n(()\n \n=\n \nnew\n \nDBController\n(\ndatabaseConnection\n));\n\n  \nrouter\n.\nroute\n(\n/github\n).\ngenerate\n(()\n \n=\n \nnew\n \nGitHubController\n(\ngithubService\n));\n\n\n}\n\n\n\n\n\n\nThese objects are called dependencies. Passing references like this allows for injecting dependencies that depend on the environment; e.g. in production, development or during tests. It also avoids tight coupling between the objects in your application.\n\n\nMinimize the access a controller has to its dependencies; e.g. don't pass it a \nStreamController\n when it only needs \nSink\n or a \nStream\n.\n\n\nUse a Test Harness\n\n\nThe test harness available in projects generated by the template makes testing considerably easier. It will require less typing and cleaner, safer test code.\n\n\nUse config.src.yaml\n\n\nUse the convention of \nconfig.src.yaml file\n to prevent configuration errors and inject test dependencies.\n\n\nUnderstand how Aqueduct Uses Isolates\n\n\nSee more in \nApplication Structure\n.\n\n\nUse HTTPController Subclasses\n\n\nSubclassing \nHTTPController\n provides significant conveniences, safeties and behaviors used by the majority of an application's request handling logic. Prefer to use this class for non-middleware controllers.\n\n\nKeep RequestSink Tidy\n\n\nA \nRequestSink\n should handle initialization, routing and nothing more. Consider moving non-initialization behavior into a service object in a separate file.\n\n\nAvoid Raw SQL Queries\n\n\nPrefer to use the Aqueduct ORM. It sends appropriate HTTP responses for different kinds of errors, validates input data and is ensures the queries match up with your data model.\n\n\nUse API Reference\n\n\nAqueduct is an object oriented framework - behaviors are implemented by instances of some type. The types of objects, their properties and their behaviors all follow similar naming conventions to make the API more discoverable.\n\n\nMost types in Aqueduct have a prefix in common with related types. For example, types like \nAuthServer\n, \nAuthStorage\n and \nAuthCode\n are all related because they deal with authentication and authorization. Methods are named consistently across classes (e.g, \nasMap\n is a common method name).\n\n\nWhen looking for a solution, look at the API reference for the objects you have access to. These objects may already have the behavior you wish to implement or have a reference to an object with that behavior.\n\n\nUse try-catch Sparingly\n\n\nAll request handling code is wrapped in a try-catch block that will interpret exceptions and errors and return meaningful HTTP responses. Unknown exceptions will yield a 500 Server Error response. In general, you do not need to use try-catch unless you want a different HTTP response than the one being returned for the exception.\n\n\nCode that throws an exception during initialization should not be caught if the error is fatal to the application launching successfully.", 
            "title": "Development Best Practices"
        }, 
        {
            "location": "/testing/best_practices/#best-practices-for-developing-aqueduct-applications", 
            "text": "", 
            "title": "Best Practices for Developing Aqueduct Applications"
        }, 
        {
            "location": "/testing/best_practices/#keep-projects-separate", 
            "text": "Because Dart is cross-platform, developers may use the same project for both an Aqueduct application and a client application. Avoid this. When projects share dependencies, it may force one of them to use older versions of libraries. Two projects in one also creates a more confusing codebase.", 
            "title": "Keep Projects Separate"
        }, 
        {
            "location": "/testing/best_practices/#use-test-driven-development-or-something-close-to-it", 
            "text": "In Aqueduct, testing is a first-class citizen. The preferred method of development is to write tests using  TestClient  - as opposed to using a client application to 'eyeball' test an endpoint in development.  Developers should at minimum write tests for the 'success case' of an endpoint and should use automated testing to verify that code written in the future does not impact code written in the past. Aqueduct has behavior and tooling for testing applications that interface with a database or other external services.", 
            "title": "Use Test Driven Development (or something close to it)"
        }, 
        {
            "location": "/testing/best_practices/#use-a-bin-script-to-verify-assumptions", 
            "text": "Keep a simple Dart script file in the  bin/  directory that imports your project. Use this file as a scratchpad to test exploratory code before committing to a test suite. Don't check this file into source control.  import   package:myapp/myapp.dart ;  Future   main ()   async   { \n   var   whatIsThis   =   await   someYetToBeNamedUsefullyMethod (); \n   print ( $ whatIsThis );  }", 
            "title": "Use a bin Script to Verify Assumptions"
        }, 
        {
            "location": "/testing/best_practices/#create-new-projects-from-a-template", 
            "text": "Use  aqueduct create  to create applications with the appropriate structure and boilerplate. There are templates for different kinds of applications; view these templates with  aqueduct create list-templates .", 
            "title": "Create New Projects from a Template"
        }, 
        {
            "location": "/testing/best_practices/#use-a-debugger", 
            "text": "Use a debugger while running tests to stop execution, view variable values and verify program flow. Right-click on this file and select 'Debug' to run the tests in the debugger. Use breakpoints (by clicking on the gutter area to the left of the text editing area) to stop execution, view variable values and verify program flow.  Each project also has a  bin/main.dart  script to run the application without  aqueduct serve . You may also right-click on this file and select 'Debug' to run the application with the debugger turned on. Use a client application, CURL or tools like Paw and Postman to issue requests while the debugger is running. Use  aqueduct document  to generate an OpenAPI specification that can be imported into HTTP client applications like Postman.", 
            "title": "Use a Debugger"
        }, 
        {
            "location": "/testing/best_practices/#use-the-suggested-project-directory-structure", 
            "text": "See  Aqueduct Project Structure .", 
            "title": "Use the Suggested Project Directory Structure"
        }, 
        {
            "location": "/testing/best_practices/#pass-services-to-controllers-in-setuprouter", 
            "text": "Pass service objects to a controller in  setupRouter  and only pass the services the controller will use.  @ override  void   setupRouter ( Router   router )   { \n   router . route ( /data ). generate (()   =   new   DBController ( databaseConnection )); \n   router . route ( /github ). generate (()   =   new   GitHubController ( githubService ));  }   These objects are called dependencies. Passing references like this allows for injecting dependencies that depend on the environment; e.g. in production, development or during tests. It also avoids tight coupling between the objects in your application.  Minimize the access a controller has to its dependencies; e.g. don't pass it a  StreamController  when it only needs  Sink  or a  Stream .", 
            "title": "Pass Services to Controllers in setupRouter"
        }, 
        {
            "location": "/testing/best_practices/#use-a-test-harness", 
            "text": "The test harness available in projects generated by the template makes testing considerably easier. It will require less typing and cleaner, safer test code.", 
            "title": "Use a Test Harness"
        }, 
        {
            "location": "/testing/best_practices/#use-configsrcyaml", 
            "text": "Use the convention of  config.src.yaml file  to prevent configuration errors and inject test dependencies.", 
            "title": "Use config.src.yaml"
        }, 
        {
            "location": "/testing/best_practices/#understand-how-aqueduct-uses-isolates", 
            "text": "See more in  Application Structure .", 
            "title": "Understand how Aqueduct Uses Isolates"
        }, 
        {
            "location": "/testing/best_practices/#use-httpcontroller-subclasses", 
            "text": "Subclassing  HTTPController  provides significant conveniences, safeties and behaviors used by the majority of an application's request handling logic. Prefer to use this class for non-middleware controllers.", 
            "title": "Use HTTPController Subclasses"
        }, 
        {
            "location": "/testing/best_practices/#keep-requestsink-tidy", 
            "text": "A  RequestSink  should handle initialization, routing and nothing more. Consider moving non-initialization behavior into a service object in a separate file.", 
            "title": "Keep RequestSink Tidy"
        }, 
        {
            "location": "/testing/best_practices/#avoid-raw-sql-queries", 
            "text": "Prefer to use the Aqueduct ORM. It sends appropriate HTTP responses for different kinds of errors, validates input data and is ensures the queries match up with your data model.", 
            "title": "Avoid Raw SQL Queries"
        }, 
        {
            "location": "/testing/best_practices/#use-api-reference", 
            "text": "Aqueduct is an object oriented framework - behaviors are implemented by instances of some type. The types of objects, their properties and their behaviors all follow similar naming conventions to make the API more discoverable.  Most types in Aqueduct have a prefix in common with related types. For example, types like  AuthServer ,  AuthStorage  and  AuthCode  are all related because they deal with authentication and authorization. Methods are named consistently across classes (e.g,  asMap  is a common method name).  When looking for a solution, look at the API reference for the objects you have access to. These objects may already have the behavior you wish to implement or have a reference to an object with that behavior.", 
            "title": "Use API Reference"
        }, 
        {
            "location": "/testing/best_practices/#use-try-catch-sparingly", 
            "text": "All request handling code is wrapped in a try-catch block that will interpret exceptions and errors and return meaningful HTTP responses. Unknown exceptions will yield a 500 Server Error response. In general, you do not need to use try-catch unless you want a different HTTP response than the one being returned for the exception.  Code that throws an exception during initialization should not be caught if the error is fatal to the application launching successfully.", 
            "title": "Use try-catch Sparingly"
        }, 
        {
            "location": "/testing/clients/", 
            "text": "Using Aqueduct when Writing Client Applications\n\n\nRunning an Aqueduct server locally while developing client applications is an important part of the development process. Run applications through their \nbin/main.dart\n script or \naqueduct serve\n. The former allows for \ndebugging\n the application with a debugger.\n\n\nEnable Logging and Return Server Errors\n\n\nEnsure that logging is on while developing client applications by registering a listener on \nRequestSink.logger\n.\n\n\nclass\n \nMyRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nMyRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n    \nlogger\n.\nonRecord\n.\nlisten\n((\nrecord\n)\n \n{\n\n      \nprint\n(\n$\nrecord\n \n${\nrecord\n.\nerror\n \n??\n \n}\n \n${\nrecord\n.\nstackTrace\n \n??\n \n}\n);\n\n    \n});\n\n  \n}\n\n  \n...\n\n\n}\n\n\n\n\n\n\nA useful feature to turn on during debugging is sending stack traces for 500 Server Error responses. Turn this flag on in a \nRequestSink\n while debugging:\n\n\nclass\n \nMyRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nMyRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n    \nRequestController\n.\nincludeErrorDetailsInServerErrorResponses\n \n=\n \ntrue\n;\n\n  \n}\n\n  \n...\n\n\n}\n\n\n\n\n\n\nWhen a 500 error is encountered, the server will send the stack trace back to the client so you can view it while developing the client application without having to switch terminals. This property should never be left on for production code.\n\n\nAvoid Port Conflicts\n\n\nAqueduct applications run through the \nbin/main.dart\n script default to port 8000. Applications run with \naqueduct serve\n default to port 8081. You may use the \n--port\n command-line option to pick a different port:\n\n\naqueduct serve --port 4000\n\n\n\n\n\nProvision a Database for Client Testing\n\n\nFor applications that use the ORM, you must have a locally running database with your application's data model. See \nprovisioning a database\n for more details.\n\n\nIf you are using OAuth 2.0, you must have also added client identifiers to the locally running database. You may add client identifiers with the \naqueduct auth\n command-line tool, or as part of a utility provisioning script:\n\n\nimport\n \npackage:aqueduct/aqueduct.dart\n;\n\n\nimport\n \npackage:aqueduct/managed_auth.dart\n;\n\n\nimport\n \npackage:myapp/myapp.dart\n;\n\n\n\nFuture\n \nmain\n()\n \nasync\n \n{\n\n  \nvar\n \ndataModel\n \n=\n \nnew\n \nManagedDataModel\n.\nfromCurrentMirrorSystem\n();\n\n  \nManagedContext\n.\ndefaultContext\n \n=\n \nnew\n \nManagedContext\n(\ndataModel\n,\n \npersistentStore\n);\n\n\n  \nvar\n \ncredentials\n \n=\n \nAuthUtility\n.\ngenerateAPICredentialPair\n(\nlocal.testing\n,\n \nsecretpassword\n);\n\n\n  \nvar\n \nmanagedCredentials\n \n=\n \nnew\n \nManagedClient\n()\n\n    \n..\nid\n \n=\n \ncredentials\n.\nid\n\n    \n..\nhashedSecret\n \n=\n \ncredentials\n.\nhashedSecret\n\n    \n..\nsalt\n \n=\n \ncredentials\n.\nsalt\n;\n\n\n  \nvar\n \nquery\n \n=\n \nnew\n \nQuery\nManagedClient\n()..\nvalues\n \n=\n \nmanagedCredentials\n;\n\n  \nawait\n \nquery\n.\ninsert\n();\n\n\n}", 
            "title": "Developing Client Applications"
        }, 
        {
            "location": "/testing/clients/#using-aqueduct-when-writing-client-applications", 
            "text": "Running an Aqueduct server locally while developing client applications is an important part of the development process. Run applications through their  bin/main.dart  script or  aqueduct serve . The former allows for  debugging  the application with a debugger.", 
            "title": "Using Aqueduct when Writing Client Applications"
        }, 
        {
            "location": "/testing/clients/#enable-logging-and-return-server-errors", 
            "text": "Ensure that logging is on while developing client applications by registering a listener on  RequestSink.logger .  class   MyRequestSink   extends   RequestSink   { \n   MyRequestSink ( ApplicationConfiguration   config )   :   super ( config )   { \n     logger . onRecord . listen (( record )   { \n       print ( $ record   ${ record . error   ??   }   ${ record . stackTrace   ??   } ); \n     }); \n   } \n   ...  }   A useful feature to turn on during debugging is sending stack traces for 500 Server Error responses. Turn this flag on in a  RequestSink  while debugging:  class   MyRequestSink   extends   RequestSink   { \n   MyRequestSink ( ApplicationConfiguration   config )   :   super ( config )   { \n     RequestController . includeErrorDetailsInServerErrorResponses   =   true ; \n   } \n   ...  }   When a 500 error is encountered, the server will send the stack trace back to the client so you can view it while developing the client application without having to switch terminals. This property should never be left on for production code.", 
            "title": "Enable Logging and Return Server Errors"
        }, 
        {
            "location": "/testing/clients/#avoid-port-conflicts", 
            "text": "Aqueduct applications run through the  bin/main.dart  script default to port 8000. Applications run with  aqueduct serve  default to port 8081. You may use the  --port  command-line option to pick a different port:  aqueduct serve --port 4000", 
            "title": "Avoid Port Conflicts"
        }, 
        {
            "location": "/testing/clients/#provision-a-database-for-client-testing", 
            "text": "For applications that use the ORM, you must have a locally running database with your application's data model. See  provisioning a database  for more details.  If you are using OAuth 2.0, you must have also added client identifiers to the locally running database. You may add client identifiers with the  aqueduct auth  command-line tool, or as part of a utility provisioning script:  import   package:aqueduct/aqueduct.dart ;  import   package:aqueduct/managed_auth.dart ;  import   package:myapp/myapp.dart ;  Future   main ()   async   { \n   var   dataModel   =   new   ManagedDataModel . fromCurrentMirrorSystem (); \n   ManagedContext . defaultContext   =   new   ManagedContext ( dataModel ,   persistentStore ); \n\n   var   credentials   =   AuthUtility . generateAPICredentialPair ( local.testing ,   secretpassword ); \n\n   var   managedCredentials   =   new   ManagedClient () \n     .. id   =   credentials . id \n     .. hashedSecret   =   credentials . hashedSecret \n     .. salt   =   credentials . salt ; \n\n   var   query   =   new   Query ManagedClient ().. values   =   managedCredentials ; \n   await   query . insert ();  }", 
            "title": "Provision a Database for Client Testing"
        }, 
        {
            "location": "/testing/tests/", 
            "text": "Testing in Aqueduct\n\n\nFrom the ground up, Aqueduct is built to be tested. In practice, this means two things:\n\n\n\n\nA deployed Aqueduct application has zero code differences from an Aqueduct application under test.\n\n\nThere are helpful utilities for writing tests in Aqueduct.\n\n\n\n\nHow Tests are Written\n\n\nA project created with \naqueduct create\n contains a test harness (in \ntest/harness/app.dart\n) for starting and stopping an application. A very simple harness looks like this:\n\n\nimport\n \npackage:myapp/myapp.dart\n;\n\n\nimport\n \npackage:aqueduct/test.dart\n;\n\n\n\nexport\n \npackage:myapp/myapp.dart\n;\n\n\nexport\n \npackage:aqueduct/test.dart\n;\n\n\nexport\n \npackage:test/test.dart\n;\n\n\nexport\n \npackage:aqueduct/aqueduct.dart\n;\n\n\n\nclass\n \nTestApplication\n \n{\n\n  \nApplication\nAppSink\n \napplication\n;\n\n  \nAppSink\n \nget\n \nsink\n \n=\n \napplication\n.\nmainIsolateSink\n;\n\n  \nTestClient\n \nclient\n;\n\n\n  \nFuture\n \nstart\n()\n \nasync\n \n{\n\n    \nRequestController\n.\nletUncaughtExceptionsEscape\n \n=\n \ntrue\n;\n\n    \napplication\n \n=\n \nnew\n \nApplication\nAppSink\n();\n\n    \napplication\n.\nconfiguration\n.\nport\n \n=\n \n0\n;\n\n    \napplication\n.\nconfiguration\n.\nconfigurationFilePath\n \n=\n \nconfig.src.yaml\n;\n\n\n    \nawait\n \napplication\n.\nstart\n(\nrunOnMainIsolate:\n \ntrue\n);\n\n\n    \nclient\n \n=\n \nnew\n \nTestClient\n(\napplication\n);\n\n  \n}\n\n\n  \nFuture\n \nstop\n()\n \nasync\n \n{\n\n    \nawait\n \napplication\n?\n.\nstop\n();\n\n  \n}\n  \n\n}\n\n\n\n\n\n\nThe type \nAppSink\n is replaced with your application's \nRequestSink\n subclass. A test file need only import this harness and start and stop the application in its \nsetUpAll\n and \ntearDownAll\n callbacks:\n\n\nimport\n \nharness/app.dart\n;\n\n\n\nvoid\n \nmain\n()\n \n{\n\n  \nvar\n \napp\n \n=\n \nnew\n \nTestApplication\n();\n\n  \nsetUpAll\n(()\n \nasync\n \n{\n\n    \nawait\n \napp\n.\nstart\n();\n\n  \n});\n\n\n  \ntearDownAll\n(()\n \nasync\n \n{\n\n    \nawait\n \napp\n.\nstop\n();\n\n  \n});\n\n\n}\n\n\n\n\n\n\nNote that a test file must be in the \ntest/\n directory of a project and its file name must end with \n_test.dart\n.\n\n\nWhen executing tests, you use the test harness' \nclient\n to issue requests and verify their response:\n\n\ntest\n(\nThat we get a 200 from /endpoint\n,\n \n()\n \nasync\n \n{\n\n  \nvar\n \nresponse\n \n=\n \nawait\n \napp\n.\nclient\n.\nrequest\n(\n/endpoint\n).\nget\n();\n\n\n  \nexpect\n(\nresponse\n,\n \nhasStatus\n(\n200\n));\n\n\n});\n\n\n\n\n\n\nUsing a TestClient\n\n\nA \nTestClient\n creates requests (instances of \nTestRequest\n), which have execution methods (like \nget\n and \npost\n) that return responses (instances of \nTestResponse\n). The purpose of an Aqueduct test is to ensure that a request elicits the intended response. For example, you may want to make sure that a request with all the right parameters returns a response with the expected status code and JSON response body. Likewise, you may want to ensure that a request with some invalid parameters returns a response with the appropriate error information.\n\n\nA \nTestClient\n provides constant information - like the base URL, default headers or default credentials - to the instances of \nTestRequest\n it creates. There are three methods for creating a request. The path is a required argument to each and need not include the base URL, port or any other information other than the path. The most basic method for creating a request is simply \nrequest\n (we'll discuss the other three shortly):\n\n\nvar\n \nrequest\n \n=\n \napp\n.\nclient\n.\nrequest\n(\n/endpoint\n);\n\n\n\n\n\n\nA \nTestRequest\n can be configured with additional headers, request body data and query parameters before being executed. There are conveniences for different types of data. For example, it is often the case to add a JSON request body. The following will automatically encode a JSON request body from Dart objects and set the Content-Type of the request to \napplication/json; charset=utf-8\n:\n\n\nvar\n \nrequest\n \n=\n \napp\n.\nclient\n.\nrequest\n(\n/endpoint\n)\n\n  \n..\njson\n \n=\n \n{\n\n    \nid\n:\n \n1\n,\n\n    \nsomething\n:\n \nelse\n\n  \n};\n\n\n\n\n\n\nHeaders can be added directly with \nheaders\n or \naddHeader\n, where some more commonly used headers have exposed properties:\n\n\nrequest\n\n  \n..\naddHeader\n(\nx-application-id\n,\n \nsomething\n)\n\n  \n..\naccept\n \n=\n \n[\nContentType\n.\nJSON\n];\n\n\n\n\n\n\nOnce configured, an execution method returns a \nFuture\nTestResponse\n for the request. There are execution methods for each of the primary HTTP methods:\n\n\nvar\n \nresponse\n \n=\n \nawait\n \nrequest\n.\npost\n();\n\n\n\n\n\n\nSee a \nlater section\n on how to verify elements of a response.\n\n\nTesting Authorized Endpoints\n\n\nMost applications will have some form of authorization for its endpoints. For this purpose, both \nTestClient\n and \nTestRequest\n have behavior for managing authorization headers during testing. A \nTestRequest\n's authorization header can be set by one of the two following methods:\n\n\n// Base64 encodes username:password, sets \nAuthorization: Basic base64String\n\n\nrequest\n.\nsetBasicAuthorization\n(\nusername\n,\n \npassword\n);\n\n\n\n// Sets \nAuthorization: Bearer Abcaklaerj893r3jnjkn\n\n\nrequest\n.\nbearerAuthorization\n \n=\n \nAbcaklaerj893r3jnjkn\n;\n\n\n\n\n\n\nYou may also create requests with an authorization header through \nTestClient\n:\n\n\nvar\n \nrequest\n \n=\n \napp\n.\nclient\n.\nclientAuthenticatedRequest\n(\n\n  \n/endpoint\n,\n \nclientID:\n \nusername\n,\n \nclientSecret:\n \npassword\n);\n\n\n\nvar\n \nrequest\n \n=\n \napp\n.\nclient\n.\nauthenticatedRequest\n(\n\n  \n/endpoint\n,\n \naccessToken:\n \nAbcaklaerj893r3jnjkn\n);\n\n\n\n\n\n\nThe value of \nclientAuthenticatedRequest\n and \nauthenticatedRequest\n is that defaults can be provided to the \nTestClient\n for the username, password or access token.\n\n\napp\n.\nclient\n.\ndefaultAccessToken\n \n=\n \nAbcaklaerj893r3jnjkn\n;\n\n\n\n// Automatically includes header \nAuthorization: Bearer Abcaklaerj893r3jnjkn\n.\n\n\nvar\n \nrequest\n \n=\n \napp\n.\nclient\n.\nauthenticatedRequest\n(\n/endpoint\n);\n\n\n\n\n\n\nSee a \nlater section\n for more details on setting up tests that use authorization.\n\n\nVerifying Responses\n\n\nOnce a \nTestRequest\n is executed and returns a \nTestResponse\n, the real work begins: verifying the response is what you expect. A \nTestResponse\n has properties for the things you would typically expect of an HTTP response: status code, headers and body. In addition to the raw string \nbody\n property, the following body-inspecting properties exist:\n\n\n\n\ndecodedBody\n is the object created by decoding the response body according to its content-type\n\n\nasList\n is \ndecodedBody\n, but cast to a \nList\n\n\nasMap\n is \ndecodedBody\n, but cast to a \nMap\n\n\n\n\nThe great part about each of these three methods is that if the body cannot be decoded according to its content-type, or cannot be cast into the expected type, an exception is thrown and your tests fail. In other words, these methods implicitly test the validity of the response body.\n\n\nUsing individual properties of a \nTestResponse\n in test expectations is a valid use case, but there are some more helpful utilities for verifying responses more clearly.\n\n\nThe most important matcher is \nhasResponse\n. This matcher verifies a status code, headers and response body in a single function call. For example:\n\n\ntest\n(\nGet 200 with key value pair\n,\n \n()\n \nasync\n \n{\n\n  \nvar\n \nresponse\n \n=\n \nawait\n \napp\n.\nclient\n.\nrequest\n(\n/endpoint\n).\nget\n();\n\n\n  \nexpect\n(\nresponse\n,\n \nhasResponse\n(\n200\n,\n \n{\n\n    \nkey\n:\n \nvalue\n\n  \n},\n \nheaders:\n \n{\n\n    \nx-app\n:\n \nabcd\n\n  \n}));\n\n\n});\n\n\n\n\n\n\nThis will validate that not only does the response have a 200 status code, but its body - after decoding - is a \nMap\n that contains \nkey: value\n and it has the header \nx-app: abcd\n.\n\n\nMatchers from the official Dart test package can be mixed and matched into \nhasResponse\n:\n\n\ntest\n(\nGet 200 with key value pair\n,\n \n()\n \nasync\n \n{\n\n  \nvar\n \nresponse\n \n=\n \nawait\n \napp\n.\nclient\n.\nrequest\n(\n/endpoint\n).\nget\n();\n\n  \nexpect\n(\nresponse\n,\n \nhasResponse\n(\n200\n,\n \n{\n\n      \ncount\n:\n \ngreaterThan\n(\n1\n)\n\n  \n}));\n\n\n});\n\n\n\n\n\n\nThis ensures that the response's body is a map, for which the key \ncount\n has a value greater than 1. We can get even cuter - this test ensures that the body is a list of objects where every one is a map with the same property:\n\n\ntest\n(\nGet 200 with a lot of key value pairs\n,\n \n()\n \nasync\n \n{\n\n  \nvar\n \nresponse\n \n=\n \nawait\n \napp\n.\nclient\n.\nrequest\n(\n/endpoint\n).\nget\n();\n\n  \nexpect\n(\nresponse\n,\n \nhasResponse\n(\n200\n,\n \neveryElement\n({\n\n      \ncount\n:\n \ngreaterThan\n(\n1\n)\n\n  \n})));\n\n\n});\n\n\n\n\n\n\nAnother valuable matcher is \npartial\n. Sometimes it doesn't make sense to validate every single key-value pair in a response. The \npartial\n matcher only checks that the body has the specified keys - extra keys don't create a mismatch.\n\n\ntest\n(\nGet 200 that at least have these keys\n,\n \n()\n \nasync\n \n{\n\n  \nvar\n \nresponse\n \n=\n \nawait\n \napp\n.\nclient\n.\nrequest\n(\n/endpoint\n).\nget\n();\n\n  \nexpect\n(\nresponse\n,\n \nhasResponse\n(\n200\n,\n \npartial\n({\n\n    \nkey1\n:\n \nisInteger\n,\n\n    \nkey2\n:\n \nisString\n,\n\n    \nkey3\n:\n \nisTimestamp\n\n  \n})));\n\n\n});\n\n\n\n\n\n\nEven if the response has keys 4, 5 and 6, as long as the values for keys 1, 2 and 3 match, this test will pass.\n\n\nWhen using \npartial\n, you can also ensure that a map doesn't have a key with the \nisNotPresent\n matcher.\n\n\ntest\n(\nGet 200 that at least have these keys\n,\n \n()\n \nasync\n \n{\n\n  \nvar\n \nresponse\n \n=\n \nawait\n \napp\n.\nclient\n.\nrequest\n(\n/endpoint\n).\nget\n();\n\n  \nexpect\n(\nresponse\n,\n \nhasResponse\n(\n200\n,\n \npartial\n({\n\n    \nkey3\n:\n \nisNotPresent\n\n  \n})));\n\n\n});\n\n\n\n\n\n\nThis ensures that \nkey3\n is not in the map. This is different than verifying \nkey3: null\n, which would be true if \nkey3\n's value was actually the null value. See the API reference for more matchers.\n\n\nSee the \nAPI Reference\n for \naqueduct/test\n for more behaviors.\n\n\nVerifying Other Data Not in the Response\n\n\nSome requests will trigger changes that are not readily available in the response. For example, if a request uploads a file, the response doesn't necessarily tell you that uploading succeeded. For that reason, you may want to verify data stores and other services the application has after issuing a request.\n\n\nRecall from the test harness at the top of this guide, \nApplication.start\n has the flag \nrunOnMainIsolate: true\n. This is a special flag that turns off Aqueduct's multi-isolate behavior and is specifically used for testing. When running on the main isolate, the application's request channel and services are directly available to the test code. This allows you to verify any expected side-effects of a request. For example, by executing a query against a database:\n\n\ntest\n(\nStarting an upload creates a pending record in the database\n,\n \n()\n \nasync\n \n{\n\n  \nvar\n \nreq\n \n=\n \napp\n.\nclient\n.\nrequest\n(\n/upload\n)\n\n    \n..\ncontentType\n \n=\n \nContentType\n.\nTEXT\n\n    \n..\nbody\n \n=\n \nsomeFileContents\n;\n\n  \nvar\n \nresponse\n \n=\n \nawait\n \nreq\n.\npost\n();\n\n  \nexpect\n(\nresponse\n,\n \nhasStatus\n(\n202\n));\n\n\n  \nvar\n \nquery\n \n=\n \nnew\n \nQuery\nUpload\n()\n\n    \n..\nwhere\n.\npending\n \n=\n \nwhereEqualTo\n(\ntrue\n);\n\n  \nvar\n \npendingUpload\n \n=\n \nawait\n \nquery\n.\nfetchOne\n();\n\n\n  \nexpect\n(\nresponse\n.\nheaders\n.\nvalue\n(\nHttpHeaders\n.\nLOCATION\n),\n \npendingUpload\n.\npath\n);\n\n\n});\n\n\n\n\n\n\nAnything the \nRequestSink\n can access, so too can the tests.\n\n\nConfiguring the Test Harness\n\n\nThe test harness' primary responsibility is to start and stop the application. Recall from earlier in this guide, the test harness started an application like so:\n\n\nFuture\n \nstart\n()\n \nasync\n \n{\n\n  \nRequestController\n.\nletUncaughtExceptionsEscape\n \n=\n \ntrue\n;\n\n  \napplication\n \n=\n \nnew\n \nApplication\nAppSink\n();\n\n  \napplication\n.\nconfiguration\n.\nport\n \n=\n \n0\n;\n\n  \napplication\n.\nconfiguration\n.\nconfigurationFilePath\n \n=\n \nconfig.src.yaml\n;\n\n\n  \nawait\n \napplication\n.\nstart\n(\nrunOnMainIsolate:\n \ntrue\n);\n\n\n  \nclient\n \n=\n \nnew\n \nTestClient\n(\napplication\n);\n\n\n}\n\n\n\n\n\n\nThere are some interesting things to note here. First, the setting of \nRequestController.letUncaughtExceptionsEscape\n. This property defaults to false - if an unknown exception is thrown in request handling code, the \nRequestController\n catches it and send a 500 Server Error response to the client. This is an important behavior for a deployed Aqueduct application - the client gets back a response and your application continues running.\n\n\nHowever, when this flag is set to true, an uncaught exception will halt the application and fail the tests. This is the behavior you want during testing - it tells you something is wrong and gives you a stack trace to hunt down the problem.\n\n\nBy setting the port number to 0, the application listens on a random, unused port. This allows test suites to run in parallel - the \nTestClient\n takes care of managing which port to send requests on for you.\n\n\nThe concept and usage of \nconfig.src.yaml\n as a configuration file for tests is best explained in \nthis guide\n.\n\n\nFor basic behavior, this test harness is suitable. If an application is using the ORM or OAuth 2.0 features of Aqueduct, it should also handle provisioning a temporary database and inserting client identifiers and their scope. (Note: if you create an application using the \ndb\n or \ndb_and_auth\n templates, the test harness is already configured in the following ways.)\n\n\nConfiguring a Database for Tests\n\n\nIt is important that you fully control the data the application is using during testing, otherwise you may not be isolating and verifying the appropriate behavior. Aqueduct's testing strategy is to create all the tables for your application's database and seed them with data before a test, and then drop those tables at the end of a test. Because Aqueduct can build your data model as tables in a database, this behavior is effectively free.\n\n\nA test harness for an ORM application should have a method that creates a \ntemporary\n \nPersistentStore\n and uploads the application's data model.\n\n\nclass\n \nTestApplication\n \n{\n\n  \n...\n\n  \nstatic\n \nFuture\n \ncreateDatabaseSchema\n(\nManagedContext\n \ncontext\n)\n \nasync\n \n{\n\n    \nvar\n \nbuilder\n \n=\n \nnew\n \nSchemaBuilder\n.\ntoSchema\n(\n\n        \ncontext\n.\npersistentStore\n,\n\n        \nnew\n \nSchema\n.\nfromDataModel\n(\ncontext\n.\ndataModel\n),\n\n        \nisTemporary:\n \ntrue\n);\n\n\n    \nfor\n \n(\nvar\n \ncmd\n \nin\n \nbuilder\n.\ncommands\n)\n \n{\n\n      \nawait\n \ncontext\n.\npersistentStore\n.\nexecute\n(\ncmd\n);\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nThis method should be invoked within \nTestApplication.start\n, right after the application is started.\n\n\nFuture\n \nstart\n()\n \nasync\n \n{\n\n  \nRequestController\n.\nletUncaughtExceptionsEscape\n \n=\n \ntrue\n;\n\n  \napplication\n \n=\n \nnew\n \nApplication\nFoobarSink\n();\n\n  \napplication\n.\nconfiguration\n.\nport\n \n=\n \n0\n;\n\n  \napplication\n.\nconfiguration\n.\nconfigurationFilePath\n \n=\n \nconfig.src.yaml\n;\n\n\n  \nawait\n \napplication\n.\nstart\n(\nrunOnMainIsolate:\n \ntrue\n);\n\n\n  \nawait\n \ncreateDatabaseSchema\n(\nManagedContext\n.\ndefaultContext\n);\n\n\n  \nclient\n \n=\n \nnew\n \nTestClient\n(\napplication\n);\n\n\n}\n\n\n\n\n\n\nNotice that the \nManagedContext.defaultContext\n will have already been set by the application's \nRequestSink\n.\n\n\nAfter a test is executed, the test database should be cleared of data so that none of the stored data test leaks into the next test. Because starting and stopping an application isn't a cheap operation, it is often better to simply delete the contents of the database rather than restart the whole application. This is why the flag \nisTemporary\n in \nSchemaBuilder.toSchema\n matters: it creates \ntemporary\n tables that only live as long as the database connection. By simply reconnecting to the database, all of the tables and data created are discarded. Therefore, all you have to do is close the connection and add the database schema again.\n\n\nHere's a method to add to a test harness to do that. (Note that a connection is always reopened anytime a persistent store attempts to execute a query.)\n\n\nFuture\n \ndiscardPersistentData\n()\n \nasync\n \n{\n\n  \nawait\n \nManagedContext\n.\ndefaultContext\n.\npersistentStore\n.\nclose\n();\n\n  \nawait\n \ncreateDatabaseSchema\n(\nManagedContext\n.\ndefaultContext\n);\n\n\n}\n\n\n\n\n\n\nThis method gets invoked in the \ntearDown\n of your tests. It runs after each test.\n\n\nimport\n \nharness/app.dart\n;\n\n\n\nvoid\n \nmain\n()\n \n{\n\n  \nvar\n \napp\n \n=\n \nnew\n \nTestApplication\n();\n\n  \nsetUpAll\n(()\n \nasync\n \n{\n\n    \nawait\n \napp\n.\nstart\n();\n\n  \n});\n\n\n  \ntearDownAll\n(()\n \nasync\n \n{\n\n    \nawait\n \napp\n.\nstop\n();\n\n  \n});\n\n\n  \ntearDown\n(()\n \nasync\n \n{\n\n    \nawait\n \napp\n.\ndiscardPersistentData\n();\n\n  \n});\n\n\n}\n\n\n\n\n\n\nConfiguring OAuth 2.0 for Tests\n\n\nAn application that uses types like \nAuthServer\n and \nAuthorizer\n must have valid client IDs for testing. These are best set up in a test harness. Here's a method to add to a test harness to create client identifiers when using \nManagedAuthStorage\n:\n\n\nstatic\n \nFuture\nManagedClient\n \naddClientRecord\n(\n\n    \n{\nString\n \nclientID:\n \ndefault\n,\n\n    \nString\n \nclientSecret:\n \ndefault\n})\n \nasync\n \n{\n\n  \nvar\n \nsalt\n;\n\n  \nvar\n \nhashedPassword\n;\n\n  \nif\n \n(\nclientSecret\n \n!=\n \nnull\n)\n \n{\n\n    \nsalt\n \n=\n \nAuthUtility\n.\ngenerateRandomSalt\n();\n\n    \nhashedPassword\n \n=\n \nAuthUtility\n.\ngeneratePasswordHash\n(\nclientSecret\n,\n \nsalt\n);\n\n  \n}\n\n\n  \nvar\n \nclientQ\n \n=\n \nnew\n \nQuery\nManagedClient\n()\n\n    \n..\nvalues\n.\nid\n \n=\n \nclientID\n\n    \n..\nvalues\n.\nsalt\n \n=\n \nsalt\n\n    \n..\nvalues\n.\nhashedSecret\n \n=\n \nhashedPassword\n;\n\n  \nreturn\n \nclientQ\n.\ninsert\n();\n\n\n}\n\n\n\n\n\n\nThis method is invoked doing application startup and again after persistent data is discarded. Additionally, when creating a test client, it often makes sense to set the its default client ID and secret to some default client identifier:\n\n\nclient\n \n=\n \nnew\n \nTestClient\n(\napplication\n)\n\n  \n..\nclientID\n \n=\n \nDefaultClientID\n\n  \n..\nclientSecret\n \n=\n \nDefaultClientSecret\n;", 
            "title": "Writing Tests"
        }, 
        {
            "location": "/testing/tests/#testing-in-aqueduct", 
            "text": "From the ground up, Aqueduct is built to be tested. In practice, this means two things:   A deployed Aqueduct application has zero code differences from an Aqueduct application under test.  There are helpful utilities for writing tests in Aqueduct.", 
            "title": "Testing in Aqueduct"
        }, 
        {
            "location": "/testing/tests/#how-tests-are-written", 
            "text": "A project created with  aqueduct create  contains a test harness (in  test/harness/app.dart ) for starting and stopping an application. A very simple harness looks like this:  import   package:myapp/myapp.dart ;  import   package:aqueduct/test.dart ;  export   package:myapp/myapp.dart ;  export   package:aqueduct/test.dart ;  export   package:test/test.dart ;  export   package:aqueduct/aqueduct.dart ;  class   TestApplication   { \n   Application AppSink   application ; \n   AppSink   get   sink   =   application . mainIsolateSink ; \n   TestClient   client ; \n\n   Future   start ()   async   { \n     RequestController . letUncaughtExceptionsEscape   =   true ; \n     application   =   new   Application AppSink (); \n     application . configuration . port   =   0 ; \n     application . configuration . configurationFilePath   =   config.src.yaml ; \n\n     await   application . start ( runOnMainIsolate:   true ); \n\n     client   =   new   TestClient ( application ); \n   } \n\n   Future   stop ()   async   { \n     await   application ? . stop (); \n   }    }   The type  AppSink  is replaced with your application's  RequestSink  subclass. A test file need only import this harness and start and stop the application in its  setUpAll  and  tearDownAll  callbacks:  import   harness/app.dart ;  void   main ()   { \n   var   app   =   new   TestApplication (); \n   setUpAll (()   async   { \n     await   app . start (); \n   }); \n\n   tearDownAll (()   async   { \n     await   app . stop (); \n   });  }   Note that a test file must be in the  test/  directory of a project and its file name must end with  _test.dart .  When executing tests, you use the test harness'  client  to issue requests and verify their response:  test ( That we get a 200 from /endpoint ,   ()   async   { \n   var   response   =   await   app . client . request ( /endpoint ). get (); \n\n   expect ( response ,   hasStatus ( 200 ));  });", 
            "title": "How Tests are Written"
        }, 
        {
            "location": "/testing/tests/#using-a-testclient", 
            "text": "A  TestClient  creates requests (instances of  TestRequest ), which have execution methods (like  get  and  post ) that return responses (instances of  TestResponse ). The purpose of an Aqueduct test is to ensure that a request elicits the intended response. For example, you may want to make sure that a request with all the right parameters returns a response with the expected status code and JSON response body. Likewise, you may want to ensure that a request with some invalid parameters returns a response with the appropriate error information.  A  TestClient  provides constant information - like the base URL, default headers or default credentials - to the instances of  TestRequest  it creates. There are three methods for creating a request. The path is a required argument to each and need not include the base URL, port or any other information other than the path. The most basic method for creating a request is simply  request  (we'll discuss the other three shortly):  var   request   =   app . client . request ( /endpoint );   A  TestRequest  can be configured with additional headers, request body data and query parameters before being executed. There are conveniences for different types of data. For example, it is often the case to add a JSON request body. The following will automatically encode a JSON request body from Dart objects and set the Content-Type of the request to  application/json; charset=utf-8 :  var   request   =   app . client . request ( /endpoint ) \n   .. json   =   { \n     id :   1 , \n     something :   else \n   };   Headers can be added directly with  headers  or  addHeader , where some more commonly used headers have exposed properties:  request \n   .. addHeader ( x-application-id ,   something ) \n   .. accept   =   [ ContentType . JSON ];   Once configured, an execution method returns a  Future TestResponse  for the request. There are execution methods for each of the primary HTTP methods:  var   response   =   await   request . post ();   See a  later section  on how to verify elements of a response.", 
            "title": "Using a TestClient"
        }, 
        {
            "location": "/testing/tests/#testing-authorized-endpoints", 
            "text": "Most applications will have some form of authorization for its endpoints. For this purpose, both  TestClient  and  TestRequest  have behavior for managing authorization headers during testing. A  TestRequest 's authorization header can be set by one of the two following methods:  // Base64 encodes username:password, sets  Authorization: Basic base64String  request . setBasicAuthorization ( username ,   password );  // Sets  Authorization: Bearer Abcaklaerj893r3jnjkn  request . bearerAuthorization   =   Abcaklaerj893r3jnjkn ;   You may also create requests with an authorization header through  TestClient :  var   request   =   app . client . clientAuthenticatedRequest ( \n   /endpoint ,   clientID:   username ,   clientSecret:   password );  var   request   =   app . client . authenticatedRequest ( \n   /endpoint ,   accessToken:   Abcaklaerj893r3jnjkn );   The value of  clientAuthenticatedRequest  and  authenticatedRequest  is that defaults can be provided to the  TestClient  for the username, password or access token.  app . client . defaultAccessToken   =   Abcaklaerj893r3jnjkn ;  // Automatically includes header  Authorization: Bearer Abcaklaerj893r3jnjkn .  var   request   =   app . client . authenticatedRequest ( /endpoint );   See a  later section  for more details on setting up tests that use authorization.", 
            "title": "Testing Authorized Endpoints"
        }, 
        {
            "location": "/testing/tests/#verifying-responses", 
            "text": "Once a  TestRequest  is executed and returns a  TestResponse , the real work begins: verifying the response is what you expect. A  TestResponse  has properties for the things you would typically expect of an HTTP response: status code, headers and body. In addition to the raw string  body  property, the following body-inspecting properties exist:   decodedBody  is the object created by decoding the response body according to its content-type  asList  is  decodedBody , but cast to a  List  asMap  is  decodedBody , but cast to a  Map   The great part about each of these three methods is that if the body cannot be decoded according to its content-type, or cannot be cast into the expected type, an exception is thrown and your tests fail. In other words, these methods implicitly test the validity of the response body.  Using individual properties of a  TestResponse  in test expectations is a valid use case, but there are some more helpful utilities for verifying responses more clearly.  The most important matcher is  hasResponse . This matcher verifies a status code, headers and response body in a single function call. For example:  test ( Get 200 with key value pair ,   ()   async   { \n   var   response   =   await   app . client . request ( /endpoint ). get (); \n\n   expect ( response ,   hasResponse ( 200 ,   { \n     key :   value \n   },   headers:   { \n     x-app :   abcd \n   }));  });   This will validate that not only does the response have a 200 status code, but its body - after decoding - is a  Map  that contains  key: value  and it has the header  x-app: abcd .  Matchers from the official Dart test package can be mixed and matched into  hasResponse :  test ( Get 200 with key value pair ,   ()   async   { \n   var   response   =   await   app . client . request ( /endpoint ). get (); \n   expect ( response ,   hasResponse ( 200 ,   { \n       count :   greaterThan ( 1 ) \n   }));  });   This ensures that the response's body is a map, for which the key  count  has a value greater than 1. We can get even cuter - this test ensures that the body is a list of objects where every one is a map with the same property:  test ( Get 200 with a lot of key value pairs ,   ()   async   { \n   var   response   =   await   app . client . request ( /endpoint ). get (); \n   expect ( response ,   hasResponse ( 200 ,   everyElement ({ \n       count :   greaterThan ( 1 ) \n   })));  });   Another valuable matcher is  partial . Sometimes it doesn't make sense to validate every single key-value pair in a response. The  partial  matcher only checks that the body has the specified keys - extra keys don't create a mismatch.  test ( Get 200 that at least have these keys ,   ()   async   { \n   var   response   =   await   app . client . request ( /endpoint ). get (); \n   expect ( response ,   hasResponse ( 200 ,   partial ({ \n     key1 :   isInteger , \n     key2 :   isString , \n     key3 :   isTimestamp \n   })));  });   Even if the response has keys 4, 5 and 6, as long as the values for keys 1, 2 and 3 match, this test will pass.  When using  partial , you can also ensure that a map doesn't have a key with the  isNotPresent  matcher.  test ( Get 200 that at least have these keys ,   ()   async   { \n   var   response   =   await   app . client . request ( /endpoint ). get (); \n   expect ( response ,   hasResponse ( 200 ,   partial ({ \n     key3 :   isNotPresent \n   })));  });   This ensures that  key3  is not in the map. This is different than verifying  key3: null , which would be true if  key3 's value was actually the null value. See the API reference for more matchers.  See the  API Reference  for  aqueduct/test  for more behaviors.", 
            "title": "Verifying Responses"
        }, 
        {
            "location": "/testing/tests/#verifying-other-data-not-in-the-response", 
            "text": "Some requests will trigger changes that are not readily available in the response. For example, if a request uploads a file, the response doesn't necessarily tell you that uploading succeeded. For that reason, you may want to verify data stores and other services the application has after issuing a request.  Recall from the test harness at the top of this guide,  Application.start  has the flag  runOnMainIsolate: true . This is a special flag that turns off Aqueduct's multi-isolate behavior and is specifically used for testing. When running on the main isolate, the application's request channel and services are directly available to the test code. This allows you to verify any expected side-effects of a request. For example, by executing a query against a database:  test ( Starting an upload creates a pending record in the database ,   ()   async   { \n   var   req   =   app . client . request ( /upload ) \n     .. contentType   =   ContentType . TEXT \n     .. body   =   someFileContents ; \n   var   response   =   await   req . post (); \n   expect ( response ,   hasStatus ( 202 )); \n\n   var   query   =   new   Query Upload () \n     .. where . pending   =   whereEqualTo ( true ); \n   var   pendingUpload   =   await   query . fetchOne (); \n\n   expect ( response . headers . value ( HttpHeaders . LOCATION ),   pendingUpload . path );  });   Anything the  RequestSink  can access, so too can the tests.", 
            "title": "Verifying Other Data Not in the Response"
        }, 
        {
            "location": "/testing/tests/#configuring-the-test-harness", 
            "text": "The test harness' primary responsibility is to start and stop the application. Recall from earlier in this guide, the test harness started an application like so:  Future   start ()   async   { \n   RequestController . letUncaughtExceptionsEscape   =   true ; \n   application   =   new   Application AppSink (); \n   application . configuration . port   =   0 ; \n   application . configuration . configurationFilePath   =   config.src.yaml ; \n\n   await   application . start ( runOnMainIsolate:   true ); \n\n   client   =   new   TestClient ( application );  }   There are some interesting things to note here. First, the setting of  RequestController.letUncaughtExceptionsEscape . This property defaults to false - if an unknown exception is thrown in request handling code, the  RequestController  catches it and send a 500 Server Error response to the client. This is an important behavior for a deployed Aqueduct application - the client gets back a response and your application continues running.  However, when this flag is set to true, an uncaught exception will halt the application and fail the tests. This is the behavior you want during testing - it tells you something is wrong and gives you a stack trace to hunt down the problem.  By setting the port number to 0, the application listens on a random, unused port. This allows test suites to run in parallel - the  TestClient  takes care of managing which port to send requests on for you.  The concept and usage of  config.src.yaml  as a configuration file for tests is best explained in  this guide .  For basic behavior, this test harness is suitable. If an application is using the ORM or OAuth 2.0 features of Aqueduct, it should also handle provisioning a temporary database and inserting client identifiers and their scope. (Note: if you create an application using the  db  or  db_and_auth  templates, the test harness is already configured in the following ways.)", 
            "title": "Configuring the Test Harness"
        }, 
        {
            "location": "/testing/tests/#configuring-a-database-for-tests", 
            "text": "It is important that you fully control the data the application is using during testing, otherwise you may not be isolating and verifying the appropriate behavior. Aqueduct's testing strategy is to create all the tables for your application's database and seed them with data before a test, and then drop those tables at the end of a test. Because Aqueduct can build your data model as tables in a database, this behavior is effectively free.  A test harness for an ORM application should have a method that creates a  temporary   PersistentStore  and uploads the application's data model.  class   TestApplication   { \n   ... \n   static   Future   createDatabaseSchema ( ManagedContext   context )   async   { \n     var   builder   =   new   SchemaBuilder . toSchema ( \n         context . persistentStore , \n         new   Schema . fromDataModel ( context . dataModel ), \n         isTemporary:   true ); \n\n     for   ( var   cmd   in   builder . commands )   { \n       await   context . persistentStore . execute ( cmd ); \n     } \n   }  }   This method should be invoked within  TestApplication.start , right after the application is started.  Future   start ()   async   { \n   RequestController . letUncaughtExceptionsEscape   =   true ; \n   application   =   new   Application FoobarSink (); \n   application . configuration . port   =   0 ; \n   application . configuration . configurationFilePath   =   config.src.yaml ; \n\n   await   application . start ( runOnMainIsolate:   true ); \n\n   await   createDatabaseSchema ( ManagedContext . defaultContext ); \n\n   client   =   new   TestClient ( application );  }   Notice that the  ManagedContext.defaultContext  will have already been set by the application's  RequestSink .  After a test is executed, the test database should be cleared of data so that none of the stored data test leaks into the next test. Because starting and stopping an application isn't a cheap operation, it is often better to simply delete the contents of the database rather than restart the whole application. This is why the flag  isTemporary  in  SchemaBuilder.toSchema  matters: it creates  temporary  tables that only live as long as the database connection. By simply reconnecting to the database, all of the tables and data created are discarded. Therefore, all you have to do is close the connection and add the database schema again.  Here's a method to add to a test harness to do that. (Note that a connection is always reopened anytime a persistent store attempts to execute a query.)  Future   discardPersistentData ()   async   { \n   await   ManagedContext . defaultContext . persistentStore . close (); \n   await   createDatabaseSchema ( ManagedContext . defaultContext );  }   This method gets invoked in the  tearDown  of your tests. It runs after each test.  import   harness/app.dart ;  void   main ()   { \n   var   app   =   new   TestApplication (); \n   setUpAll (()   async   { \n     await   app . start (); \n   }); \n\n   tearDownAll (()   async   { \n     await   app . stop (); \n   }); \n\n   tearDown (()   async   { \n     await   app . discardPersistentData (); \n   });  }", 
            "title": "Configuring a Database for Tests"
        }, 
        {
            "location": "/testing/tests/#configuring-oauth-20-for-tests", 
            "text": "An application that uses types like  AuthServer  and  Authorizer  must have valid client IDs for testing. These are best set up in a test harness. Here's a method to add to a test harness to create client identifiers when using  ManagedAuthStorage :  static   Future ManagedClient   addClientRecord ( \n     { String   clientID:   default , \n     String   clientSecret:   default })   async   { \n   var   salt ; \n   var   hashedPassword ; \n   if   ( clientSecret   !=   null )   { \n     salt   =   AuthUtility . generateRandomSalt (); \n     hashedPassword   =   AuthUtility . generatePasswordHash ( clientSecret ,   salt ); \n   } \n\n   var   clientQ   =   new   Query ManagedClient () \n     .. values . id   =   clientID \n     .. values . salt   =   salt \n     .. values . hashedSecret   =   hashedPassword ; \n   return   clientQ . insert ();  }   This method is invoked doing application startup and again after persistent data is discarded. Additionally, when creating a test client, it often makes sense to set the its default client ID and secret to some default client identifier:  client   =   new   TestClient ( application ) \n   .. clientID   =   DefaultClientID \n   .. clientSecret   =   DefaultClientSecret ;", 
            "title": "Configuring OAuth 2.0 for Tests"
        }, 
        {
            "location": "/testing/mock/", 
            "text": "Mocking External Services\n\n\nAn Aqueduct application often communicates with another server. For example, an application might make requests to the GitHub API to collect analytics about a team's workflow. When running automated tests, consuming the actual GitHub API isn't feasible - because GitHub will probably rate limit you and because the data being returned is constantly changing.\n\n\nTo solve this problem, you can create \"mocks\" of a service during testing. Aqueduct has two testing utilities for this purpose - \nMockServer\n and \nMockHTTPServer\n - in the \naqueduct/test\n library.\n\n\nUsing a MockHTTPServer\n\n\nWhen testing your application, you send it requests using a \nTestClient\n. As part of the request handling logic, your application might issue requests to some other server. \nMockHTTPServer\n allows you to validate that the request your application sent was correct and gives you control what the responses are to those requests. For example, \ngithubMock\n is an instance of \nMockHTTPServer\n in the following test, which ensures that the request was constructed correctly:\n\n\ntest\n(\nWill get correct user from GitHub\n,\n \n()\n \nasync\n \n{\n\n  \nvar\n \nresponse\n \n=\n\n    \nawait\n \napp\n.\nclient\n.\nauthenticatedRequest\n(\n/github_profile/fred\n).\nget\n();\n\n\n  \nvar\n \nrequestSentByYourApplicationToGitHub\n \n=\n \nawait\n \ngithubMock\n.\nnext\n();\n\n  \nexpect\n(\nrequestSentByYourApplicationToGitHub\n.\nmethod\n,\n \nGET\n);\n\n  \nexpect\n(\nrequestSentByYourApplicationToGitHub\n.\npath\n,\n \n/users/search?name=fred\n);\n\n\n});\n\n\n\n\n\n\nIn the above code, we are expecting that anytime the request \nGET /github_profile/fred\n is sent to your application, that it turns around and searches for a user in GitHub's API. This test ensures that we have correctly translated our request to a request to be made to the GitHub API. If no request was made - because of a programmer error - this test would fail because the \nFuture\n returned from \ngithubMock.next()\n would never complete. There is no next request, because none was ever delivered!\n\n\nBy default, any request sent to a \nMockHTTPServer\n is a 200 OK Response with an empty body. You may change this behavior by queuing responses in a mock server.\n\n\ntest\n(\nWill get correct user from GitHub\n,\n \n()\n \nasync\n \n{\n\n  \ngithubMock\n.\nqueueResponse\n(\nnew\n \nResponse\n.\nok\n({\nid\n:\n \n1\n,\n \nname\n:\n \nfred\n}));\n\n\n  \nvar\n \nresponse\n \n=\n\n    \nawait\n \napp\n.\nclient\n.\nauthenticatedRequest\n(\n/github_profile/fred\n).\nget\n();\n\n  \nexpect\n(\nresponse\n,\n \nhasResponse\n(\n200\n,\n \npartial\n({\n\n    \nid\n:\n \n1\n,\n\n    \nname\n:\n \nfred\n\n  \n})))\n\n\n});\n\n\n\n\n\n\nIn the above code, \nqueueResponse\n adds a 200 OK Response to the mock server queue with a specific body. The mock server will send that response for the next request it receives. In the implementation of \n/github_profile/fred\n, your application sends a \nGET /users/search?name=fred\n to the GitHub API - except the GitHub API is your mock server, and it returns the response you queued instead. Thus, the queued up response is the expected response of the GitHub API.\n\n\nAfter the request completes, the response is removed from the queue and subsequent responses will go back to the default. You may queue as many responses as you like. You may also simulate a failed request - one that never gets a response - like so:\n\n\nmockServer\n.\nqueueResponse\n(\nMockHTTPServer\n.\nmockConnectionFailureResponse\n);\n\n\n\n\n\n\nYou may also subclass \nMockHTTPServer\n and override its \nopen\n method to add logic to determine the response. Please see the implementation of \nMockHTTPServer.open\n for more details.\n\n\nConfiguring a MockHTTPServer\n\n\nA \nMockHTTPServer\n is created when setting up tests. It must be closed when tearing down tests. If you use the same mock server to across all tests (e.g., open it in \nsetUpAll\n), make sure to clear it after each test:\n\n\nimport\n \npackage:aqueduct/test.dart\n;\n\n\n\nvoid\n \nmain\n()\n \n{\n\n  \nvar\n \nmockServer\n \n=\n \nnew\n \nMockHTTPServer\n(\n4000\n);\n\n\n  \nsetUpAll\n(()\n \nasync\n \n{\n\n    \nawait\n \nmockServer\n.\nopen\n();\n\n  \n});\n\n\n  \ntearDownAll\n(()\n \nasync\n \n{\n\n    \nawait\n \nmockServer\n.\nclose\n();\n\n  \n});\n\n\n  \ntearDown\n(()\n \nasync\n \n{\n\n    \nmockServer\n.\nclear\n();\n\n  \n});\n\n\n}\n\n\n\n\n\n\nAn instance of \nMockHTTPServer\n listens on localhost on a specific port. An application that makes testable external service requests should provide the base URI for those services in a configuration file. The URI for that service in the \nconfiguration file used during testing\n should point at localhost and a specific port. For example, if a deployed \nconfig.yaml\n file has the following key-values:\n\n\ngithub\n:\n\n  \nbaseURL\n:\n \nhttps\n://\napi\n.\ngithub\n.\ncom\n/\n  \n\n\n\n\n\nThen \nconfig.src.yaml\n would have:\n\n\ngithub\n:\n\n  \nbaseURL\n:\n \nhttp\n://\nlocalhost\n:\n4000\n/\n\n\n\n\n\n\nYour application reads this configuration file and injects the base URL into the service that will execute requests.\n\n\nclass\n \nAppConfigurationItem\n \nextends\n \nConfigurationItem\n \n{\n\n  \nAppConfigurationItem\n(\nString\n \nfileName\n)\n \n:\n \nsuper\n.\nfromFile\n(\nfileName\n);\n\n\n  \nAPIConfiguration\n \ngithub\n;\n\n\n}\n\n\n\nclass\n \nAppRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nAppRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n    \nvar\n \nconfigValues\n \n=\n \nnew\n \nAppConfigurationItem\n(\nconfig\n.\nconfigurationFilePath\n);\n\n\n    \ngithubService\n \n=\n \nnew\n \nGitHubService\n(\nbaseURL:\n \nconfigValues\n.\ngithub\n.\nbaseURL\n);\n\n  \n}\n\n\n}\n\n\n\n\n\n\nNote that \nAPIConfiguration\n is an existing type and is meant for this purpose.\n\n\nAlso note that the testing strategy for database connections is \nnot\n to use a mock but to use a temporary, local database that is set up and torn down during tests. This is possible because you own the data model generating code - whereas you probably don't have access to an external service's development API.", 
            "title": "Mocking Services"
        }, 
        {
            "location": "/testing/mock/#mocking-external-services", 
            "text": "An Aqueduct application often communicates with another server. For example, an application might make requests to the GitHub API to collect analytics about a team's workflow. When running automated tests, consuming the actual GitHub API isn't feasible - because GitHub will probably rate limit you and because the data being returned is constantly changing.  To solve this problem, you can create \"mocks\" of a service during testing. Aqueduct has two testing utilities for this purpose -  MockServer  and  MockHTTPServer  - in the  aqueduct/test  library.", 
            "title": "Mocking External Services"
        }, 
        {
            "location": "/testing/mock/#using-a-mockhttpserver", 
            "text": "When testing your application, you send it requests using a  TestClient . As part of the request handling logic, your application might issue requests to some other server.  MockHTTPServer  allows you to validate that the request your application sent was correct and gives you control what the responses are to those requests. For example,  githubMock  is an instance of  MockHTTPServer  in the following test, which ensures that the request was constructed correctly:  test ( Will get correct user from GitHub ,   ()   async   { \n   var   response   = \n     await   app . client . authenticatedRequest ( /github_profile/fred ). get (); \n\n   var   requestSentByYourApplicationToGitHub   =   await   githubMock . next (); \n   expect ( requestSentByYourApplicationToGitHub . method ,   GET ); \n   expect ( requestSentByYourApplicationToGitHub . path ,   /users/search?name=fred );  });   In the above code, we are expecting that anytime the request  GET /github_profile/fred  is sent to your application, that it turns around and searches for a user in GitHub's API. This test ensures that we have correctly translated our request to a request to be made to the GitHub API. If no request was made - because of a programmer error - this test would fail because the  Future  returned from  githubMock.next()  would never complete. There is no next request, because none was ever delivered!  By default, any request sent to a  MockHTTPServer  is a 200 OK Response with an empty body. You may change this behavior by queuing responses in a mock server.  test ( Will get correct user from GitHub ,   ()   async   { \n   githubMock . queueResponse ( new   Response . ok ({ id :   1 ,   name :   fred })); \n\n   var   response   = \n     await   app . client . authenticatedRequest ( /github_profile/fred ). get (); \n   expect ( response ,   hasResponse ( 200 ,   partial ({ \n     id :   1 , \n     name :   fred \n   })))  });   In the above code,  queueResponse  adds a 200 OK Response to the mock server queue with a specific body. The mock server will send that response for the next request it receives. In the implementation of  /github_profile/fred , your application sends a  GET /users/search?name=fred  to the GitHub API - except the GitHub API is your mock server, and it returns the response you queued instead. Thus, the queued up response is the expected response of the GitHub API.  After the request completes, the response is removed from the queue and subsequent responses will go back to the default. You may queue as many responses as you like. You may also simulate a failed request - one that never gets a response - like so:  mockServer . queueResponse ( MockHTTPServer . mockConnectionFailureResponse );   You may also subclass  MockHTTPServer  and override its  open  method to add logic to determine the response. Please see the implementation of  MockHTTPServer.open  for more details.", 
            "title": "Using a MockHTTPServer"
        }, 
        {
            "location": "/testing/mock/#configuring-a-mockhttpserver", 
            "text": "A  MockHTTPServer  is created when setting up tests. It must be closed when tearing down tests. If you use the same mock server to across all tests (e.g., open it in  setUpAll ), make sure to clear it after each test:  import   package:aqueduct/test.dart ;  void   main ()   { \n   var   mockServer   =   new   MockHTTPServer ( 4000 ); \n\n   setUpAll (()   async   { \n     await   mockServer . open (); \n   }); \n\n   tearDownAll (()   async   { \n     await   mockServer . close (); \n   }); \n\n   tearDown (()   async   { \n     mockServer . clear (); \n   });  }   An instance of  MockHTTPServer  listens on localhost on a specific port. An application that makes testable external service requests should provide the base URI for those services in a configuration file. The URI for that service in the  configuration file used during testing  should point at localhost and a specific port. For example, if a deployed  config.yaml  file has the following key-values:  github : \n   baseURL :   https :// api . github . com /     Then  config.src.yaml  would have:  github : \n   baseURL :   http :// localhost : 4000 /   Your application reads this configuration file and injects the base URL into the service that will execute requests.  class   AppConfigurationItem   extends   ConfigurationItem   { \n   AppConfigurationItem ( String   fileName )   :   super . fromFile ( fileName ); \n\n   APIConfiguration   github ;  }  class   AppRequestSink   extends   RequestSink   { \n   AppRequestSink ( ApplicationConfiguration   config )   :   super ( config )   { \n     var   configValues   =   new   AppConfigurationItem ( config . configurationFilePath ); \n\n     githubService   =   new   GitHubService ( baseURL:   configValues . github . baseURL ); \n   }  }   Note that  APIConfiguration  is an existing type and is meant for this purpose.  Also note that the testing strategy for database connections is  not  to use a mock but to use a temporary, local database that is set up and torn down during tests. This is possible because you own the data model generating code - whereas you probably don't have access to an external service's development API.", 
            "title": "Configuring a MockHTTPServer"
        }, 
        {
            "location": "/deploy/overview/", 
            "text": "Tasks\n\n\nAqueduct has a built in tool, \naqueduct\n, for deploying Aqueduct applications, managing database schemas, OAuth 2.0 clients and creating projects. See \nGetting Started\n for installation instructions. Many of the tasks for deployment rely on using this tool.\n\n\nAqueduct applications can be run anywhere that Dart can be installed. This topic covers deployment tasks on common hosting services. If you are just getting started or a hobbyist, you should consider using \nHeroku\n to host your applications.\n\n\nGuides\n\n\n\n\nRunning an Aqueduct Application Locally\n\n\nRunning an Aqueduct Application on Heroku\n\n\nRunning an Aqueduct Application on Amazon Web Services (AWS)\n\n\nRunning an Aqueduct Application without aqueduct serve", 
            "title": "Overview"
        }, 
        {
            "location": "/deploy/overview/#tasks", 
            "text": "Aqueduct has a built in tool,  aqueduct , for deploying Aqueduct applications, managing database schemas, OAuth 2.0 clients and creating projects. See  Getting Started  for installation instructions. Many of the tasks for deployment rely on using this tool.  Aqueduct applications can be run anywhere that Dart can be installed. This topic covers deployment tasks on common hosting services. If you are just getting started or a hobbyist, you should consider using  Heroku  to host your applications.", 
            "title": "Tasks"
        }, 
        {
            "location": "/deploy/overview/#guides", 
            "text": "Running an Aqueduct Application Locally  Running an Aqueduct Application on Heroku  Running an Aqueduct Application on Amazon Web Services (AWS)  Running an Aqueduct Application without aqueduct serve", 
            "title": "Guides"
        }, 
        {
            "location": "/deploy/deploy_local/", 
            "text": "Deploying an Aqueduct Application on a Local Machine\n\n\nFor other deployment options, see \nDeploying Aqueduct Applications\n.\n\n\nPurpose\n\n\nTo run a local development version of an Aqueduct application with persistent storage. This is useful in developing client applications against an Aqueduct application. Make sure to also read \nTesting Aqueduct Applications\n.\n\n\nPrerequisites\n\n\n\n\nDart has been installed.\n\n\nPostgreSQL has been installed locally.\n\n\nAqueduct has been activated globally.\n\n\nAn application has been created with \naqueduct create\n.\n\n\n\n\nIf one or more of these is not true, see \nGetting Started\n.\n\n\nOverview\n\n\n\n\nCreate a local database.\n\n\nUpload the application schema to the local database.\n\n\nAdd an OAuth 2.0 client.\n\n\nModify the configuration file.\n\n\nRun the application.\n\n\n\n\nEstimated Time: \n5 minutes.\n\n\nStep 1: Create a Local Database\n\n\nCreate a database with the same name as your application and a user that can access that database. Do not use the name 'dart_test' for the database; this database is used by Aqueduct to run tests by default.\n\n\nRun the following SQL locally with a user that has privileges to create databases. (If using \nPostgres.app\n, open the \npsql\n terminal from the \nPostgres.app\n status menu item \nOpen psql\n).\n\n\nCREATE\n \nDATABASE\n \napp_name\n;\n\n\nCREATE\n \nUSER\n \napp_name_user\n \nWITH\n \nCREATEDB\n;\n\n\nALTER\n \nUSER\n \napp_name_user\n \nWITH\n \nPASSWORD\n \nyourpassword\n;\n\n\nGRANT\n \nALL\n \nON\n \nDATABASE\n \napp_name\n \nTO\n \napp_name_user\n;\n\n\n\n\n\n\nStep 2: Upload the Application Schema\n\n\nRun the database schema generation tool from the project directory:\n\n\naqueduct db generate\n\n\n\n\n\nThis command creates the file \nmigrations/00000001_Initial.migration.dart\n. Now, run the database migration tool to execute the migration file against the local database. Ensure that the values for the option \n--connect\n match those of the database created in the last step.\n\n\naqueduct db upgrade --connect postgres://app_name_user:yourpassword@localhost:5432/app_name\n\n\n\n\n\n(Note that you may provide database credentials in a file named \ndatabase.yaml\n instead of using \n--connect\n. See \naqueduct db --help\n for details.)\n\n\nStep 3: Add an OAuth 2.0 client.\n\n\nFrom the command line, run the following, ensuring that the values for the option \n--connect\n match the recently created database.\n\n\naqueduct auth add-client --id com.app_name.standard --secret abcdefghi --connect postgres://app_name_user:yourpassword@localhost:5432/app_name\n\n\n\n\n\nStep 4: Modify the Configuration File\n\n\nIf \nconfig.yaml\n doesn't exist, create it by copying the configuration file template \nconfig.yaml.src\n.\n\n\nIn \nconfig.yaml\n, update the database credentials to the local database.\n\n\ndatabase\n:\n\n \nusername\n:\n \napp_name_user\n\n \npassword\n:\n \nyourpassword\n\n \nhost\n:\n \nlocalhost\n\n \nport\n:\n \n5432\n\n \ndatabaseName\n:\n \napp_name\n\n\n\n\n\n\nStep 5: Run the Application\n\n\nFrom the project directory, run:\n\n\naqueduct serve\n\n\n\n\n\nYour application is now running.\n\n\nNote: You can add the \n--observe\n flag to \naqueduct serve\n to run Observatory. Observatory will automatically open in a browser if the platform supports it.", 
            "title": "Deploy Locally"
        }, 
        {
            "location": "/deploy/deploy_local/#deploying-an-aqueduct-application-on-a-local-machine", 
            "text": "For other deployment options, see  Deploying Aqueduct Applications .", 
            "title": "Deploying an Aqueduct Application on a Local Machine"
        }, 
        {
            "location": "/deploy/deploy_local/#purpose", 
            "text": "To run a local development version of an Aqueduct application with persistent storage. This is useful in developing client applications against an Aqueduct application. Make sure to also read  Testing Aqueduct Applications .", 
            "title": "Purpose"
        }, 
        {
            "location": "/deploy/deploy_local/#prerequisites", 
            "text": "Dart has been installed.  PostgreSQL has been installed locally.  Aqueduct has been activated globally.  An application has been created with  aqueduct create .   If one or more of these is not true, see  Getting Started .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/deploy/deploy_local/#overview", 
            "text": "Create a local database.  Upload the application schema to the local database.  Add an OAuth 2.0 client.  Modify the configuration file.  Run the application.   Estimated Time:  5 minutes.", 
            "title": "Overview"
        }, 
        {
            "location": "/deploy/deploy_local/#step-1-create-a-local-database", 
            "text": "Create a database with the same name as your application and a user that can access that database. Do not use the name 'dart_test' for the database; this database is used by Aqueduct to run tests by default.  Run the following SQL locally with a user that has privileges to create databases. (If using  Postgres.app , open the  psql  terminal from the  Postgres.app  status menu item  Open psql ).  CREATE   DATABASE   app_name ;  CREATE   USER   app_name_user   WITH   CREATEDB ;  ALTER   USER   app_name_user   WITH   PASSWORD   yourpassword ;  GRANT   ALL   ON   DATABASE   app_name   TO   app_name_user ;", 
            "title": "Step 1: Create a Local Database"
        }, 
        {
            "location": "/deploy/deploy_local/#step-2-upload-the-application-schema", 
            "text": "Run the database schema generation tool from the project directory:  aqueduct db generate  This command creates the file  migrations/00000001_Initial.migration.dart . Now, run the database migration tool to execute the migration file against the local database. Ensure that the values for the option  --connect  match those of the database created in the last step.  aqueduct db upgrade --connect postgres://app_name_user:yourpassword@localhost:5432/app_name  (Note that you may provide database credentials in a file named  database.yaml  instead of using  --connect . See  aqueduct db --help  for details.)", 
            "title": "Step 2: Upload the Application Schema"
        }, 
        {
            "location": "/deploy/deploy_local/#step-3-add-an-oauth-20-client", 
            "text": "From the command line, run the following, ensuring that the values for the option  --connect  match the recently created database.  aqueduct auth add-client --id com.app_name.standard --secret abcdefghi --connect postgres://app_name_user:yourpassword@localhost:5432/app_name", 
            "title": "Step 3: Add an OAuth 2.0 client."
        }, 
        {
            "location": "/deploy/deploy_local/#step-4-modify-the-configuration-file", 
            "text": "If  config.yaml  doesn't exist, create it by copying the configuration file template  config.yaml.src .  In  config.yaml , update the database credentials to the local database.  database : \n  username :   app_name_user \n  password :   yourpassword \n  host :   localhost \n  port :   5432 \n  databaseName :   app_name", 
            "title": "Step 4: Modify the Configuration File"
        }, 
        {
            "location": "/deploy/deploy_local/#step-5-run-the-application", 
            "text": "From the project directory, run:  aqueduct serve  Your application is now running.  Note: You can add the  --observe  flag to  aqueduct serve  to run Observatory. Observatory will automatically open in a browser if the platform supports it.", 
            "title": "Step 5: Run the Application"
        }, 
        {
            "location": "/deploy/deploy_heroku/", 
            "text": "Deploying an Aqueduct Application on Heroku\n\n\nFor other deployment options, see \nDeploying Aqueduct Applications\n.\n\n\nPurpose\n\n\nTo run a production Aqueduct application on Heroku. Make sure to also read \nTesting Aqueduct Applications\n.\n\n\nPrerequisites\n\n\n\n\nDart has been installed.\n\n\nA Heroku account.\n\n\ngit\n has been installed.\n\n\nheroku\n has been installed.\n\n\nAqueduct has been activated.\n\n\n\n\nOverview\n\n\n\n\nSetting up a Heroku application\n\n\nSetting up an Aqueduct application to run on Heroku\n\n\nConfiguring application values\n\n\nRunning the Aqueduct application\n\n\n\n\nEstimated Time: \n5 minutes.\n\n\nStep 1: Setting up a Heroku Application\n\n\nCreate a new application in Heroku. Add the 'Heroku Postgres' add-on.\n\n\nNavigate to the Settings tab in the Heroku web interface and click 'Reveal Config Vars'. Note the DATABASE_URL, it'll get used later.\n\n\nStep 2: Setting up an Aqueduct Application to Run on Heroku\n\n\nIf you have not yet, create a new Aqueduct application on your local machine, go into that directory, and initialize it as a git repository:\n\n\naqueduct create app_name\n\ncd\n app_name\ngit init\n\n\n\n\n\nLogin to Heroku and run the Aqueduct tool to configure a project for Heroku. The value for \n--heroku\n \nmust\n be the name of the Heroku application (not the Aqueduct application, unless they are the same, obvi).\n\n\nheroku login\naqueduct setup --heroku\n=\napp_name\n\n\n\n\n\nThis command will create the files Heroku needs to run the application, remove \nconfig.yaml\n from \n.gitignore\n (you'll see why in a moment) and runs some \nheroku\n commands to set up the Heroku application's environment.\n\n\nStep 3: Configuring Application Values\n\n\nHeroku provides configuration values through environment variables, where Aqueduct normally provides them in \nconfig.yaml\n file. Because Aqueduct uses \nsafe_config\n, configuration files can map keys to environment variables with a simple syntax. The \nconfig.yaml\n file's values get replaced with their environment variable names and it gets checked into source control. To map configuration values to an environment variable, the value for a configuration key is prefixed with a dollar sign (\n$\n) followed by the case-sensitive name of the environment variable.\n\n\nModify \nconfig.yaml\n to appear as follows:\n\n\ndatabase: $DATABASE_URL\nlogging:\n type: console\n\n\n\n\n\nRecall that \naqueduct setup\n with the \n--heroku\n option removes \nconfig.yaml\n from \n.gitignore\n.\n\n\nStep 4: Running the Aqueduct Application\n\n\nFirst, create a database migration. The \nProcfile\n declared that Heroku will automatically run any migration files prior to running the application as long as they are checked into source control.\n\n\naqueduct db generate\n\n\n\n\n\nNow, add all of the files to \ngit\n and push it to heroku:\n\n\ngit add .\ngit commit -m \ninitial commit\n\ngit push heroku master\n\n\n\n\n\nNext, set up an OAuth 2.0 client id:\n\n\nheroku run /app/dart-sdk/bin/pub global run aqueduct:aqueduct auth add-client --id com.app.standard --secret secret --connect \n\\$\nDATABASE_URL\n\n\n\n\n\nFinally, spin up a dyno and the application will start receiving requests:\n\n\nheroku ps:scale \nweb\n=\n1", 
            "title": "Deploy on Heroku"
        }, 
        {
            "location": "/deploy/deploy_heroku/#deploying-an-aqueduct-application-on-heroku", 
            "text": "For other deployment options, see  Deploying Aqueduct Applications .", 
            "title": "Deploying an Aqueduct Application on Heroku"
        }, 
        {
            "location": "/deploy/deploy_heroku/#purpose", 
            "text": "To run a production Aqueduct application on Heroku. Make sure to also read  Testing Aqueduct Applications .", 
            "title": "Purpose"
        }, 
        {
            "location": "/deploy/deploy_heroku/#prerequisites", 
            "text": "Dart has been installed.  A Heroku account.  git  has been installed.  heroku  has been installed.  Aqueduct has been activated.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/deploy/deploy_heroku/#overview", 
            "text": "Setting up a Heroku application  Setting up an Aqueduct application to run on Heroku  Configuring application values  Running the Aqueduct application   Estimated Time:  5 minutes.", 
            "title": "Overview"
        }, 
        {
            "location": "/deploy/deploy_heroku/#step-1-setting-up-a-heroku-application", 
            "text": "Create a new application in Heroku. Add the 'Heroku Postgres' add-on.  Navigate to the Settings tab in the Heroku web interface and click 'Reveal Config Vars'. Note the DATABASE_URL, it'll get used later.", 
            "title": "Step 1: Setting up a Heroku Application"
        }, 
        {
            "location": "/deploy/deploy_heroku/#step-2-setting-up-an-aqueduct-application-to-run-on-heroku", 
            "text": "If you have not yet, create a new Aqueduct application on your local machine, go into that directory, and initialize it as a git repository:  aqueduct create app_name cd  app_name\ngit init  Login to Heroku and run the Aqueduct tool to configure a project for Heroku. The value for  --heroku   must  be the name of the Heroku application (not the Aqueduct application, unless they are the same, obvi).  heroku login\naqueduct setup --heroku = app_name  This command will create the files Heroku needs to run the application, remove  config.yaml  from  .gitignore  (you'll see why in a moment) and runs some  heroku  commands to set up the Heroku application's environment.", 
            "title": "Step 2: Setting up an Aqueduct Application to Run on Heroku"
        }, 
        {
            "location": "/deploy/deploy_heroku/#step-3-configuring-application-values", 
            "text": "Heroku provides configuration values through environment variables, where Aqueduct normally provides them in  config.yaml  file. Because Aqueduct uses  safe_config , configuration files can map keys to environment variables with a simple syntax. The  config.yaml  file's values get replaced with their environment variable names and it gets checked into source control. To map configuration values to an environment variable, the value for a configuration key is prefixed with a dollar sign ( $ ) followed by the case-sensitive name of the environment variable.  Modify  config.yaml  to appear as follows:  database: $DATABASE_URL\nlogging:\n type: console  Recall that  aqueduct setup  with the  --heroku  option removes  config.yaml  from  .gitignore .", 
            "title": "Step 3: Configuring Application Values"
        }, 
        {
            "location": "/deploy/deploy_heroku/#step-4-running-the-aqueduct-application", 
            "text": "First, create a database migration. The  Procfile  declared that Heroku will automatically run any migration files prior to running the application as long as they are checked into source control.  aqueduct db generate  Now, add all of the files to  git  and push it to heroku:  git add .\ngit commit -m  initial commit \ngit push heroku master  Next, set up an OAuth 2.0 client id:  heroku run /app/dart-sdk/bin/pub global run aqueduct:aqueduct auth add-client --id com.app.standard --secret secret --connect  \\$ DATABASE_URL  Finally, spin up a dyno and the application will start receiving requests:  heroku ps:scale  web = 1", 
            "title": "Step 4: Running the Aqueduct Application"
        }, 
        {
            "location": "/deploy/deploy_aws/", 
            "text": "Deploying an Aqueduct Application on Amazon Web Services (AWS)\n\n\nFor other deployment options, see \nDeploying Aqueduct Applications\n.\n\n\nPurpose\n\n\nTo run a production Aqueduct application on Amazon Web Services. Make sure to also read \nTesting Aqueduct Applications\n.\n\n\nPrerequisites\n\n\n\n\nDart has been installed on your local machine.\n\n\nAn AWS Account\n\n\nA GitHub Account*\n\n\ngit\n has been installed\n on your local machine.\n\n\nAqueduct has been activated on your local machine.\n\n\n\n\n* GitHub will be used for transferring code to the remote machine. You could use \nftp\n, \nscp\n, \nrsync\n, another Git provider, another VCS system, AWS's CodeDeploy, etc.\n\n\nEstimated Time: \n15 minutes.\n\n\nOverview\n\n\n\n\nSetting up the Aqueduct application and GitHub\n\n\nSetting up an EC2 Instance\n\n\nSetting up a Database\n\n\nConfiguring application values\n\n\nRunning the Aqueduct application\n\n\n\n\nStep 1: Setting up the Aqueduct Application\n\n\nSet up a new GitHub repository with the name of your application. The purpose of GitHub here is to transfer the application code to the AWS instance. There are other ways of accomplishing this, so as long as you can get the source code to the machine, you're in good shape.\n\n\nIf you have not yet, create a new Aqueduct application on your local machine, go into that directory, and initialize it as a git repository:\n\n\naqueduct create app_name\n\ncd\n app_name\ngit init\n\n\n\n\n\nThen, setup your local git repository with your remote git repository for the application by executing one of the following commands in the project's directory:\n\n\n# If your machine is set up to use git over SSH ...\n\ngit remote add origin git@github.com:organization/app_name.git\n\n\n# If your machine is set up to use git over HTTPS\n\ngit remote add origin https://github.com/organization/app_name.git\n\n\n# If you are unsure or haven\nt set up GitHub before,\n\n\n# see https://help.github.com/articles/set-up-git/\n\n\n\n\n\n\nThen, grab the repository contents:\n\n\ngit pull\n\n\n\n\n\nKeep the GitHub web interface open, as you'll have to come back to it one more time.\n\n\nStep 2: Setting up an EC2 Instance\n\n\nIn the AWS EC2 control panel, create a new Ubuntu instance. Make sure your VPC has DNS resolution (the default VPC configuration does). Choose or create a security group that allows both HTTP and SSH access for this instance. The rest of the default configuration values are fine.\n\n\nLaunch that instance. When prompted, make sure you either create a new key pair or have access to an existing key pair.\n\n\nAfter creating the EC2 instance, select it in the AWS console and click 'Connect' for instructions on how to SSH into the instance.  \n\n\nIt's useful to add the \nssh\n command that connects to this instance as an alias in your shell and the key file into more permanent storage. The command is something like \nssh -i key.pem ubuntu@host\n. Move the key file \nkey.pem\n into \n~/.ssh\n (it may be named differently):\n\n\ncp key.pem ~/.ssh/key.pem\n\n\n\n\n\nThen add the following line to the file \n~/.bash_profile\n and then reload your profle:\n\n\nalias app_name=\nssh -i ~/.ssh/key.pem ubuntu@host\n\nsource ~/.bash_profile\n\n\n\n\n\nNext, SSH into the EC2 instance by executing the alias locally:\n\n\napp_name\n\n\n\n\n\nOnce the shell for the instance is opened, install Dart (these instructions are located at https://www.dartlang.org/install/linux):\n\n\nsudo apt-get update\nsudo apt-get install apt-transport-https\nsudo sh -c \ncurl https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add -\n\nsudo sh -c \ncurl https://storage.googleapis.com/download.dartlang.org/linux/debian/dart_stable.list \n /etc/apt/sources.list.d/dart_stable.list\n\nsudo apt-get update\nsudo apt-get install dart\n\n\n\n\n\nWhen these steps are completed correctly, the following command will yield \n/usr/bin/dart\n:\n\n\nwhich dart\n\n\n\n\n\nAdd the Dart executable directories to your path by adding the following line to the end of the file \n~/.profile\n:\n\n\nexport PATH=$PATH:\n/usr/lib/dart/bin\n:\n~/.pub-cache/bin\n\n\n\n\n\n\nThen reload the profile:\n\n\nsource\n ~/.profile\n\n\n\n\n\nNow, we'll give this instance permission to clone the application repository from GitHub. In the instance's shell, install \ngit\n and create a new SSH key:\n\n\nsudo apt-get install git\nssh-keygen -t rsa -b \n4096\n -C \nyouremail\n\n\n\n\n\n\nThis command will prompt you three times (for a file name, password and password confirm). Simply hit the Enter key each time.\nThen, add the following to the file \n/etc/ssh/ssh_config\n (requires \nsudo\n):\n\n\nHost github.com\n    Hostname github.com\n    IdentityFile ~/.ssh/id_rsa\n    User git\n\n\n\n\n\nPrint out the contents of the public key and copy them:\n\n\ncat ~/.ssh/id_rsa.pub\n\n\n\n\n\nThe contents will start with the phrase \nssh-rsa\n and end with your email, and you must copy all of it.\n\n\nIn the GitHub repository web interface, select the \nSettings\n tab then select \nDeploy keys\n. Click \nAdd deploy key\n. Enter \"AWS\" for the title and paste the contents of the public key into the \nKey\n area. Then click \nAdd key\n.\n\n\nTo ensure this all works, clone the repository onto the AWS instance:\n\n\ngit clone git@github.com:organization/app_name.git\n\n\n\n\n\nAt this point, the repository should mostly be empty, but as long as it clones correctly you're in good shape.\n\n\nStep 3: Setting up a Database\n\n\nIn the AWS control panel, select the RDS service. Choose the \nInstances\n item from the left hand panel and select \nLaunch DB Instance\n. Choose PostgreSQL and configure the database details. Make sure to store the username and password as you'll need them shortly.\n\n\nIn the \nConfigure Advanced Settings\n, make sure the database is Publicly Accessible. Set \nDatabase Name\n to the name of your application, this will make it easy to remember.\n\n\nAdd a new Inbound entry to the security group for the database. The type must be \nPostgreSQL\n (which automatically configures the protocol to \nTCP\n and the port range to \n5432\n). Choose a custom Source and enter the name of the security group that the EC2 instance is in. (You can start by typing \"sg-\", and it give you a drop-down list so that you can select the appropriate one.)\n\n\nThen, launch the database.\n\n\nOnce the database has finished launching, we must upload the application's schema. From the project directory on your local machine, run the following:\n\n\naqueduct db generate\n\n\n\n\n\nNext, run the newly generated migration file on the database, substituting the values in the \n--connect\n option with values from the recently configured database:\n\n\naqueduct db upgrade --connect postgres://username:password@host:5432/app_name\n\n\n\n\n\nStep 4: Configuring the Application\n\n\nConfiguring an Aqueduct application on AWS means having a configuration file that lives on the instance, but is not checked into source control. There are tools for managing configurations across instances, but those are up to you.\n\n\nIn the project directory on your local machine, add all of the project files to the git repository:\n\n\ngit add .\ngit commit -am \nInitial commit\n\ngit push -u origin master\n\n\n\n\n\nOn the EC2 instance, grab these files from the repository (this assumes you ran \ngit clone\n earlier).\n\n\ncd\n app_name\ngit pull\n\n\n\n\n\nCreate a new configuration file just for this instance by cloning the configuration template file that is checked into the repository:\n\n\ncp config.yaml.src config.yaml\n\n\n\n\n\nModify \nconfig.yaml\n by replacing the database credentials with the credentials of the RDS database and change \nlogging:type\n to \nfile\n:\n\n\ndatabase\n:\n\n \nusername\n:\n \nusername\n\n \npassword\n:\n \npassword\n\n \nhost\n:\n \nhost\n\n \nport\n:\n \n5432\n\n \ndatabaseName\n:\n \napp_name\n\n\nlogging\n:\n\n \ntype\n:\n \nfile\n\n \nfilename\n:\n \napi\n.\nlog\n\n\n\n\n\n\nStep 5: Running the Application\n\n\nThen, activate the Aqueduct package:\n\n\npub global activate aqueduct\n\n\n\n\n\nFetch the application's dependencies:\n\n\npub get\n\n\n\n\n\nNow, run the application in \n--detached\n mode:\n\n\naqueduct serve --detached\n\n\n\n\n\nBy default, an Aqueduct application will listen on port 8081. HTTP requests will come in on port 80. You can't bind to port 80 without using sudo. Instead, reroute HTTP requests on port 80 to port 8081 by entering the following on the EC2 instance:\n\n\nsudo iptables -t nat -A PREROUTING -p tcp --dport \n80\n -j REDIRECT --to \n8081\n\nsudo iptables-save\n\n\n\n\n\nThen - either locally or remotely - add a new OAuth 2.0 client:\n\n\n  aqueduct auth add-client --id com.app.standard --secret secret --connect postgres://user:password@deploy-aws.hexthing.us-east-1.rds.amazonaws.com:5432/deploy_aws\n\n\n\n\n\nYour Aqueduct application is now up and running.", 
            "title": "Deploy on AWS"
        }, 
        {
            "location": "/deploy/deploy_aws/#deploying-an-aqueduct-application-on-amazon-web-services-aws", 
            "text": "For other deployment options, see  Deploying Aqueduct Applications .", 
            "title": "Deploying an Aqueduct Application on Amazon Web Services (AWS)"
        }, 
        {
            "location": "/deploy/deploy_aws/#purpose", 
            "text": "To run a production Aqueduct application on Amazon Web Services. Make sure to also read  Testing Aqueduct Applications .", 
            "title": "Purpose"
        }, 
        {
            "location": "/deploy/deploy_aws/#prerequisites", 
            "text": "Dart has been installed on your local machine.  An AWS Account  A GitHub Account*  git  has been installed  on your local machine.  Aqueduct has been activated on your local machine.   * GitHub will be used for transferring code to the remote machine. You could use  ftp ,  scp ,  rsync , another Git provider, another VCS system, AWS's CodeDeploy, etc.  Estimated Time:  15 minutes.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/deploy/deploy_aws/#overview", 
            "text": "Setting up the Aqueduct application and GitHub  Setting up an EC2 Instance  Setting up a Database  Configuring application values  Running the Aqueduct application", 
            "title": "Overview"
        }, 
        {
            "location": "/deploy/deploy_aws/#step-1-setting-up-the-aqueduct-application", 
            "text": "Set up a new GitHub repository with the name of your application. The purpose of GitHub here is to transfer the application code to the AWS instance. There are other ways of accomplishing this, so as long as you can get the source code to the machine, you're in good shape.  If you have not yet, create a new Aqueduct application on your local machine, go into that directory, and initialize it as a git repository:  aqueduct create app_name cd  app_name\ngit init  Then, setup your local git repository with your remote git repository for the application by executing one of the following commands in the project's directory:  # If your machine is set up to use git over SSH ... \ngit remote add origin git@github.com:organization/app_name.git # If your machine is set up to use git over HTTPS \ngit remote add origin https://github.com/organization/app_name.git # If you are unsure or haven t set up GitHub before,  # see https://help.github.com/articles/set-up-git/   Then, grab the repository contents:  git pull  Keep the GitHub web interface open, as you'll have to come back to it one more time.", 
            "title": "Step 1: Setting up the Aqueduct Application"
        }, 
        {
            "location": "/deploy/deploy_aws/#step-2-setting-up-an-ec2-instance", 
            "text": "In the AWS EC2 control panel, create a new Ubuntu instance. Make sure your VPC has DNS resolution (the default VPC configuration does). Choose or create a security group that allows both HTTP and SSH access for this instance. The rest of the default configuration values are fine.  Launch that instance. When prompted, make sure you either create a new key pair or have access to an existing key pair.  After creating the EC2 instance, select it in the AWS console and click 'Connect' for instructions on how to SSH into the instance.    It's useful to add the  ssh  command that connects to this instance as an alias in your shell and the key file into more permanent storage. The command is something like  ssh -i key.pem ubuntu@host . Move the key file  key.pem  into  ~/.ssh  (it may be named differently):  cp key.pem ~/.ssh/key.pem  Then add the following line to the file  ~/.bash_profile  and then reload your profle:  alias app_name= ssh -i ~/.ssh/key.pem ubuntu@host \nsource ~/.bash_profile  Next, SSH into the EC2 instance by executing the alias locally:  app_name  Once the shell for the instance is opened, install Dart (these instructions are located at https://www.dartlang.org/install/linux):  sudo apt-get update\nsudo apt-get install apt-transport-https\nsudo sh -c  curl https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - \nsudo sh -c  curl https://storage.googleapis.com/download.dartlang.org/linux/debian/dart_stable.list   /etc/apt/sources.list.d/dart_stable.list \nsudo apt-get update\nsudo apt-get install dart  When these steps are completed correctly, the following command will yield  /usr/bin/dart :  which dart  Add the Dart executable directories to your path by adding the following line to the end of the file  ~/.profile :  export PATH=$PATH: /usr/lib/dart/bin : ~/.pub-cache/bin   Then reload the profile:  source  ~/.profile  Now, we'll give this instance permission to clone the application repository from GitHub. In the instance's shell, install  git  and create a new SSH key:  sudo apt-get install git\nssh-keygen -t rsa -b  4096  -C  youremail   This command will prompt you three times (for a file name, password and password confirm). Simply hit the Enter key each time.\nThen, add the following to the file  /etc/ssh/ssh_config  (requires  sudo ):  Host github.com\n    Hostname github.com\n    IdentityFile ~/.ssh/id_rsa\n    User git  Print out the contents of the public key and copy them:  cat ~/.ssh/id_rsa.pub  The contents will start with the phrase  ssh-rsa  and end with your email, and you must copy all of it.  In the GitHub repository web interface, select the  Settings  tab then select  Deploy keys . Click  Add deploy key . Enter \"AWS\" for the title and paste the contents of the public key into the  Key  area. Then click  Add key .  To ensure this all works, clone the repository onto the AWS instance:  git clone git@github.com:organization/app_name.git  At this point, the repository should mostly be empty, but as long as it clones correctly you're in good shape.", 
            "title": "Step 2: Setting up an EC2 Instance"
        }, 
        {
            "location": "/deploy/deploy_aws/#step-3-setting-up-a-database", 
            "text": "In the AWS control panel, select the RDS service. Choose the  Instances  item from the left hand panel and select  Launch DB Instance . Choose PostgreSQL and configure the database details. Make sure to store the username and password as you'll need them shortly.  In the  Configure Advanced Settings , make sure the database is Publicly Accessible. Set  Database Name  to the name of your application, this will make it easy to remember.  Add a new Inbound entry to the security group for the database. The type must be  PostgreSQL  (which automatically configures the protocol to  TCP  and the port range to  5432 ). Choose a custom Source and enter the name of the security group that the EC2 instance is in. (You can start by typing \"sg-\", and it give you a drop-down list so that you can select the appropriate one.)  Then, launch the database.  Once the database has finished launching, we must upload the application's schema. From the project directory on your local machine, run the following:  aqueduct db generate  Next, run the newly generated migration file on the database, substituting the values in the  --connect  option with values from the recently configured database:  aqueduct db upgrade --connect postgres://username:password@host:5432/app_name", 
            "title": "Step 3: Setting up a Database"
        }, 
        {
            "location": "/deploy/deploy_aws/#step-4-configuring-the-application", 
            "text": "Configuring an Aqueduct application on AWS means having a configuration file that lives on the instance, but is not checked into source control. There are tools for managing configurations across instances, but those are up to you.  In the project directory on your local machine, add all of the project files to the git repository:  git add .\ngit commit -am  Initial commit \ngit push -u origin master  On the EC2 instance, grab these files from the repository (this assumes you ran  git clone  earlier).  cd  app_name\ngit pull  Create a new configuration file just for this instance by cloning the configuration template file that is checked into the repository:  cp config.yaml.src config.yaml  Modify  config.yaml  by replacing the database credentials with the credentials of the RDS database and change  logging:type  to  file :  database : \n  username :   username \n  password :   password \n  host :   host \n  port :   5432 \n  databaseName :   app_name  logging : \n  type :   file \n  filename :   api . log", 
            "title": "Step 4: Configuring the Application"
        }, 
        {
            "location": "/deploy/deploy_aws/#step-5-running-the-application", 
            "text": "Then, activate the Aqueduct package:  pub global activate aqueduct  Fetch the application's dependencies:  pub get  Now, run the application in  --detached  mode:  aqueduct serve --detached  By default, an Aqueduct application will listen on port 8081. HTTP requests will come in on port 80. You can't bind to port 80 without using sudo. Instead, reroute HTTP requests on port 80 to port 8081 by entering the following on the EC2 instance:  sudo iptables -t nat -A PREROUTING -p tcp --dport  80  -j REDIRECT --to  8081 \nsudo iptables-save  Then - either locally or remotely - add a new OAuth 2.0 client:    aqueduct auth add-client --id com.app.standard --secret secret --connect postgres://user:password@deploy-aws.hexthing.us-east-1.rds.amazonaws.com:5432/deploy_aws  Your Aqueduct application is now up and running.", 
            "title": "Step 5: Running the Application"
        }, 
        {
            "location": "/deploy/script/", 
            "text": "You may also run Aqueduct applications with a standalone script, instead of \naqueduct serve\n. In fact, \naqueduct serve\n creates a temporary Dart script to run the application. That script looks something like this:\n\n\nimport\n \ndart:async\n;\n\n\nimport\n \ndart:io\n;\n\n\n\nimport\n \npackage:aqueduct/aqueduct.dart\n;\n\n\nimport\n \npackage:my_application/my_application.dart\n;\n\n\n\nmain\n()\n \nasync\n \n{\n\n  \ntry\n \n{\n\n    \nvar\n \napp\n \n=\n \nnew\n \nApplication\nMyRequestSink\n();\n\n    \nvar\n \nconfig\n \n=\n \nnew\n \nApplicationConfiguration\n()\n\n      \n..\nport\n \n=\n \n8081\n\n      \n..\nconfigurationFilePath\n \n=\n \nconfig.yaml\n;\n\n\n    \napp\n.\nconfiguration\n \n=\n \nconfig\n;\n\n\n    \nawait\n \napp\n.\nstart\n(\nnumberOfInstances:\n \n3\n);\n    \n  \n}\n \ncatch\n \n(\ne\n,\n \nst\n)\n \n{\n\n    \nawait\n \nwriteError\n(\n$\ne\n\\n\n \n$\nst\n);\n\n  \n}\n\n\n}\n\n\n\nFuture\n \nwriteError\n(\nString\n \nerror\n)\n \nasync\n \n{\n\n  \nprint\n(\n$\nerror\n);\n\n\n}\n\n\n\n\n\n\nThe \naqueduct serve\n command properly exits and reports the error if the application fails to start.\n\n\nApplications that aren't use \naqueduct serve\n must be sure to take appropriate action when the application fails to start such that the runner of the script is aware of the failure. A standalone start script should be placed in the \nbin\n directory of a project.", 
            "title": "Deploying without aqueduct serve"
        }, 
        {
            "location": "/cli/overview/", 
            "text": "Tasks\n\n\nThe Aqueduct command-line utility creates projects, runs applications, manages database schemas and other tasks.\n\n\nThis tool is installed through \npub\n:\n\n\npub global activate aqueduct\n\n\n\n\n\nMake sure you update the tool by running this command when a new version of Aqueduct is released.\n\n\nAll command-line tools have a \n--help\n option to show their options.\n\n\nGuides\n\n\n\n\nCreating Applications\n\n\nRunning Applications\n\n\nManaging a Database\n\n\nManaging OAuth 2.0 Clients and Scopes\n\n\nDocumenting an API\n\n\nProject Setup Utilities", 
            "title": "Overview"
        }, 
        {
            "location": "/cli/overview/#tasks", 
            "text": "The Aqueduct command-line utility creates projects, runs applications, manages database schemas and other tasks.  This tool is installed through  pub :  pub global activate aqueduct  Make sure you update the tool by running this command when a new version of Aqueduct is released.  All command-line tools have a  --help  option to show their options.", 
            "title": "Tasks"
        }, 
        {
            "location": "/cli/overview/#guides", 
            "text": "Creating Applications  Running Applications  Managing a Database  Managing OAuth 2.0 Clients and Scopes  Documenting an API  Project Setup Utilities", 
            "title": "Guides"
        }, 
        {
            "location": "/cli/create/", 
            "text": "Creating Aqueduct Applications\n\n\nThe \naqueduct create\n command-line tool creates applications from a template. The usage is:\n\n\naqueduct create app_name\n\n\n\n\n\nThe application name must be snake_case - all lower case, no spaces, no symbols other than \n_\n.\n\n\nBy default, a generated project is fairly empty - it has the minimal content, but the structure of a complete application. For applications that use Aqueduct's ORM or OAuth 2.0 behavior, extended templates exist. These can be listed with the following:\n\n\naqueduct create list-templates\n\n\n\n\n\nTo pick a template, add the \n-t\n option to \naqueduct create\n. For example, the following uses the \ndb\n template:\n\n\naqueduct create -t db app_name\n\n\n\n\n\nThe templates are located in the Aqueduct package under \nexamples/templates\n. When creating a new project from a template, the tool copies the contents of one of the template directories into your current working directory and substitutes some names with your project name.", 
            "title": "Creating Applications"
        }, 
        {
            "location": "/cli/create/#creating-aqueduct-applications", 
            "text": "The  aqueduct create  command-line tool creates applications from a template. The usage is:  aqueduct create app_name  The application name must be snake_case - all lower case, no spaces, no symbols other than  _ .  By default, a generated project is fairly empty - it has the minimal content, but the structure of a complete application. For applications that use Aqueduct's ORM or OAuth 2.0 behavior, extended templates exist. These can be listed with the following:  aqueduct create list-templates  To pick a template, add the  -t  option to  aqueduct create . For example, the following uses the  db  template:  aqueduct create -t db app_name  The templates are located in the Aqueduct package under  examples/templates . When creating a new project from a template, the tool copies the contents of one of the template directories into your current working directory and substitutes some names with your project name.", 
            "title": "Creating Aqueduct Applications"
        }, 
        {
            "location": "/cli/running/", 
            "text": "Running Applications with Aqueduct Serve\n\n\nThe \naqueduct serve\n command-line tool runs applications. This tool is run in a project directory and it generates a Dart script that bootstraps your application.\n\n\nThe structure of the project does matter - if you are creating a project from the template, the appropriate structure already exists. Otherwise, you must ensure you have a library file with the same name as your application (as defined in \npubspec.yaml\n). For example, an application named \ntodo\n must have a \nlib/todo.dart\n file. This file must import the file that declares your application's \nRequestSink\n.\n\n\nYou may specify options like the number of isolates to run the application on and which port to listen for requests on. See more details with \naqueduct serve --help\n.", 
            "title": "Running Applications"
        }, 
        {
            "location": "/cli/running/#running-applications-with-aqueduct-serve", 
            "text": "The  aqueduct serve  command-line tool runs applications. This tool is run in a project directory and it generates a Dart script that bootstraps your application.  The structure of the project does matter - if you are creating a project from the template, the appropriate structure already exists. Otherwise, you must ensure you have a library file with the same name as your application (as defined in  pubspec.yaml ). For example, an application named  todo  must have a  lib/todo.dart  file. This file must import the file that declares your application's  RequestSink .  You may specify options like the number of isolates to run the application on and which port to listen for requests on. See more details with  aqueduct serve --help .", 
            "title": "Running Applications with Aqueduct Serve"
        }, 
        {
            "location": "/cli/document/", 
            "text": "Documenting Aqueduct Applications\n\n\nThe \naqueduct document\n tool generates an OpenAPI (formerly Swagger) specification by reflecting on your application's code. This command is run in a project directory and will emit the JSON specification to stdout. You can redirect this to a file:\n\n\naqueduct document \n swagger.json\n\n\n\n\n\nThe file \nconfig.src.yaml\n must exist in your project directory so that the application can be initialized in 'test' mode for the documentation to be generated.", 
            "title": "Generating an OpenAPI/Swagger Specification"
        }, 
        {
            "location": "/cli/document/#documenting-aqueduct-applications", 
            "text": "The  aqueduct document  tool generates an OpenAPI (formerly Swagger) specification by reflecting on your application's code. This command is run in a project directory and will emit the JSON specification to stdout. You can redirect this to a file:  aqueduct document   swagger.json  The file  config.src.yaml  must exist in your project directory so that the application can be initialized in 'test' mode for the documentation to be generated.", 
            "title": "Documenting Aqueduct Applications"
        }, 
        {
            "location": "/cli/setup/", 
            "text": "The Aqueduct Setup Tool\n\n\nThe \naqueduct setup\n tool is used for two tasks:\n\n\n\n\nCreating a local test database\n\n\nModifying a project so that it can be deployed to Heroku.\n\n\n\n\nDuring the initial setup of a development machine, after PostgreSQL has been installed locally, the following command creates a local database specifically for testing.\n\n\naqueduct setup\n\n\n\n\n\nThis creates a database user named \ndart\n with password \ndart\n and creates a database \ndart_test\n that \ndart\n has access to on the local machine.\n\n\nFor using \naqueduct setup\n to deploy Heroku applications, see \nthis guide\n.", 
            "title": "Other Tools"
        }, 
        {
            "location": "/cli/setup/#the-aqueduct-setup-tool", 
            "text": "The  aqueduct setup  tool is used for two tasks:   Creating a local test database  Modifying a project so that it can be deployed to Heroku.   During the initial setup of a development machine, after PostgreSQL has been installed locally, the following command creates a local database specifically for testing.  aqueduct setup  This creates a database user named  dart  with password  dart  and creates a database  dart_test  that  dart  has access to on the local machine.  For using  aqueduct setup  to deploy Heroku applications, see  this guide .", 
            "title": "The Aqueduct Setup Tool"
        }, 
        {
            "location": "/snippets/overview/", 
            "text": "Aqueduct Snippets\n\n\nThese snippets are quick examples of common code that you can use and modify in your application.\n\n\n\n\nHTTP Routing, Request and Response Snippets\n\n\nORM and Database Snippets\n\n\nAuthorization and Authentication Snippets\n\n\nIntegration Test Snippets", 
            "title": "Overview"
        }, 
        {
            "location": "/snippets/overview/#aqueduct-snippets", 
            "text": "These snippets are quick examples of common code that you can use and modify in your application.   HTTP Routing, Request and Response Snippets  ORM and Database Snippets  Authorization and Authentication Snippets  Integration Test Snippets", 
            "title": "Aqueduct Snippets"
        }, 
        {
            "location": "/snippets/http/", 
            "text": "Aqueduct HTTP Snippets\n\n\nHello, World\n\n\nclass\n \nAppRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nAppRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n);\n\n\n  \n@\noverride\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n    \nrouter\n.\nroute\n(\n/hello_world\n).\nlisten\n((\nrequest\n)\n \nasync\n \n{\n\n      \nreturn\n \nnew\n \nResponse\n.\nok\n(\nHello, world!\n)\n\n        \n..\ncontentType\n \n=\n \nContentType\n.\nTEXT\n;\n\n    \n});\n\n  \n}\n\n\n}\n\n\n\n\n\n\nRoute Variables\n\n\nclass\n \nAppRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nAppRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n);\n\n\n  \n@\noverride\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n    \nrouter\n.\nroute\n(\n/variable/[:variable]\n).\nlisten\n((\nrequest\n)\n \nasync\n \n{\n\n      \nreturn\n \nnew\n \nResponse\n.\nok\n({\n\n        \nmethod\n:\n \nrequest\n.\ninnerRequest\n.\nmethod\n,\n\n        \npath\n:\n \nrequest\n.\npath\n.\nvariables\n[\nvariable\n]\n \n??\n \nnot specified\n\n      \n});\n      \n    \n});\n\n  \n}\n\n\n}\n\n\n\n\n\n\nGrouping Routes and Binding Path Variables\n\n\nclass\n \nAppRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nAppRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n);\n\n\n  \n@\noverride\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n    \nrouter\n\n      \n.\nroute\n(\n/users/[:id]\n)\n\n      \n.\ngenerate\n(()\n \n=\n \nnew\n \nController\n());\n\n  \n}\n\n\n}\n\n\n\nclass\n \nController\n \nextends\n \nHTTPController\n \n{\n\n  \nfinal\n \nList\nString\n \nthings\n \n=\n \nconst\n \n[\nthing1\n,\n \nthing2\n];\n\n\n  \n@\nhttpGet\n\n  \nFuture\nResponse\n \ngetThings\n()\n \nasync\n \n{\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\nthings\n);\n\n  \n}\n\n\n  \n@\nhttpGet\n\n  \nFuture\nResponse\n \ngetThing\n(\n@\nHTTPPath\n(\nid\n)\n \nint\n \nid\n)\n \nasync\n \n{\n\n    \nif\n \n(\nid\n \n \n0\n \n||\n \nid\n \n=\n \nthings\n.\nlength\n)\n \n{\n\n      \nreturn\n \nnew\n \nResponse\n.\nnotFound\n();\n\n    \n}\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\nthings\n[\nid\n]);\n\n  \n}\n\n\n}\n\n\n\n\n\n\nCustom Middleware\n\n\nclass\n \nAppRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nAppRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n);\n\n\n  \n@\noverride\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n    \nrouter\n\n      \n.\nroute\n(\n/rate_limit\n)\n\n      \n.\npipe\n(\nnew\n \nRateLimiter\n())\n\n      \n.\nlisten\n((\nreq\n)\n \nasync\n \n=\n \nnew\n \nResponse\n.\nok\n({\n\n        \nrequests_remaining\n:\n \nreq\n.\nattachments\n[\nremaining\n]\n\n      \n}));\n\n  \n}\n\n\n}\n\n\n\nclass\n \nRateLimiter\n \nextends\n \nRequestController\n \n{\n\n  \n@\noverride\n\n  \nFuture\nRequestOrResponse\n \nprocessRequest\n(\nRequest\n \nrequest\n)\n \nasync\n \n{\n\n    \nvar\n \napiKey\n \n=\n \nrequest\n.\ninnerRequest\n.\nheaders\n.\nvalue\n(\nx-apikey\n);\n\n    \nvar\n \nrequestsRemaining\n \n=\n \nawait\n \nremainingRequestsForAPIKey\n(\napiKey\n);\n\n    \nif\n \n(\nrequestsRemaining\n \n=\n \n0\n)\n \n{\n\n      \nreturn\n \nnew\n \nResponse\n(\n429\n,\n \nnull\n,\n \nnull\n);\n\n    \n}\n\n\n    \nrequest\n.\naddResponseModifier\n((\nr\n)\n \n{\n\n      \nr\n.\nheaders\n[\nx-remaining-requests\n]\n \n=\n \nrequestsRemaining\n;\n\n    \n});\n\n\n    \nreturn\n \nrequest\n;\n\n  \n}\n\n\n}\n\n\n\n\n\n\nApplication-Wide CORS Allowed Origins\n\n\nclass\n \nAppRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nAppRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n    \n// All controllers will use this policy by default\n\n    \nCORSPolicy\n.\ndefaultPolicy\n.\nallowedOrigins\n \n=\n \n[\nhttps://mywebsite.com\n];\n\n  \n}\n\n\n  \n@\noverride\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n    \nrouter\n.\nroute\n(\n/things\n).\nlisten\n((\nrequest\n)\n \nasync\n \n{\n\n      \nreturn\n \nnew\n \nResponse\n.\nok\n([\nWidget\n,\n \nDoodad\n,\n \nTransformer\n]);\n\n    \n});\n\n  \n}\n\n\n}\n\n\n\n\n\n\nServe Files and Set Cache-Control Headers\n\n\nclass\n \nAppRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nAppRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n);\n\n\n  \n@\noverride\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n    \nrouter\n.\nroute\n(\n/files/*\n).\npipe\n(\n\n      \nnew\n \nHTTPFileController\n(\nweb\n)\n\n        \n..\naddCachePolicy\n(\nnew\n \nHTTPCachePolicy\n(\nexpirationFromNow:\n \nnew\n \nDuration\n(\ndays:\n \n365\n)),\n\n          \n(\npath\n)\n \n=\n \npath\n.\nendsWith\n(\n.js\n)\n \n||\n \npath\n.\nendsWith\n(\n.css\n))\n      \n    \n);\n\n  \n}\n\n\n}\n\n\n\n\n\n\nStreaming Responses (Server Side Events with text/event-stream)\n\n\nclass\n \nAppSink\n \nextends\n \nRequestSink\n \n{\n\n  \nAppSink\n(\nApplicationConfiguration\n \nappConfig\n)\n \n:\n \nsuper\n(\nappConfig\n)\n \n{\n\n    \nvar\n \ncount\n \n=\n \n0\n;\n\n    \nnew\n \nTimer\n.\nperiodic\n(\nnew\n \nDuration\n(\nseconds:\n \n1\n),\n \n(\n_\n)\n \n{\n\n      \ncount\n \n++\n;\n\n      \ncontroller\n.\nadd\n(\nThis server has been up for \n$\ncount\n seconds\n\\n\n);\n\n    \n});\n\n  \n}\n\n\n  \nfinal\n \nStreamController\nString\n \ncontroller\n \n=\n \nnew\n \nStreamController\nString\n();\n  \n\n  \n@\noverride\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n    \nrouter\n.\nroute\n(\n/stream\n).\nlisten\n((\nreq\n)\n \nasync\n \n{\n\n      \nreturn\n \nnew\n \nResponse\n.\nok\n(\ncontroller\n.\nstream\n)\n\n          \n..\nbufferOutput\n \n=\n \nfalse\n\n          \n..\ncontentType\n \n=\n \nnew\n \nContentType\n(\n\n            \ntext\n,\n \nevent-stream\n,\n \ncharset:\n \nutf-8\n);\n\n    \n});\n\n  \n}\n\n\n}\n\n\n\n\n\n\nA websocket server\n\n\nclass\n \nAppRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nAppRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n    \n// When another isolate gets a websocket message, echo it to\n\n    \n// websockets connected on this isolate.\n\n    \nmessageHub\n.\nlisten\n(\nsendBytesToConnectedClients\n);\n\n  \n}\n\n\n  \nList\nWebSocket\n \nwebsockets\n \n=\n \n[];\n\n\n  \n@\noverride\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n    \n// Allow websocket clients to connect to ws://host/connect\n\n    \nrouter\n.\nroute\n(\n/connect\n).\nlisten\n((\nrequest\n)\n \nasync\n \n{\n\n      \nvar\n \nwebsocket\n \n=\n \nawait\n \nWebSocketTransformer\n.\nupgrade\n(\nrequest\n.\ninnerRequest\n);\n\n      \nwebsocket\n.\nlisten\n(\necho\n,\n \nonDone:\n \n()\n \n{\n\n        \nwebsockets\n.\nremove\n(\nwebsocket\n);\n\n      \n},\n \ncancelOnError:\n \ntrue\n);\n\n      \nwebsockets\n.\nadd\n(\nwebsocket\n);\n\n\n      \n// Take request out of channel\n\n      \nreturn\n \nnull\n;\n\n    \n});\n\n  \n}\n\n\n  \nvoid\n \nsendBytesToConnectedClients\n(\nList\nint\n \nbytes\n)\n \n{\n\n    \nwebsockets\n.\nforEach\n((\nws\n)\n \n{\n\n      \nws\n.\nadd\n(\nbytes\n);\n\n    \n});\n\n  \n}\n\n\n  \nvoid\n \necho\n(\nList\nint\n \nbytes\n)\n \n{\n\n    \nsendBytesToConnectedClients\n(\nbytes\n);\n\n\n    \n// Send to other isolates\n\n    \nmessageHub\n.\nadd\n(\nbytes\n);\n\n  \n}\n\n\n}\n\n\n\n\n\n\nSetting Content-Type and Encoding a Response Body\n\n\nclass\n \nAppRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nfinal\n \nContentType\n \nCSV\n \n=\n \nnew\n \nContentType\n(\ntext\n,\n \ncsv\n,\n \ncharset:\n \nutf-8\n);\n\n\n  \nAppRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n\n    \n// CsvCodec extends dart:convert.Codec\n\n    \nHTTPCodecRepository\n.\ndefaultInstance\n.\nadd\n(\nCSV\n,\n \nnew\n \nCsvCodec\n());\n\n  \n}\n\n\n  \n@\noverride\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n    \nrouter\n.\nroute\n(\n/csv\n).\nlisten\n((\nreq\n)\n \nasync\n \n{\n\n      \n// These values will get converted by CsvCodec into a comma-separated string\n\n      \nreturn\n \nnew\n \nResponse\n.\nok\n([[\n1\n,\n \n2\n,\n \n3\n],\n \n[\na\n,\n \nb\n,\n \nc\n]])\n\n        \n..\ncontentType\n \n=\n \nCSV\n;\n\n    \n});\n\n  \n}\n\n\n}\n\n\n\n\n\n\nProxy a File From Another Server\n\n\nclass\n \nAppRequestSink\n \nextends\n \nRequestSink\n \n{\n\n  \nAppRequestSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n);\n\n\n  \n@\noverride\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n    \nrouter\n.\nroute\n(\n/proxy/*\n).\nlisten\n((\nreq\n)\n \nasync\n \n{\n\n      \nvar\n \nfileURL\n \n=\n \nhttps://otherserver/\n${\nreq\n.\npath\n.\nremainingPath\n}\n;\n\n      \nvar\n \nfileRequest\n \n=\n \nawait\n \nclient\n.\ngetUrl\n(\nurl\n);\n\n      \nvar\n \nfileResponse\n \n=\n \nawait\n \nreq\n.\nclose\n();\n\n      \nif\n \n(\nfileResponse\n.\nstatusCode\n \n!=\n \n200\n)\n \n{\n\n        \nreturn\n \nnew\n \nResponse\n.\nnotFound\n();\n\n      \n}\n\n\n      \n// A dart:io.HttpResponse is a Stream\nList\nint\n of its body bytes.\n\n      \nreturn\n \nnew\n \nResponse\n.\nok\n(\nfileResponse\n)\n\n        \n..\ncontentType\n \n=\n \nfileResponse\n.\nheaders\n.\ncontentType\n\n        \n// let the data just pass through because it has already been encoded\n\n        \n// according to content-type; applying encoding again would cause\n\n        \n// an issue\n\n        \n..\nencodeBody\n \n=\n \nfalse\n;\n\n    \n});\n\n  \n}\n\n\n}", 
            "title": "HTTP"
        }, 
        {
            "location": "/snippets/http/#aqueduct-http-snippets", 
            "text": "", 
            "title": "Aqueduct HTTP Snippets"
        }, 
        {
            "location": "/snippets/http/#hello-world", 
            "text": "class   AppRequestSink   extends   RequestSink   { \n   AppRequestSink ( ApplicationConfiguration   config )   :   super ( config ); \n\n   @ override \n   void   setupRouter ( Router   router )   { \n     router . route ( /hello_world ). listen (( request )   async   { \n       return   new   Response . ok ( Hello, world! ) \n         .. contentType   =   ContentType . TEXT ; \n     }); \n   }  }", 
            "title": "Hello, World"
        }, 
        {
            "location": "/snippets/http/#route-variables", 
            "text": "class   AppRequestSink   extends   RequestSink   { \n   AppRequestSink ( ApplicationConfiguration   config )   :   super ( config ); \n\n   @ override \n   void   setupRouter ( Router   router )   { \n     router . route ( /variable/[:variable] ). listen (( request )   async   { \n       return   new   Response . ok ({ \n         method :   request . innerRequest . method , \n         path :   request . path . variables [ variable ]   ??   not specified \n       });       \n     }); \n   }  }", 
            "title": "Route Variables"
        }, 
        {
            "location": "/snippets/http/#grouping-routes-and-binding-path-variables", 
            "text": "class   AppRequestSink   extends   RequestSink   { \n   AppRequestSink ( ApplicationConfiguration   config )   :   super ( config ); \n\n   @ override \n   void   setupRouter ( Router   router )   { \n     router \n       . route ( /users/[:id] ) \n       . generate (()   =   new   Controller ()); \n   }  }  class   Controller   extends   HTTPController   { \n   final   List String   things   =   const   [ thing1 ,   thing2 ]; \n\n   @ httpGet \n   Future Response   getThings ()   async   { \n     return   new   Response . ok ( things ); \n   } \n\n   @ httpGet \n   Future Response   getThing ( @ HTTPPath ( id )   int   id )   async   { \n     if   ( id     0   ||   id   =   things . length )   { \n       return   new   Response . notFound (); \n     } \n     return   new   Response . ok ( things [ id ]); \n   }  }", 
            "title": "Grouping Routes and Binding Path Variables"
        }, 
        {
            "location": "/snippets/http/#custom-middleware", 
            "text": "class   AppRequestSink   extends   RequestSink   { \n   AppRequestSink ( ApplicationConfiguration   config )   :   super ( config ); \n\n   @ override \n   void   setupRouter ( Router   router )   { \n     router \n       . route ( /rate_limit ) \n       . pipe ( new   RateLimiter ()) \n       . listen (( req )   async   =   new   Response . ok ({ \n         requests_remaining :   req . attachments [ remaining ] \n       })); \n   }  }  class   RateLimiter   extends   RequestController   { \n   @ override \n   Future RequestOrResponse   processRequest ( Request   request )   async   { \n     var   apiKey   =   request . innerRequest . headers . value ( x-apikey ); \n     var   requestsRemaining   =   await   remainingRequestsForAPIKey ( apiKey ); \n     if   ( requestsRemaining   =   0 )   { \n       return   new   Response ( 429 ,   null ,   null ); \n     } \n\n     request . addResponseModifier (( r )   { \n       r . headers [ x-remaining-requests ]   =   requestsRemaining ; \n     }); \n\n     return   request ; \n   }  }", 
            "title": "Custom Middleware"
        }, 
        {
            "location": "/snippets/http/#application-wide-cors-allowed-origins", 
            "text": "class   AppRequestSink   extends   RequestSink   { \n   AppRequestSink ( ApplicationConfiguration   config )   :   super ( config )   { \n     // All controllers will use this policy by default \n     CORSPolicy . defaultPolicy . allowedOrigins   =   [ https://mywebsite.com ]; \n   } \n\n   @ override \n   void   setupRouter ( Router   router )   { \n     router . route ( /things ). listen (( request )   async   { \n       return   new   Response . ok ([ Widget ,   Doodad ,   Transformer ]); \n     }); \n   }  }", 
            "title": "Application-Wide CORS Allowed Origins"
        }, 
        {
            "location": "/snippets/http/#serve-files-and-set-cache-control-headers", 
            "text": "class   AppRequestSink   extends   RequestSink   { \n   AppRequestSink ( ApplicationConfiguration   config )   :   super ( config ); \n\n   @ override \n   void   setupRouter ( Router   router )   { \n     router . route ( /files/* ). pipe ( \n       new   HTTPFileController ( web ) \n         .. addCachePolicy ( new   HTTPCachePolicy ( expirationFromNow:   new   Duration ( days:   365 )), \n           ( path )   =   path . endsWith ( .js )   ||   path . endsWith ( .css ))       \n     ); \n   }  }", 
            "title": "Serve Files and Set Cache-Control Headers"
        }, 
        {
            "location": "/snippets/http/#streaming-responses-server-side-events-with-textevent-stream", 
            "text": "class   AppSink   extends   RequestSink   { \n   AppSink ( ApplicationConfiguration   appConfig )   :   super ( appConfig )   { \n     var   count   =   0 ; \n     new   Timer . periodic ( new   Duration ( seconds:   1 ),   ( _ )   { \n       count   ++ ; \n       controller . add ( This server has been up for  $ count  seconds \\n ); \n     }); \n   } \n\n   final   StreamController String   controller   =   new   StreamController String ();   \n\n   @ override \n   void   setupRouter ( Router   router )   { \n     router . route ( /stream ). listen (( req )   async   { \n       return   new   Response . ok ( controller . stream ) \n           .. bufferOutput   =   false \n           .. contentType   =   new   ContentType ( \n             text ,   event-stream ,   charset:   utf-8 ); \n     }); \n   }  }", 
            "title": "Streaming Responses (Server Side Events with text/event-stream)"
        }, 
        {
            "location": "/snippets/http/#a-websocket-server", 
            "text": "class   AppRequestSink   extends   RequestSink   { \n   AppRequestSink ( ApplicationConfiguration   config )   :   super ( config )   { \n     // When another isolate gets a websocket message, echo it to \n     // websockets connected on this isolate. \n     messageHub . listen ( sendBytesToConnectedClients ); \n   } \n\n   List WebSocket   websockets   =   []; \n\n   @ override \n   void   setupRouter ( Router   router )   { \n     // Allow websocket clients to connect to ws://host/connect \n     router . route ( /connect ). listen (( request )   async   { \n       var   websocket   =   await   WebSocketTransformer . upgrade ( request . innerRequest ); \n       websocket . listen ( echo ,   onDone:   ()   { \n         websockets . remove ( websocket ); \n       },   cancelOnError:   true ); \n       websockets . add ( websocket ); \n\n       // Take request out of channel \n       return   null ; \n     }); \n   } \n\n   void   sendBytesToConnectedClients ( List int   bytes )   { \n     websockets . forEach (( ws )   { \n       ws . add ( bytes ); \n     }); \n   } \n\n   void   echo ( List int   bytes )   { \n     sendBytesToConnectedClients ( bytes ); \n\n     // Send to other isolates \n     messageHub . add ( bytes ); \n   }  }", 
            "title": "A websocket server"
        }, 
        {
            "location": "/snippets/http/#setting-content-type-and-encoding-a-response-body", 
            "text": "class   AppRequestSink   extends   RequestSink   { \n   final   ContentType   CSV   =   new   ContentType ( text ,   csv ,   charset:   utf-8 ); \n\n   AppRequestSink ( ApplicationConfiguration   config )   :   super ( config )   { \n     // CsvCodec extends dart:convert.Codec \n     HTTPCodecRepository . defaultInstance . add ( CSV ,   new   CsvCodec ()); \n   } \n\n   @ override \n   void   setupRouter ( Router   router )   { \n     router . route ( /csv ). listen (( req )   async   { \n       // These values will get converted by CsvCodec into a comma-separated string \n       return   new   Response . ok ([[ 1 ,   2 ,   3 ],   [ a ,   b ,   c ]]) \n         .. contentType   =   CSV ; \n     }); \n   }  }", 
            "title": "Setting Content-Type and Encoding a Response Body"
        }, 
        {
            "location": "/snippets/http/#proxy-a-file-from-another-server", 
            "text": "class   AppRequestSink   extends   RequestSink   { \n   AppRequestSink ( ApplicationConfiguration   config )   :   super ( config ); \n\n   @ override \n   void   setupRouter ( Router   router )   { \n     router . route ( /proxy/* ). listen (( req )   async   { \n       var   fileURL   =   https://otherserver/ ${ req . path . remainingPath } ; \n       var   fileRequest   =   await   client . getUrl ( url ); \n       var   fileResponse   =   await   req . close (); \n       if   ( fileResponse . statusCode   !=   200 )   { \n         return   new   Response . notFound (); \n       } \n\n       // A dart:io.HttpResponse is a Stream List int  of its body bytes. \n       return   new   Response . ok ( fileResponse ) \n         .. contentType   =   fileResponse . headers . contentType \n         // let the data just pass through because it has already been encoded \n         // according to content-type; applying encoding again would cause \n         // an issue \n         .. encodeBody   =   false ; \n     }); \n   }  }", 
            "title": "Proxy a File From Another Server"
        }, 
        {
            "location": "/snippets/orm/", 
            "text": "Aqueduct ORM Snippets\n\n\nFilter Query by Column/Property (WHERE clause)\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nEmployee\n()\n\n  \n..\nwhere\n.\ntitle\n \n=\n \nwhereEqualTo\n(\nProgrammer\n);\n\n\nvar\n \nemployees\n \n=\n \nawait\n \nquery\n.\nfetch\n();\n\n\n\n\n\n\nFetching Only Some Columns/Properties\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nEmployee\n()\n\n  \n..\nresultingProperties\n((\ne\n)\n \n=\n \n[\ne\n.\nid\n,\n \ne\n.\nname\n]);\n\n\nvar\n \nemployees\n \n=\n \nawait\n \nquery\n.\nfetch\n();\n\n\n\n\n\n\nSorting Rows/Objects\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nEmployee\n()\n\n  \n..\nsortBy\n((\ne\n)\n \n=\n \ne\n.\nsalary\n,\n \nQuerySortOrder\n.\nascending\n);\n\n\nvar\n \nemployees\n \n=\n \nawait\n \nquery\n.\nfetch\n();\n\n\n\n\n\n\nFetching Only One Row/Object\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nEmployee\n()\n\n  \n..\nwhere\n.\nid\n \n=\n \nwhereEqualTo\n(\n1\n);\n\n\nvar\n \nemployee\n \n=\n \nawait\n \nquery\n.\nfetchOne\n();\n\n\n\n\n\n\nExecuting a Join (Has-One)\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nTeam\n()\n\n  \n..\njoin\n(\nobject:\n \n(\ne\n)\n \n=\n \ne\n.\nleague\n);\n\n\nvar\n \nteamsAndTheirLeague\n \n=\n \nawait\n \nquery\n.\nfetch\n();\n\n\n\n\n\n\nExecuting a Join (Has-Many)\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nTeam\n()\n\n  \n..\njoin\n(\nset\n:\n \n(\ne\n)\n \n=\n \ne\n.\nplayers\n);\n\n\nvar\n \nteamsAndTheirPlayers\n \n=\n \nawait\n \nquery\n.\nfetch\n();\n\n\n\n\n\n\nFiltering Joined Rows/Objects\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nTeam\n();\n\n\n\nvar\n \nsubquery\n \n=\n \nquery\n.\njoin\n(\nset\n:\n \n(\ne\n)\n \n=\n \ne\n.\nplayers\n)\n\n  \n..\nwhere\n.\nyearsPlayed\n \n=\n \nwhereLessThanOrEqualTo\n(\n1\n);\n\n\n\nvar\n \nteamsAndTheirRookiePlayers\n \n=\n \nawait\n \nquery\n.\nfetch\n();\n\n\n\n\n\n\nFilter Rows/Objects by Relationship Property\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nTeam\n()\n\n  \n..\nwhere\n.\nplayers\n.\nhaveAtLeastOneWhere\n.\nyearsPlayed\n \n=\n \nwhereLessThanOrEqualTo\n(\n1\n);\n\n\n\nvar\n \nteamsWithRookies\n \n=\n \nawait\n \nquery\n.\nfetch\n();\n\n\n\n\n\n\nComplex/Unsupported WHERE Clause (using 'OR')\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nTeam\n()\n\n  \n..\npredicate\n \n=\n \nnew\n \nQueryPredicate\n(\nname = \n@name1\n OR name = \n@name2\n,\n \n{\n\n      \nname1\n:\n \nBadgers\n,\n\n      \nname2\n:\n \nGophers\n\n    \n});\n\n\n\nvar\n \nbadgerAndGopherTeams\n \n=\n \nawait\n \nquery\n.\nfetch\n();\n\n\n\n\n\n\nUpdating a Row/Object\n\n\nvar\n \nquery\n \n=\n \nnew\n \nQuery\nTeam\n()\n\n  \n..\nwhere\n.\nid\n \n=\n \nwhereEqualTo\n(\n10\n)\n\n  \n..\nvalues\n.\nname\n \n=\n \nBadgers\n;\n\n\n\nvar\n \nteam\n \n=\n \nawait\n \nquery\n.\nupdateOne\n();\n\n\n\n\n\n\nConfigure a Database Connection from Configuration File\n\n\nclass\n \nAppSink\n \nextends\n \nRequestSink\n \n{\n\n  \nAppSink\n(\nApplicationConfiguration\n \nconfig\n)\n \n:\n \nsuper\n(\nconfig\n)\n \n{\n  \n    \nvar\n \noptions\n \n=\n \nnew\n \nMyAppConfiguration\n(\nappConfig\n.\nconfigurationFilePath\n);\n\n    \ncontext\n \n=\n \ncontextWithConnectionInfo\n(\noptions\n.\ndatabase\n);\n\n  \n}\n\n\n  \nManagedContext\n \ncontext\n;\n\n\n  \n@\noverride\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nr\n)\n \n{\n\n\n  \n}\n\n\n  \nManagedContext\n \ncontextWithConnectionInfo\n(\n\n      \nDatabaseConnectionConfiguration\n \nconnectionInfo\n)\n \n{\n\n    \nvar\n \ndataModel\n \n=\n \nnew\n \nManagedDataModel\n.\nfromCurrentMirrorSystem\n();\n\n    \nvar\n \npsc\n \n=\n \nnew\n \nPostgreSQLPersistentStore\n.\nfromConnectionInfo\n(\n\n        \nconnectionInfo\n.\nusername\n,\n\n        \nconnectionInfo\n.\npassword\n,\n\n        \nconnectionInfo\n.\nhost\n,\n\n        \nconnectionInfo\n.\nport\n,\n\n        \nconnectionInfo\n.\ndatabaseName\n);\n\n\n    \nreturn\n \nnew\n \nManagedContext\n(\ndataModel\n,\n \npsc\n);\n\n  \n}\n\n\n}\n\n\n\nclass\n \nMyAppConfiguration\n \nextends\n \nConfigurationItem\n \n{\n\n  \nMyAppConfiguration\n(\nString\n \nfileName\n)\n \n:\n \nsuper\n.\nfromFile\n(\nfileName\n);\n\n\n  \nDatabaseConnectionConfiguration\n \ndatabase\n;\n\n\n}", 
            "title": "ORM"
        }, 
        {
            "location": "/snippets/orm/#aqueduct-orm-snippets", 
            "text": "", 
            "title": "Aqueduct ORM Snippets"
        }, 
        {
            "location": "/snippets/orm/#filter-query-by-columnproperty-where-clause", 
            "text": "var   query   =   new   Query Employee () \n   .. where . title   =   whereEqualTo ( Programmer );  var   employees   =   await   query . fetch ();", 
            "title": "Filter Query by Column/Property (WHERE clause)"
        }, 
        {
            "location": "/snippets/orm/#fetching-only-some-columnsproperties", 
            "text": "var   query   =   new   Query Employee () \n   .. resultingProperties (( e )   =   [ e . id ,   e . name ]);  var   employees   =   await   query . fetch ();", 
            "title": "Fetching Only Some Columns/Properties"
        }, 
        {
            "location": "/snippets/orm/#sorting-rowsobjects", 
            "text": "var   query   =   new   Query Employee () \n   .. sortBy (( e )   =   e . salary ,   QuerySortOrder . ascending );  var   employees   =   await   query . fetch ();", 
            "title": "Sorting Rows/Objects"
        }, 
        {
            "location": "/snippets/orm/#fetching-only-one-rowobject", 
            "text": "var   query   =   new   Query Employee () \n   .. where . id   =   whereEqualTo ( 1 );  var   employee   =   await   query . fetchOne ();", 
            "title": "Fetching Only One Row/Object"
        }, 
        {
            "location": "/snippets/orm/#executing-a-join-has-one", 
            "text": "var   query   =   new   Query Team () \n   .. join ( object:   ( e )   =   e . league );  var   teamsAndTheirLeague   =   await   query . fetch ();", 
            "title": "Executing a Join (Has-One)"
        }, 
        {
            "location": "/snippets/orm/#executing-a-join-has-many", 
            "text": "var   query   =   new   Query Team () \n   .. join ( set :   ( e )   =   e . players );  var   teamsAndTheirPlayers   =   await   query . fetch ();", 
            "title": "Executing a Join (Has-Many)"
        }, 
        {
            "location": "/snippets/orm/#filtering-joined-rowsobjects", 
            "text": "var   query   =   new   Query Team ();  var   subquery   =   query . join ( set :   ( e )   =   e . players ) \n   .. where . yearsPlayed   =   whereLessThanOrEqualTo ( 1 );  var   teamsAndTheirRookiePlayers   =   await   query . fetch ();", 
            "title": "Filtering Joined Rows/Objects"
        }, 
        {
            "location": "/snippets/orm/#filter-rowsobjects-by-relationship-property", 
            "text": "var   query   =   new   Query Team () \n   .. where . players . haveAtLeastOneWhere . yearsPlayed   =   whereLessThanOrEqualTo ( 1 );  var   teamsWithRookies   =   await   query . fetch ();", 
            "title": "Filter Rows/Objects by Relationship Property"
        }, 
        {
            "location": "/snippets/orm/#complexunsupported-where-clause-using-or", 
            "text": "var   query   =   new   Query Team () \n   .. predicate   =   new   QueryPredicate ( name =  @name1  OR name =  @name2 ,   { \n       name1 :   Badgers , \n       name2 :   Gophers \n     });  var   badgerAndGopherTeams   =   await   query . fetch ();", 
            "title": "Complex/Unsupported WHERE Clause (using 'OR')"
        }, 
        {
            "location": "/snippets/orm/#updating-a-rowobject", 
            "text": "var   query   =   new   Query Team () \n   .. where . id   =   whereEqualTo ( 10 ) \n   .. values . name   =   Badgers ;  var   team   =   await   query . updateOne ();", 
            "title": "Updating a Row/Object"
        }, 
        {
            "location": "/snippets/orm/#configure-a-database-connection-from-configuration-file", 
            "text": "class   AppSink   extends   RequestSink   { \n   AppSink ( ApplicationConfiguration   config )   :   super ( config )   {   \n     var   options   =   new   MyAppConfiguration ( appConfig . configurationFilePath ); \n     context   =   contextWithConnectionInfo ( options . database ); \n   } \n\n   ManagedContext   context ; \n\n   @ override \n   void   setupRouter ( Router   r )   { \n\n   } \n\n   ManagedContext   contextWithConnectionInfo ( \n       DatabaseConnectionConfiguration   connectionInfo )   { \n     var   dataModel   =   new   ManagedDataModel . fromCurrentMirrorSystem (); \n     var   psc   =   new   PostgreSQLPersistentStore . fromConnectionInfo ( \n         connectionInfo . username , \n         connectionInfo . password , \n         connectionInfo . host , \n         connectionInfo . port , \n         connectionInfo . databaseName ); \n\n     return   new   ManagedContext ( dataModel ,   psc ); \n   }  }  class   MyAppConfiguration   extends   ConfigurationItem   { \n   MyAppConfiguration ( String   fileName )   :   super . fromFile ( fileName ); \n\n   DatabaseConnectionConfiguration   database ;  }", 
            "title": "Configure a Database Connection from Configuration File"
        }, 
        {
            "location": "/snippets/auth/", 
            "text": "Aqueduct Authorization and Authentication Snippets\n\n\nEnable OAuth 2.0\n\n\nimport\n \npackage:aqueduct/aqueduct.dart\n;\n\n\nimport\n \npackage:aqueduct/managed_auth.dart\n;\n\n\n\nclass\n \nAppSink\n \nextends\n \nRequestSink\n \n{\n\n  \nAppSink\n(\nApplicationConfiguration\n \nappConfig\n)\n \n:\n \nsuper\n(\nappConfig\n)\n \n{\n\n    \nvar\n \ndataModel\n \n=\n \nnew\n \nManagedDataModel\n.\nfromCurrentMirrorSystem\n();\n\n    \nvar\n \npsc\n \n=\n \nnew\n \nPostgreSQLPersistentStore\n.\nfromConnectionInfo\n(\n\n        \nusername\n,\n\n        \npassword\n,\n\n        \nlocalhost\n,\n\n        \n5432\n\n        \nmy_app\n);\n\n\n    \nManagedContext\n.\ndefaultContext\n \n=\n \nnew\n \nManagedContext\n(\ndataModel\n,\n \npsc\n);\n\n\n    \nvar\n \nauthStorage\n \n=\n \nnew\n \nManagedAuthStorage\nUser\n(\nManagedContext\n.\ndefaultContext\n);\n\n    \nauthServer\n \n=\n \nnew\n \nAuthServer\n(\nauthStorage\n);\n\n  \n}\n\n\n  \nAuthServer\n \nauthServer\n;\n\n\n  \n@\noverride\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n    \nrouter\n.\nroute\n(\n/auth/token\n).\ngenerate\n(()\n \n=\n \nnew\n \nAuthController\n(\nauthServer\n));\n  \n  \n}\n\n\n}\n\n\n\n\n\n\nAdd OAuth 2.0 Clients to Database\n\n\naqueduct auth add-client \\\n  --id com.app.test \\\n  --secret supersecret \\\n  --allowed-scopes \nprofile kiosk:location raw_db_access.readonly\n \\\n  --connect postgres://username:password@localhost:5432/my_app\n\n\n\n\n\nRequire OAuth 2.0 Scope to Access Routes\n\n\nimport\n \npackage:aqueduct/aqueduct.dart\n;\n\n\nimport\n \npackage:aqueduct/managed_auth.dart\n;\n\n\n\nclass\n \nAppSink\n \nextends\n \nRequestSink\n \n{\n\n  \nAppSink\n(\nApplicationConfiguration\n \nappConfig\n)\n \n:\n \nsuper\n(\nappConfig\n)\n \n{\n\n    \nvar\n \ndataModel\n \n=\n \nnew\n \nManagedDataModel\n.\nfromCurrentMirrorSystem\n();\n\n    \nvar\n \npsc\n \n=\n \nnew\n \nPostgreSQLPersistentStore\n.\nfromConnectionInfo\n(\n\n        \nusername\n,\n\n        \npassword\n,\n\n        \nlocalhost\n,\n\n        \n5432\n\n        \nmy_app\n);\n\n\n    \nManagedContext\n.\ndefaultContext\n \n=\n \nnew\n \nManagedContext\n(\ndataModel\n,\n \npsc\n);\n\n\n    \nvar\n \nauthStorage\n \n=\n \nnew\n \nManagedAuthStorage\nUser\n(\nManagedContext\n.\ndefaultContext\n);\n\n    \nauthServer\n \n=\n \nnew\n \nAuthServer\n(\nauthStorage\n);\n\n  \n}\n\n\n  \nAuthServer\n \nauthServer\n;\n\n\n  \n@\noverride\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n    \nrouter\n.\nroute\n(\n/auth/token\n).\ngenerate\n(()\n \n=\n \nnew\n \nAuthController\n(\nauthServer\n));\n\n\n    \nrouter\n\n      \n.\nroute\n(\n/profile\n)\n\n      \n.\npipe\n(\nnew\n \nAuthorizer\n.\nbearer\n(\nauthServer\n,\n \nscopes:\n \n[\nprofile.readonly\n]))\n\n      \n.\ngenerate\n(()\n \n=\n \nnew\n \nProfileController\n());\n\n  \n}\n\n\n}\n\n\n\nclass\n \nProfileController\n \nextends\n \nHTTPController\n \n{\n\n  \n@\nhttpGet\n\n  \nFuture\nResponse\n \ngetProfile\n()\n \nasync\n \n{\n\n    \nvar\n \nid\n \n=\n \nrequest\n.\nauthorization\n.\nresourceOwnerIdentifier\n;\n\n    \nreturn\n \nnew\n \nResponse\n.\nok\n(\nawait\n \nprofileForUserID\n(\nid\n));\n\n  \n}\n\n\n}\n\n\n\n\n\n\nBasic Authentication\n\n\nimport\n \npackage:aqueduct/aqueduct.dart\n;\n\n\n\nclass\n \nAppSink\n \nextends\n \nRequestSink\n \n{\n\n  \nAppSink\n(\nApplicationConfiguration\n \nappConfig\n)\n \n:\n \nsuper\n(\nappConfig\n)\n \n{\n\n    \npasswordVerifier\n \n=\n \nnew\n \nPasswordVerifier\n();\n\n  \n}\n\n\n  \nPasswordVerified\n \npasswordVerifier\n;\n\n\n  \n@\noverride\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n    \nrouter\n\n      \n.\nroute\n(\n/profile\n)\n\n      \n.\npipe\n(\nnew\n \nAuthorizer\n.\nbasic\n(\npasswordVerifier\n))\n\n      \n.\nlisten\n((\nreq\n)\n \nasync\n \n=\n \nnew\n \nResponse\n.\nok\n(\nnull\n));\n\n  \n}\n\n\n}\n\n\n\nclass\n \nPasswordVerifier\n \nextends\n \nAuthValidator\n \n{\n\n  \n@\noverride\n\n  \nFuture\nAuthorization\n \nfromBasicCredentials\n(\nAuthBasicCredentials\n \nusernameAndPassword\n)\n \nasync\n \n{\n\n    \nif\n \n(\n!\nisPasswordCorrect\n(\nusernameAndPassword\n))\n \n{\n\n      \nreturn\n \nnull\n;\n\n    \n}\n\n\n    \nreturn\n \nnew\n \nAuthorization\n(\nnull\n,\n \nusernameAndPassword\n.\nusername\n,\n \nthis\n);\n\n  \n}\n\n\n  \n@\noverride\n\n  \nFuture\nAuthorization\n \nfromBearerToken\n(\nString\n \nbearerToken\n,\n \n{\nList\nAuthScope\n \nscopesRequired\n})\n \n{\n\n    \nthrow\n \nnew\n \nHTTPResponseException\n(\n400\n,\n \nUse basic authorization\n);\n\n  \n}\n\n\n}\n\n\n\n\n\n\nAdd OAuth 2.0 Authorization Code Flow\n\n\nimport\n \npackage:aqueduct/aqueduct.dart\n;\n\n\nimport\n \npackage:aqueduct/managed_auth.dart\n;\n\n\n\nclass\n \nAppSink\n \nextends\n \nRequestSink\n \n{\n\n  \nAppSink\n(\nApplicationConfiguration\n \nappConfig\n)\n \n:\n \nsuper\n(\nappConfig\n)\n \n{\n\n    \nvar\n \ndataModel\n \n=\n \nnew\n \nManagedDataModel\n.\nfromCurrentMirrorSystem\n();\n\n    \nvar\n \npsc\n \n=\n \nnew\n \nPostgreSQLPersistentStore\n.\nfromConnectionInfo\n(\n\n        \nusername\n,\n\n        \npassword\n,\n\n        \nlocalhost\n,\n\n        \n5432\n\n        \nmy_app\n);\n\n\n    \nManagedContext\n.\ndefaultContext\n \n=\n \nnew\n \nManagedContext\n(\ndataModel\n,\n \npsc\n);\n\n\n    \nvar\n \nauthStorage\n \n=\n \nnew\n \nManagedAuthStorage\nUser\n(\nManagedContext\n.\ndefaultContext\n);\n\n    \nauthServer\n \n=\n \nnew\n \nAuthServer\n(\nauthStorage\n);\n\n  \n}\n\n\n  \nAuthServer\n \nauthServer\n;\n\n\n  \n@\noverride\n\n  \nvoid\n \nsetupRouter\n(\nRouter\n \nrouter\n)\n \n{\n\n    \nrouter\n.\nroute\n(\n/auth/token\n).\ngenerate\n(()\n \n=\n \nnew\n \nAuthController\n(\nauthServer\n));\n  \n\n    \nrouter\n.\nroute\n(\n/auth/code\n).\ngenerate\n(()\n \n=\n \nnew\n \nAuthCodeController\n(\nauthServer\n,\n\n        \nrenderAuthorizationPageHTML:\n \nrenderLoginPage\n));\n\n  \n}\n\n\n  \nFuture\nString\n \nrenderLoginPage\n(\n\n    \nAuthCodeController\n \ncontroller\n,\n \nUri\n \nrequestURI\n,\n \nMap\nString\n,\n \nString\n \nqueryParameters\n)\n \nasync\n \n{\n\n\n    \nreturn\n \n\n\n!DOCTYPE html\n\n\nhtml lang=\nen\n\n\n\nhead\n\n\n    \nmeta charset=\nUTF-8\n\n\n    \ntitle\nLogin\n/title\n\n\n/head\n\n\n\nbody\n\n\ndiv class=\ncontainer\n\n\n    \nh1\nLogin\n/h1\n\n\n    \nform action=\n${requestURI.path}\n method=\nPOST\n\n\n        \ninput type=\nhidden\n name=\nstate\n value=\n${queryParameters[\nstate\n]}\n\n\n        \ninput type=\nhidden\n name=\nclient_id\n value=\n${queryParameters[\nclient_id\n]}\n\n\n        \ninput type=\nhidden\n name=\nresponse_type\n value=\ncode\n\n\n        \ndiv class=\nform-group\n\n\n            \nlabel for=\nusername\nUser Name\n/label\n\n\n            \ninput type=\ntext\n class=\nform-control\n name=\nusername\n placeholder=\nPlease enter your user name\n\n\n        \n/div\n\n\n        \ndiv class=\nform-group\n\n\n            \nlabel for=\npassword\nPassword\n/label\n\n\n            \ninput type=\npassword\n class=\nform-control\n name=\npassword\n placeholder=\nPlease enter your password\n\n\n        \n/div\n\n\n        \nbutton type=\nsubmit\n class=\nbtn btn-success\nLogin\n/button\n\n\n    \n/form\n\n\n/div\n\n\n/body\n\n\n\n/html\n\n\n    \n;\n    \n  \n}\n\n\n}", 
            "title": "Authentication and Authorization"
        }, 
        {
            "location": "/snippets/auth/#aqueduct-authorization-and-authentication-snippets", 
            "text": "", 
            "title": "Aqueduct Authorization and Authentication Snippets"
        }, 
        {
            "location": "/snippets/auth/#enable-oauth-20", 
            "text": "import   package:aqueduct/aqueduct.dart ;  import   package:aqueduct/managed_auth.dart ;  class   AppSink   extends   RequestSink   { \n   AppSink ( ApplicationConfiguration   appConfig )   :   super ( appConfig )   { \n     var   dataModel   =   new   ManagedDataModel . fromCurrentMirrorSystem (); \n     var   psc   =   new   PostgreSQLPersistentStore . fromConnectionInfo ( \n         username , \n         password , \n         localhost , \n         5432 \n         my_app ); \n\n     ManagedContext . defaultContext   =   new   ManagedContext ( dataModel ,   psc ); \n\n     var   authStorage   =   new   ManagedAuthStorage User ( ManagedContext . defaultContext ); \n     authServer   =   new   AuthServer ( authStorage ); \n   } \n\n   AuthServer   authServer ; \n\n   @ override \n   void   setupRouter ( Router   router )   { \n     router . route ( /auth/token ). generate (()   =   new   AuthController ( authServer ));   \n   }  }", 
            "title": "Enable OAuth 2.0"
        }, 
        {
            "location": "/snippets/auth/#add-oauth-20-clients-to-database", 
            "text": "aqueduct auth add-client \\\n  --id com.app.test \\\n  --secret supersecret \\\n  --allowed-scopes  profile kiosk:location raw_db_access.readonly  \\\n  --connect postgres://username:password@localhost:5432/my_app", 
            "title": "Add OAuth 2.0 Clients to Database"
        }, 
        {
            "location": "/snippets/auth/#require-oauth-20-scope-to-access-routes", 
            "text": "import   package:aqueduct/aqueduct.dart ;  import   package:aqueduct/managed_auth.dart ;  class   AppSink   extends   RequestSink   { \n   AppSink ( ApplicationConfiguration   appConfig )   :   super ( appConfig )   { \n     var   dataModel   =   new   ManagedDataModel . fromCurrentMirrorSystem (); \n     var   psc   =   new   PostgreSQLPersistentStore . fromConnectionInfo ( \n         username , \n         password , \n         localhost , \n         5432 \n         my_app ); \n\n     ManagedContext . defaultContext   =   new   ManagedContext ( dataModel ,   psc ); \n\n     var   authStorage   =   new   ManagedAuthStorage User ( ManagedContext . defaultContext ); \n     authServer   =   new   AuthServer ( authStorage ); \n   } \n\n   AuthServer   authServer ; \n\n   @ override \n   void   setupRouter ( Router   router )   { \n     router . route ( /auth/token ). generate (()   =   new   AuthController ( authServer )); \n\n     router \n       . route ( /profile ) \n       . pipe ( new   Authorizer . bearer ( authServer ,   scopes:   [ profile.readonly ])) \n       . generate (()   =   new   ProfileController ()); \n   }  }  class   ProfileController   extends   HTTPController   { \n   @ httpGet \n   Future Response   getProfile ()   async   { \n     var   id   =   request . authorization . resourceOwnerIdentifier ; \n     return   new   Response . ok ( await   profileForUserID ( id )); \n   }  }", 
            "title": "Require OAuth 2.0 Scope to Access Routes"
        }, 
        {
            "location": "/snippets/auth/#basic-authentication", 
            "text": "import   package:aqueduct/aqueduct.dart ;  class   AppSink   extends   RequestSink   { \n   AppSink ( ApplicationConfiguration   appConfig )   :   super ( appConfig )   { \n     passwordVerifier   =   new   PasswordVerifier (); \n   } \n\n   PasswordVerified   passwordVerifier ; \n\n   @ override \n   void   setupRouter ( Router   router )   { \n     router \n       . route ( /profile ) \n       . pipe ( new   Authorizer . basic ( passwordVerifier )) \n       . listen (( req )   async   =   new   Response . ok ( null )); \n   }  }  class   PasswordVerifier   extends   AuthValidator   { \n   @ override \n   Future Authorization   fromBasicCredentials ( AuthBasicCredentials   usernameAndPassword )   async   { \n     if   ( ! isPasswordCorrect ( usernameAndPassword ))   { \n       return   null ; \n     } \n\n     return   new   Authorization ( null ,   usernameAndPassword . username ,   this ); \n   } \n\n   @ override \n   Future Authorization   fromBearerToken ( String   bearerToken ,   { List AuthScope   scopesRequired })   { \n     throw   new   HTTPResponseException ( 400 ,   Use basic authorization ); \n   }  }", 
            "title": "Basic Authentication"
        }, 
        {
            "location": "/snippets/auth/#add-oauth-20-authorization-code-flow", 
            "text": "import   package:aqueduct/aqueduct.dart ;  import   package:aqueduct/managed_auth.dart ;  class   AppSink   extends   RequestSink   { \n   AppSink ( ApplicationConfiguration   appConfig )   :   super ( appConfig )   { \n     var   dataModel   =   new   ManagedDataModel . fromCurrentMirrorSystem (); \n     var   psc   =   new   PostgreSQLPersistentStore . fromConnectionInfo ( \n         username , \n         password , \n         localhost , \n         5432 \n         my_app ); \n\n     ManagedContext . defaultContext   =   new   ManagedContext ( dataModel ,   psc ); \n\n     var   authStorage   =   new   ManagedAuthStorage User ( ManagedContext . defaultContext ); \n     authServer   =   new   AuthServer ( authStorage ); \n   } \n\n   AuthServer   authServer ; \n\n   @ override \n   void   setupRouter ( Router   router )   { \n     router . route ( /auth/token ). generate (()   =   new   AuthController ( authServer ));   \n\n     router . route ( /auth/code ). generate (()   =   new   AuthCodeController ( authServer , \n         renderAuthorizationPageHTML:   renderLoginPage )); \n   } \n\n   Future String   renderLoginPage ( \n     AuthCodeController   controller ,   Uri   requestURI ,   Map String ,   String   queryParameters )   async   { \n\n     return    !DOCTYPE html  html lang= en  head       meta charset= UTF-8       title Login /title  /head  body  div class= container       h1 Login /h1       form action= ${requestURI.path}  method= POST           input type= hidden  name= state  value= ${queryParameters[ state ]}           input type= hidden  name= client_id  value= ${queryParameters[ client_id ]}           input type= hidden  name= response_type  value= code           div class= form-group               label for= username User Name /label               input type= text  class= form-control  name= username  placeholder= Please enter your user name           /div           div class= form-group               label for= password Password /label               input type= password  class= form-control  name= password  placeholder= Please enter your password           /div           button type= submit  class= btn btn-success Login /button       /form  /div  /body  /html       ;     \n   }  }", 
            "title": "Add OAuth 2.0 Authorization Code Flow"
        }, 
        {
            "location": "/snippets/test/", 
            "text": "Aqueduct Test Snippets\n\n\nExpect that Response Returns a JSON Object with an ID\n\n\ntest\n(\nthat Response Returns a JSON Object\n,\n \n()\n \nasync\n \n{\n\n  \nexpectResponse\n(\n\n    \nawait\n \napp\n.\nclient\n.\nrequest\n(\n/endpoint\n).\nget\n(),\n\n    \n200\n,\n \nbody:\n \n{\n\n      \nid\n:\n \nisNumber\n\n    \n}\n\n  \n);\n\n\n});\n\n\n\n\n\n\nExpect that Response Returns a List of JSON Objects with IDs\n\n\ntest\n(\nthat Response returns a list of JSON Objects with IDs\n,\n \n()\n \nasync\n \n{\n\n  \nexpectResponse\n(\n\n    \nawait\n \napp\n.\nclient\n.\nrequest\n(\n/endpoint\n).\nget\n(),\n\n    \n200\n,\n \nbody:\n \neveryElement\n({\n\n      \nid\n:\n \nisNumber\n\n    \n})\n\n  \n);\n\n\n});\n\n\n\n\n\n\nExpect that Last-Modified Header Is After Date\n\n\ntest\n(\nthat Last-Modified Header Is After Date \n,\n \n()\n \nasync\n \n{\n\n  \nexpect\n(\n\n    \nawait\n \napp\n.\nclient\n.\nrequest\n(\n/endpoint\n).\nget\n(),\n\n    \nhasHeaders\n({\n\n      \nlast-modified\n:\n \nisAfter\n(\nnew\n \nDateTime\n(\n2017\n))\n\n    \n});\n\n\n});\n\n\n\n\n\n\nHTTP POST with JSON in Test\n\n\ntest\n(\nthat can send JSON body\n,\n \n()\n \nasync\n \n{\n\n  \nvar\n \nrequest\n \n=\n \napp\n.\nclient\n.\nrequest\n(\n/endpoint\n)\n\n    \n..\njson\n \n=\n \n{\n\n      \nid\n:\n \n1\n\n    \n};\n\n  \nexpect\n(\nawait\n \nrequest\n.\npost\n(),\n \nhasStatus\n(\n202\n));\n\n\n});", 
            "title": "Testing"
        }, 
        {
            "location": "/snippets/test/#aqueduct-test-snippets", 
            "text": "", 
            "title": "Aqueduct Test Snippets"
        }, 
        {
            "location": "/snippets/test/#expect-that-response-returns-a-json-object-with-an-id", 
            "text": "test ( that Response Returns a JSON Object ,   ()   async   { \n   expectResponse ( \n     await   app . client . request ( /endpoint ). get (), \n     200 ,   body:   { \n       id :   isNumber \n     } \n   );  });", 
            "title": "Expect that Response Returns a JSON Object with an ID"
        }, 
        {
            "location": "/snippets/test/#expect-that-response-returns-a-list-of-json-objects-with-ids", 
            "text": "test ( that Response returns a list of JSON Objects with IDs ,   ()   async   { \n   expectResponse ( \n     await   app . client . request ( /endpoint ). get (), \n     200 ,   body:   everyElement ({ \n       id :   isNumber \n     }) \n   );  });", 
            "title": "Expect that Response Returns a List of JSON Objects with IDs"
        }, 
        {
            "location": "/snippets/test/#expect-that-last-modified-header-is-after-date", 
            "text": "test ( that Last-Modified Header Is After Date  ,   ()   async   { \n   expect ( \n     await   app . client . request ( /endpoint ). get (), \n     hasHeaders ({ \n       last-modified :   isAfter ( new   DateTime ( 2017 )) \n     });  });", 
            "title": "Expect that Last-Modified Header Is After Date"
        }, 
        {
            "location": "/snippets/test/#http-post-with-json-in-test", 
            "text": "test ( that can send JSON body ,   ()   async   { \n   var   request   =   app . client . request ( /endpoint ) \n     .. json   =   { \n       id :   1 \n     }; \n   expect ( await   request . post (),   hasStatus ( 202 ));  });", 
            "title": "HTTP POST with JSON in Test"
        }
    ]
}